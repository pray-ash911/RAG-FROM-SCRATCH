[
  {
    "chunk_id": "doc_1_p1_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "word2vec Parameter Learning Explained\nXin Rong\nronxin@umich.edu\nAbstract\nThe word2vec model and application by Mikolov et al. have attracted a great\namount of attention in recent two years. The vector representations of words learned\nby word2vec models have been shown to carry semantic meanings and are useful in\nvarious NLP tasks. As an increasing number of researchers would like to experiment\nwith word2vec or similar techniques, I notice that there lacks a material that compre-\nhensivelyexplainstheparameterlearningprocessofwordembeddingmodelsindetails,\nthuspreventingresearchersthatarenon-expertsinneuralnetworksfromunderstanding\nthe working mechanism of such models. This note provides detailed derivations and explanations of the parameter up-\ndate equations of the word2vec models, including the original continuous bag-of-word\n(CBOW) and skip-gram (SG) models, as well as advanced optimization techniques,\nincluding hierarchical softmax and negative sampling. Intuitive interpretations of the\ngradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation\nis provided. I also created an interactive demo, wevi, to facilitate the intuitive under-\nstanding of the model.1\n1 Continuous Bag-of-Word Model\n1.1 One-word context\nWe start from the simplest version of the continuous bag-of-word model (CBOW) intro-\nduced in Mikolov et al. (2013a). We assume that there is only one word considered per\ncontext, whichmeansthemodelwillpredictonetargetwordgivenonecontextword, which\nislikeabigrammodel."
  },
  {
    "chunk_id": "doc_1_p1_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Forreaderswhoarenewtoneuralnetworks,itisrecommendedthat\nonegothroughAppendixAforaquickreviewoftheimportantconceptsandterminologies\nbefore proceeding further. Figure 1 shows the network model under the simplified context definition2. In our\nsetting, the vocabulary size is V, and the hidden layer size is N. The units on adjacent\n1An online interactive demo is available at: http://bit.ly/wevi-online. 2In Figures 1, 2, 3, and the rest of this note, W(cid:48) is not the transpose of W, but a different matrix\ninstead. 1\n6102\nnuJ\n5\n]LC.sc[\n4v8372.1141:viXra"
  },
  {
    "chunk_id": "doc_1_p2_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Input layer Hidden layer Output layer\nx y\n1 1\nx y\n2 2\nx y\n3 h 3\n1\nh\n2\nx y\nk h j\ni\nW ={w } W' ={w' }\nV×N ki h N×V ij\nN\nx y\nV V\nFigure 1: A simple CBOW model with only one word in the context\nlayers are fully connected. The input is a one-hot encoded vector, which means for a given\ninput context word, only one out of V units, {x ,··· ,x }, will be 1, and all other units\n1 V\nare 0. The weights between the input layer and the output layer can be represented by a\nV × N matrix W. Each row of W is the N-dimension vector representation v of the\nw\nassociated word of the input layer. Formally, row i of W is vT. Given a context (a word),\nw\nassuming x = 1 and x = 0 for k(cid:48) (cid:54)= k, we have\nk k(cid:48)\nh = WTx = WT := vT , (1)\n(k,·) wI\nwhich is essentially copying the k-th row of W to h. v is the vector representation of the\nwI\ninput word w . This implies that the link (activation) function of the hidden layer units is\nI\nsimply linear (i.e., directly passing its weighted sum of inputs to the next layer). Fromthehiddenlayertotheoutputlayer,thereisadifferentweightmatrixW(cid:48) = {w(cid:48) },\nij\nwhich is an N ×V matrix."
  },
  {
    "chunk_id": "doc_1_p2_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Using these weights, we can compute a score u for each word\nj\nin the vocabulary,\nu = v(cid:48) T h, (2)\nj wj\nwhere v(cid:48) is the j-th column of the matrix W(cid:48). Then we can use softmax, a log-linear\nwj\nclassification model, to obtain the posterior distribution of words, which is a multinomial\ndistribution. exp(u )\nj\np(w |w ) = y = , (3)\nj I j (cid:80)V\nexp(u )\nj(cid:48)=1 j(cid:48)\nwhere y is the output of the j-the unit in the output layer. Substituting (1) and (2) into\nj\n2"
  },
  {
    "chunk_id": "doc_1_p3_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "(3), we obtain\n(cid:16) (cid:17)\nexp v(cid:48) Tv\nwj wI\np(w |w ) = (4)\nj I (cid:16) (cid:17)\n(cid:80)V exp v(cid:48) Tv\nj(cid:48)=1 w j(cid:48) wI\nNote that v and v(cid:48) are two representations of the word w. v comes from rows of\nw w w\nW, which is the input→hidden weight matrix, and v(cid:48) comes from columns of W(cid:48), which\nw\nis the hidden→output matrix. In subsequent analysis, we call v as the “input vector”,\nw\nand v(cid:48) as the “output vector” of the word w.\nw\nUpdate equation for hidden→output weights\nLet us now derive the weight update equation for this model. Although the actual com-\nputation is impractical (explained below), we are doing the derivation to gain insights on\nthis original model with no tricks applied. For a review of basics of backpropagation, see\nAppendix A. The training objective (for one training sample) is to maximize (4), the conditional\nprobability of observing the actual output word w (denote its index in the output layer\nO\nas j∗) given the input context word w with regard to the weights."
  },
  {
    "chunk_id": "doc_1_p3_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "I\nmaxp(w |w ) = maxy (5)\nO I j∗\n= maxlogy (6)\nj∗\nV\n(cid:88)\n= u −log exp(u ) := −E, (7)\nj∗ j(cid:48)\nj(cid:48)=1\nwhere E = −logp(w |w ) is our loss function (we want to minimize E), and j∗ is the\nO I\nindex of the actual output word in the output layer. Note that this loss function can be\nunderstood as a special case of the cross-entropy measurement between two probabilistic\ndistributions. Letusnowderivetheupdateequationoftheweightsbetweenhiddenandoutputlayers. Take the derivative of E with regard to j-th unit’s net input u , we obtain\nj\n∂E\n= y −t := e (8)\nj j j\n∂u\nj\nwhere t = 1(j = j∗), i.e., t will only be 1 when the j-th unit is the actual output word,\nj j\notherwise t = 0. Note that this derivative is simply the prediction error e of the output\nj j\nlayer. Next we take the derivative on w(cid:48) to obtain the gradient on the hidden→output\nij\nweights. ∂E ∂E ∂u\nj\n= · = e ·h (9)\n∂w(cid:48) ∂u ∂w(cid:48) j i\nij j ij\n3"
  },
  {
    "chunk_id": "doc_1_p4_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Therefore, using stochastic gradient descent, we obtain the weight updating equation for\nhidden→output weights:\nw(cid:48) (new) = w(cid:48) (old) −η·e ·h . (10)\nij ij j i\nor\nv(cid:48) (new) = v(cid:48) (old) −η·e ·h for j = 1,2,··· ,V. (11)\nwj wj j\nwhere η > 0 is the learning rate, e = y −t , and h is the i-th unit in the hidden layer;\nj j j i\nv(cid:48) is the output vector of w . Note that this update equation implies that we have to\nwj j\ngo through every possible word in the vocabulary, check its output probability y , and\nj\ncompare y with its expected output t (either 0 or 1). If y > t (“overestimating”),\nj j j j\nthen we subtract a proportion of the hidden vector h (i.e., v ) from v(cid:48) , thus making\nwI wj\nv(cid:48) farther away from v ; if y < t (“underestimating”, which is true only if t = 1,\nwj wI j j j\ni.e., w = w ), we add some h to v(cid:48) , thus making v(cid:48) closer3 to v . If y is very\nj O wO wO wI j\nclose to t , then according to the update equation, very little change will be made to the\nj\nweights."
  },
  {
    "chunk_id": "doc_1_p4_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Note, again, that v (input vector) and v(cid:48) (output vector) are two different\nw w\nvector representations of the word w.\nUpdate equation for input→hidden weights\nHaving obtained the update equations for W(cid:48), we can now move on to W. We take the\nderivative of E on the output of the hidden layer, obtaining\nV V\n∂E = (cid:88) ∂E · ∂u j = (cid:88) e ·w(cid:48) := EH (12)\n∂h ∂u ∂h j ij i\ni j i\nj=1 j=1\nwhere h is the output of the i-th unit of the hidden layer; u is defined in (2), the net\ni j\ninput of the j-th unit in the output layer; and e = y −t is the prediction error of the\nj j j\nj-th word in the output layer. EH, an N-dim vector, is the sum of the output vectors of\nall words in the vocabulary, weighted by their prediction error. Next we should take the derivative of E on W. First, recall that the hidden layer\nperforms a linear computation on the values from the input layer. Expanding the vector\nnotation in (1) we get\nV\n(cid:88)\nh = x ·w (13)\ni k ki\nk=1\nNow we can take the derivative of E with regard to each element of W, obtaining\n∂E ∂E ∂h\ni\n= · = EH ·x (14)\ni k\n∂w ∂h ∂w\nki i ki\n3Here when I say “closer” or “farther”, I meant using the inner product instead of Euclidean as the\ndistance measurement. 4"
  },
  {
    "chunk_id": "doc_1_p5_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "This is equivalent to the tensor product of x and EH, i.e.,\n∂E\n= x⊗EH = xEHT (15)\n∂W\nfrom which we obtain a V ×N matrix. Since only one component of x is non-zero, only\none row of ∂E is non-zero, and the value of that row is EHT, an N-dim vector. We obtain\n∂W\nthe update equation of W as\nv(new) = v(old)−ηEHT (16)\nwI wI\nwhere v is a row of W, the “input vector” of the only context word, and is the only row\nwI\nof W whose derivative is non-zero. All the other rows of W will remain unchanged after\nthis iteration, because their derivatives are zero. Intuitively, since vector EH is the sum of output vectors of all words in vocabulary\nweighted by their prediction error e = y −t , we can understand (16) as adding a portion\nj j j\nof every output vector in vocabulary to the input vector of the context word. If, in the\noutputlayer,theprobabilityofawordw beingtheoutputwordisoverestimated(y > t ),\nj j j\nthentheinputvectorofthecontextwordw willtendtomovefartherawayfromtheoutput\nI\nvector of w ; conversely if the probability of w being the output word is underestimated\nj j\n(y < t ), then the input vector w will tend to move closer to the output vector of w ;\nj j I j\nif the probability of w is fairly accurately predicted, then it will have little effect on the\nj\nmovementoftheinputvectorofw ."
  },
  {
    "chunk_id": "doc_1_p5_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "Themovementoftheinputvectorofw isdetermined\nI I\nby the prediction error of all vectors in the vocabulary; the larger the prediction error, the\nmore significant effects a word will exert on the movement on the input vector of the\ncontext word. As we iteratively update the model parameters by going through context-target word\npairs generated from a training corpus, the effects on the vectors will accumulate. We\ncan imagine that the output vector of a word w is “dragged” back-and-forth by the input\nvectors of w’s co-occurring neighbors, as if there are physical strings between the vector\nof w and the vectors of its neighbors. Similarly, an input vector can also be considered as\nbeing dragged by many output vectors. This interpretation can remind us of gravity, or\nforce-directed graph layout. The equilibrium length of each imaginary string is related to\nthe strength of cooccurrence between the associated pair of words, as well as the learning\nrate. After many iterations, the relative positions of the input and output vectors will\neventually stabilize. 1.2 Multi-word context\nFigure 2 shows the CBOW model with a multi-word context setting. When computing\nthe hidden layer output, instead of directly copying the input vector of the input context\nword, the CBOW model takes the average of the vectors of the input context words, and\n5"
  },
  {
    "chunk_id": "doc_1_p6_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "use the product of the input→hidden weight matrix and the average vector as the output. 1\nh = WT(x +x +···+x ) (17)\n1 2 C\nC\n1\n= (v +v +···+v )T (18)\nC\nw1 w2 wC\nwhereC isthenumberofwordsinthecontext,w ,··· ,w arethewordstheinthecontext,\n1 C\nand v is the input vector of a word w. The loss function is\nw\nE = = −logp(w |w ,··· ,w ) (19)\nO I,1 I,C\nV\n(cid:88)\n= −u +log exp(u ) (20)\nj∗ j(cid:48)\nj(cid:48)=1\nV\n= −v(cid:48) T ·h+log (cid:88) exp(v(cid:48) T ·h) (21)\nwO wj\nj(cid:48)=1\nwhich is the same as (7), the objective of the one-word-context model, except that h is\ndifferent, as defined in (18) instead of (1). Input layer\nx\n1k\nW\nV×N\nOutput layer\nHidden layer\nx 2k W V×N h i W' N×V y j\nN-dim\nV-dim\nW\nV×N\nx\nCk\nC×V-dim\nFigure 2: Continuous bag-of-word model\n6"
  },
  {
    "chunk_id": "doc_1_p7_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "The update equation for the hidden→output weights stay the same as that for the\none-word-context model (11). We copy it here:\nv(cid:48) (new) = v(cid:48) (old) −η·e ·h for j = 1,2,··· ,V. (22)\nwj wj j\nNote that we need to apply this to every element of the hidden→output weight matrix for\neach training instance. The update equation for input→hidden weights is similar to (16), except that now we\nneed to apply the following equation for every word w in the context:\nI,c\n1\nv(new) = v(old)− ·η·EHT for c = 1,2,··· ,C. (23)\nwI,c wI,c C\nwherev istheinputvectorofthec-thwordintheinputcontext; η isapositivelearning\nwI,c\nrate; and EH = ∂E is given by (12). The intuitive understanding of this update equation\n∂hi\nis the same as that for (16). 2 Skip-Gram Model\nThe skip-gram model is introduced in Mikolov et al. (2013a,b). Figure 3 shows the skip-\ngram model. It is the opposite of the CBOW model. The target word is now at the input\nlayer, and the context words are on the output layer. We still use v to denote the input vector of the only word on the input layer, and\nwI\nthus we have the same definition of the hidden-layer outputs h as in (1), which means h is\nsimplycopying(andtransposing)arowoftheinput→hiddenweightmatrix,W,associated\nwith the input word w ."
  },
  {
    "chunk_id": "doc_1_p7_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "We copy the definition of h below:\nI\nh = WT := vT , (24)\n(k,·) wI\nOn the output layer, instead of outputing one multinomial distribution, we are output-\ning C multinomial distributions. Each output is computed using the same hidden→output\nmatrix:\nexp(u )\nc,j\np(w = w |w ) = y = (25)\nc,j O,c I c,j (cid:80)V\nexp(u )\nj(cid:48)=1 j(cid:48)\nwhere w is the j-th word on the c-th panel of the output layer; w is the actual c-th\nc,j O,c\nword in the output context words; w is the only input word; y is the output of the j-th\nI c,j\nunit on the c-th panel of the output layer; u is the net input of the j-th unit on the c-th\nc,j\npanel of the output layer. Because the output layer panels share the same weights, thus\nu = u = v(cid:48) T ·h, for c = 1,2,··· ,C (26)\nc,j j wj\nwhere v(cid:48) is the output vector of the j-th word in the vocabulary, w , and also v(cid:48) is\nwj j wj\ntaken from a column of the hidden→output weight matrix, W(cid:48). 7"
  },
  {
    "chunk_id": "doc_1_p8_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Output layer\ny\n1,j\nW'\nN×V\nInput layer\nHidden layer\nx k W V×N h i W' N×V y 2,j\nN-dim\nV-dim\nW'\nN×V\ny\nC,j\nC×V-dim\nFigure 3: The skip-gram model. The derivation of parameter update equations is not so different from the one-word-\ncontext model. The loss function is changed to\nE = −logp(w ,w ,··· ,w |w ) (27)\nO,1 O,2 O,C I\nC\n(cid:89) exp(u c,j∗ )\n= −log c (28)\n(cid:80)V\nexp(u )\nc=1 j(cid:48)=1 j(cid:48)\nC V\n(cid:88) (cid:88)\n= − u +C ·log exp(u ) (29)\nj∗ j(cid:48)\nc\nc=1 j(cid:48)=1\nwhere j∗ is the index of the actual c-th output context word in the vocabulary. c\nWe take the derivative of E with regard to the net input of every unit on every panel\nof the output layer, u and obtain\nc,j\n∂E\n= y −t := e (30)\nc,j c,j c,j\n∂u\nc,j\nwhich is the prediction error on the unit, the same as in (8). For notation simplicity, we\ndefine a V-dimensional vector EI = {EI ,··· ,EI } as the sum of prediction errors over all\n1 V\n8"
  },
  {
    "chunk_id": "doc_1_p9_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "context words:\nC\n(cid:88)\nEI = e (31)\nj c,j\nc=1\nNext, we take the derivative of E with regard to the hidden→output matrix W(cid:48), and\nobtain\nC\n∂E (cid:88) ∂E ∂u c,j\n= · = EI ·h (32)\n∂w(cid:48) ∂u ∂w(cid:48) j i\nij c=1 c,j ij\nThus we obtain the update equation for the hidden→output matrix W(cid:48),\nw(cid:48) (new) = w(cid:48) (old) −η·EI ·h (33)\nij ij j i\nor\nv(cid:48) (new) = v(cid:48) (old) −η·EI ·h for j = 1,2,··· ,V. (34)\nwj wj j\nThe intuitive understanding of this update equation is the same as that for (11), except\nthat the prediction error is summed across all context words in the output layer. Note that\nwe need to apply this update equation for every element of the hidden→output matrix for\neach training instance. The derivation of the update equation for the input→hidden matrix is identical to (12)\nto (16), except taking into account that the prediction error e is replaced with EI . We\nj j\ndirectly give the update equation:\nv(new) = v(old)−η·EHT (35)\nwI wI\nwhere EH is an N-dim vector, each component of which is defined as\nV\n(cid:88)\nEH = EI ·w(cid:48) . (36)\ni j ij\nj=1\nThe intuitive understanding of (35) is the same as that for (16)."
  },
  {
    "chunk_id": "doc_1_p9_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "3 Optimizing Computational Efficiency\nSo far the models we have discussed (“bigram” model, CBOW and skip-gram) are both in\ntheir original forms, without any efficiency optimization tricks being applied. For all these models, there exist two vector representations for each word in the vo-\ncabulary: the input vector v , and the output vector v(cid:48) . Learning the input vectors is\nw w\ncheap; but learning the output vectors is very expensive. From the update equations (22)\nand (33), we can find that, in order to update v(cid:48) , for each training instance, we have to\nw\niterate through every word w in the vocabulary, compute their net input u , probability\nj j\n9"
  },
  {
    "chunk_id": "doc_1_p10_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "prediction y (or y for skip-gram), their prediction error e (or EI for skip-gram), and\nj c,j j j\nfinally use their prediction error to update their output vector v(cid:48). j\nDoing such computations for all words for every training instance is very expensive,\nmaking it impractical to scale up to large vocabularies or large training corpora. To solve\nthisproblem,anintuitionistolimitthenumberofoutputvectorsthatmustbeupdatedper\ntraining instance. One elegant approach to achieving this is hierarchical softmax; another\napproach is through sampling, which will be discussed in the next section. Both tricks optimize only the computation of the updates for output vectors. In our\nderivations, we care about three values: (1) E, the new objective function; (2) ∂E , the\n∂v(cid:48)\nw\nnew update equation for the output vectors; and (3) ∂E, the weighted sum of predictions\n∂h\nerrors to be backpropagated for updating input vectors. 3.1 Hierarchical Softmax\nHierarchical softmax is an efficient way of computing softmax (Morin and Bengio, 2005;\nMnih and Hinton, 2009). The model uses a binary tree to represent all words in the\nvocabulary. The V words must be leaf units of the tree. It can be proved that there are\nV −1 inner units. For each leaf unit, there exists a unique path from the root to the unit;\nand this path is used to estimate the probability of the word represented by the leaf unit. See Figure 4 for an example tree. n(w ,1)\n2\nn(w ,2)\n2\nn(w ,3)\n2\nw w w w w w\n1 2 3 4 V-1 V\nFigure 4: An example binary tree for the hierarchical softmax model."
  },
  {
    "chunk_id": "doc_1_p10_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "The white units are\nwords in the vocabulary, and the dark units are inner units. An example path from root to\nw is highlighted. In the example shown, the length of the path L(w ) = 4. n(w,j) means\n2 2\nthe j-th unit on the path from root to the word w.\nIn the hierarchical softmax model, there is no output vector representation for words. Instead, each of the V −1 inner units has an output vector v(cid:48) . And the probability of\nn(w,j)\na word being the output word is defined as\nL(w)−1\np(w = w ) = (cid:89) σ (cid:16) n(w,j +1) = ch(n(w,j)) ·v(cid:48) T h (cid:17) (37)\nO n(w,j)\n(cid:74) (cid:75)\nj=1\n10"
  },
  {
    "chunk_id": "doc_1_p11_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "wherech(n)is theleftchildofunitn; v(cid:48) isthe vectorrepresentation(“outputvector”)\nn(w,j)\nof the inner unit n(w,j); h is the output value of the hidden layer (in the skip-gram model\nh = v ; and in CBOW, h = 1 (cid:80)C v ); x is a special function defined as\nwI C c=1 wc\n(cid:74) (cid:75)\n(cid:40)\n1 if x is true;\nx = (38)\n(cid:74) (cid:75) −1 otherwise. Let us intuitively understand the equation by going through an example. Looking at\nFigure 4, suppose we want to compute the probability that w being the output word. We\n2\ndefine this probability as the probability of a random walk starting from the root ending\nat the leaf unit in question. At each inner unit (including the root unit), we need to assign\nthe probabilities of going left and going right.4 We define the probability of going left at\nan inner unit n to be\n(cid:16) (cid:17)\np(n,left) = σ v(cid:48) T ·h (39)\nn\nwhich is determined by both the vector representation of the inner unit, and the hidden\nlayer’s output value (which is then determined by the vector representation of the input\nword(s))."
  },
  {
    "chunk_id": "doc_1_p11_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "Apparently the probability of going right at unit n is\n(cid:16) (cid:17) (cid:16) (cid:17)\np(n,right) = 1−σ v(cid:48) T ·h = σ −v(cid:48) T ·h (40)\nn n\nFollowing the path from the root to w in Figure 4, we can compute the probability of w\n2 2\nbeing the output word as\np(w = w ) = p(n(w ,1),left)·p(n(w ,2),left)·p(n(w ,3),right) (41)\n2 O 2 2 2\n(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)\n= σ v(cid:48) T h ·σ v(cid:48) T h ·σ −v(cid:48) T h (42)\nn(w2,1) n(w2,2) n(w2,3)\nwhich is exactly what is given by (37). It should not be hard to verify that\nV\n(cid:88)\np(w = w ) = 1 (43)\ni O\ni=1\nmaking the hierarchical softmax a well defined multinomial distribution among all words. Now let us derive the parameter update equation for the vector representations of the\ninner units. For simplicity, we look at the one-word context model first. Extending the\nupdate equations to CBOW and skip-gram models is easy."
  },
  {
    "chunk_id": "doc_1_p11_fixed_2",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "For the simplicity of notation, we define the following shortenizations without intro-\nducing ambiguity:\n· := n(w,j +1) = ch(n(w,j)) (44)\n(cid:74) (cid:75) (cid:74) (cid:75)\n4Whileaninnerunitofabinarytreemaynotalwayshavebothchildren,abinaryHuffmantree’sinner\nunits always do. Although theoretically one can use many different types of trees for hierarchical softmax,\nword2vec uses a binary Huffman tree for fast training. 11"
  },
  {
    "chunk_id": "doc_1_p12_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "v(cid:48) := v(cid:48) (45)\nj nw,j\nFor a training instance, the error function is defined as\nL(w)−1\nE = −logp(w = w |w ) = −\n(cid:88)\nlogσ( ·\nv(cid:48)T\nh) (46)\nO I j\n(cid:74) (cid:75)\nj=1\nWe take the derivative of E with regard to v(cid:48)h, obtaining\nj\n∂E (cid:16) (cid:17)\n= σ( ·\nv(cid:48)T\nh)−1 · (47)\n∂v(cid:48)h j\nj (cid:74) (cid:75) (cid:74) (cid:75)\n(cid:40)\nσ(v(cid:48)Th)−1 ( · = 1)\n= σ(v j (cid:48)Th) ( (cid:74) · (cid:75) = −1) (48)\nj\n(cid:74) (cid:75)\n=\nσ(v(cid:48)T\nh)−t (49)\nj j\nwhere t = 1 if · = 1 and t = 0 otherwise."
  },
  {
    "chunk_id": "doc_1_p12_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "j j\n(cid:74) (cid:75)\nNext we take the derivative of E with regard to the vector representation of the inner\nunit n(w,j) and obtain\n∂E ∂E ∂v(cid:48)h (cid:16) (cid:17)\n= ·\nj\n=\nσ(v(cid:48)T\nh)−t ·h (50)\n∂v(cid:48) ∂v(cid:48)h ∂v(cid:48) j j\nj j j\nwhich results in the following update equation:\n(cid:16) (cid:17)\nv(cid:48)(new)\n=\nv(cid:48)(old)\n−η\nσ(v(cid:48)T\nh)−t ·h (51)\nj j j j\nwhich should be applied to j = 1,2,··· ,L(w)−1. We can understand σ(v(cid:48)Th)−t as\nj j\nthe prediction error for the inner unit n(w,j). The “task” for each inner unit is to predict\nwhether it should follow the left child or the right child in the random walk. t = 1 means\nj\nthe ground truth is to follow the left child; t = 0 means it should follow the right child. j\nσ(v(cid:48)Th) is the prediction result. For a training instance, if the prediction of the inner unit\nj\nis very close to the ground truth, then its vector representation v(cid:48) will move very little;\nj\notherwisev(cid:48) willmoveinanappropriatedirectionbymoving(eithercloserorfartheraway5\nj\nfrom h) so as to reduce the prediction error for this instance. This update equation can\nbe used for both CBOW and the skip-gram model."
  },
  {
    "chunk_id": "doc_1_p12_fixed_2",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "When used for the skip-gram model,\nwe need to repeat this update procedure for each of the C words in the output context. 5Again, the distance measurement is inner product. 12"
  },
  {
    "chunk_id": "doc_1_p13_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "In order to backpropagate the error to learn input→hidden weights, we take the deriva-\ntive of E with regard to the output of the hidden layer and obtain\n∂E L( (cid:88) w)−1 ∂E ∂v j (cid:48)h\n= · (52)\n∂h ∂v(cid:48)h ∂h\nj=1 j\nL(w)−1\n= (cid:88) (cid:16) σ(v(cid:48)T h)−t (cid:17) ·v(cid:48) (53)\nj j j\nj=1\n:= EH (54)\nwhich can be directly substituted into (23) to obtain the update equation for the input\nvectors of CBOW. For the skip-gram model, we need to calculate a EH value for each word\nin the skip-gram context, and plug the sum of the EH values into (35) to obtain the update\nequation for the input vector. From the update equations, we can see that the computational complexity per training\ninstance per context word is reduced from O(V) to O(log(V)), which is a big improvement\ninspeed. Westillhaveroughlythesamenumberofparameters(V−1vectorsforinner-units\ncompared to originally V output vectors for words). 3.2 Negative Sampling\nThe idea of negative sampling is more straightforward than hierarchical softmax: in order\nto deal with the difficulty of having too many output vectors that need to be updated per\niteration, we only update a sample of them. Apparently the output word (i.e., the ground truth, or positive sample) should be kept\nin our sample and gets updated, and we need to sample a few words as negative samples\n(hence “negative sampling”)."
  },
  {
    "chunk_id": "doc_1_p13_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "A probabilistic distribution is needed for the sampling pro-\ncess, and it can be arbitrarily chosen. We call this distribution the noise distribution, and\ndenote it as P (w). One can determine a good distribution empirically.6\nn\nIn word2vec, instead of using a form of negative sampling that produces a well-defined\nposterior multinomial distribution, the authors argue that the following simplified training\nobjective is capable of producing high-quality word embeddings:7\nE = −logσ(v(cid:48) T h)− (cid:88) logσ(−v(cid:48) T h) (55)\nwO wj\nwj∈Wneg\nwhere w is the output word (i.e., the positive sample), and v(cid:48) is its output vector; h is\nO wO\nthe output value of the hidden layer: h = 1 (cid:80)C v in the CBOW model and h = v\nC c=1 wc wI\n6As described in (Mikolov et al., 2013b), word2vec uses a unigram distribution raised to the 3th power\n4\nfor the best quality of results. 7GoldbergandLevy(2014)provideatheoreticalanalysisonthereasonofusingthisobjectivefunction. 13"
  },
  {
    "chunk_id": "doc_1_p14_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "in the skip-gram model; W = {w |j = 1,··· ,K} is the set of words that are sampled\nneg j\nbased on P (w), i.e., negative samples. n\nTo obtain the update equations of the word vectors under negative sampling, we first\ntake the derivative of E with regard to the net input of the output unit w :\nj\n(cid:40)\n∂E σ(v(cid:48) Th)−1 if w = w\n= wj j O (56)\n∂v(cid:48) Th σ(v(cid:48) Th) if w ∈ W\nwj wj j neg\n= σ(v(cid:48) T h)−t (57)\nwj j\nwhere t is the “label” of word w . t = 1 when w is a positive sample; t = 0 otherwise. j j j\nNext we take the derivative of E with regard to the output vector of the word w ,\nj\n∂E ∂E\n∂v(cid:48) Th\n(cid:16) (cid:17)\n= · wj = σ(v(cid:48) T h)−t h (58)\n∂v(cid:48) ∂v(cid:48) Th ∂v(cid:48) wj j\nwj wj wj\nwhich results in the following update equation for its output vector:\n(cid:16) (cid:17)\nv(cid:48) (new) = v(cid:48) (old) −η σ(v(cid:48) T h)−t h (59)\nwj wj wj j\nwhichonlyneedstobeappliedtow ∈ {w }∪W insteadofeverywordinthevocabulary. j O neg\nThis shows why we may save a significant amount of computational effort per iteration."
  },
  {
    "chunk_id": "doc_1_p14_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "The intuitive understanding of the above update equation should be the same as that\nof (11). This equation can be used for both CBOW and the skip-gram model. For the\nskip-gram model, we apply this equation for one context word at a time. To backpropagate the error to the hidden layer and thus update the input vectors\nof words, we need to take the derivative of E with regard to the hidden layer’s output,\nobtaining\n∂E = (cid:88) ∂E · ∂v w (cid:48) j Th (60)\n∂h ∂v(cid:48) Th ∂h\nwj∈{wO}∪Wneg wj\n= (cid:88) (cid:16) σ(v(cid:48) T h)−t (cid:17) v(cid:48) := EH (61)\nwj j wj\nwj∈{wO}∪Wneg\nBy plugging EH into (23) we obtain the update equation for the input vectors of the\nCBOW model. For the skip-gram model, we need to calculate a EH value for each word in\nthe skip-gram context, and plug the sum of the EH values into (35) to obtain the update\nequation for the input vector. 14"
  },
  {
    "chunk_id": "doc_1_p15_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "Acknowledgement\nThe author would like to thank Eytan Adar, Qiaozhu Mei, Jian Tang, Dragomir Radev,\nDaniel Pressel, Thomas Dean, Sudeep Gandhe, Peter Lau, Luheng He, Tomas Mikolov,\nHao Jiang, and Oded Shmueli for discussions on the topic and/or improving the writing of\nthe note. References\nGoldberg, Y. and Levy, O. (2014). word2vec explained: deriving mikolov et al.’s negative-\nsampling word-embedding method. arXiv:1402.3722 [cs, stat]. arXiv: 1402.3722. Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013b). Distributed\nrepresentations of words and phrases and their compositionality. In Advances in Neural\nInformation Processing Systems, pages 3111–3119. Mnih, A. and Hinton, G. E. (2009). A scalable hierarchical distributed language model. In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L., editors, Advances in Neural\nInformation Processing Systems 21, pages 1081–1088. Curran Associates, Inc. Morin,F.andBengio,Y.(2005). Hierarchicalprobabilisticneuralnetworklanguagemodel. In AISTATS, volume 5, pages 246–252. Citeseer. 15"
  },
  {
    "chunk_id": "doc_1_p16_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "A Back Propagation Basics\nA.1 Learning Algorithms for a Single Unit\nFigure5showsanartificialneuron(unit). {x ,··· ,x }areinputvalues; {w ,··· ,w }are\n1 K 1 K\nweights;yisascalaroutput;andf isthelinkfunction(alsocalledactivation/decision/transfer\nfunction). x w\n1 1\nx w\n2 2\nx 3 w f y\n3\nw\nx K\nK\nFigure 5: An artificial neuron\nThe unit works in the following way:\ny = f(u), (62)\nwhere u is a scalar number, which is the net input (or “new input”) of the neuron. u is\ndefined as\nK\n(cid:88)\nu = w x . (63)\ni i\ni=0\nUsing vector notation, we can write\nu = wTx (64)\nNote that here we ignore the bias term in u. To include a bias term, one can simply\nadd an input dimension (e.g., x ) that is constant 1. 0\nApparently, different link functions result in distinct behaviors of the neuron. We\ndiscuss two example choices of link functions here. The first example choice of f(u) is the unit step function (aka Heaviside step\nfunction):\n(cid:40)\n1 if u > 0\nf(u) = (65)\n0 otherwise\nA neuron with this link function is called a perceptron. The learning algorithm for a\nperceptron is the perceptron algorithm. Its update equation is defined as:\nw(new) = w(old)−η·(y−t)·x (66)\n16"
  },
  {
    "chunk_id": "doc_1_p17_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "where t is the label (gold standard) and η is the learning rate (η > 0). Note that a\nperceptron is a linear classifier, which means its description capacity can be very limited. If we want to fit more complex functions, we need to use a non-linear model. The second example choice of f(u) is the logistic function (a most common kind of\nsigmoid function), defined as\n1\nσ(u) = (67)\n1+e−u\nThe logistic function has two primary good properties: (1) the output y is always between\n0 and 1, and (2) unlike a unit step function, σ(u) is smooth and differentiable, making the\nderivation of update equation very easy. Note that σ(u) also has the following two properties that can be very convenient and\nwill be used in our subsequent derivations:\nσ(−u) = 1−σ(u) (68)\ndσ(u)\n= σ(u)σ(−u) (69)\ndu\nWe use stochastic gradient descent as the learning algorithm of this model. In order to\nderivetheupdateequation,weneedtodefinetheerrorfunction,i.e.,thetrainingobjective. The following objective function seems to be convenient:\n1\nE = (t−y)2 (70)\n2\nWe take the derivative of E with regard to w ,\ni\n∂E ∂E ∂y ∂u\n= · · (71)\n∂w ∂y ∂u ∂w\ni i\n= (y−t)·y(1−y)·x (72)\ni\nwhere ∂y = y(1 − y) because y = f(u) = σ(u), and (68) and (69)."
  },
  {
    "chunk_id": "doc_1_p17_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "Once we have the\n∂u\nderivative, we can apply stochastic gradient descent:\nw(new) = w(old)−η·(y−t)·y(1−y)·x. (73)\nA.2 Back-propagation with Multi-Layer Network\nFigure 6 shows a multi-layer neural network with an input layer {x } = {x ,··· ,x }, a\nk 1 K\nhidden layer {h } = {h ,··· ,h }, and an output layer {y } = {y ,··· ,y }. For clarity\ni 1 N j 1 M\nwe use k,i,j as the subscript for input, hidden, and output layer units respectively. We use\nu and u(cid:48) to denote the net input of hidden layer units and output layer units respectively. i j\nWe want to derive the update equation for learning the weights w between the input\nki\nand hidden layers, and w(cid:48) between the hidden and output layers. We assume that all the\nij\n17"
  },
  {
    "chunk_id": "doc_1_p18_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "Input layer Hidden layer Output layer\nx y\n1 h 1\n1\nx y\n2 h 2\n2\nx y\n3 3\nh\nN\nx y\nK M\n{w } {w' }\nki ij\nFigure 6: A multi-layer neural network with one hidden layer\ncomputation units (i.e., units in the hidden layer and the output layer) use the logistic\nfunction σ(u) as the link function. Therefore, for a unit h in the hidden layer, its output\ni\nis defined as\n(cid:32) K (cid:33)\n(cid:88)\nh = σ(u ) = σ w x . (74)\ni i ki k\nk=1\nSimilarly, for a unit y in the output layer, its output is defined as\nj\n(cid:32) N (cid:33)\n(cid:88)\ny = σ(u(cid:48)) = σ w(cid:48) h . (75)\nj j ij i\ni=1\nWe use the squared sum error function given by\nM\n1 (cid:88)\nE(x,t,W,W(cid:48)) = (y −t )2, (76)\nj j\n2\nj=1\nwhere W = {w }, a K ×N weight matrix (input-hidden), and W(cid:48) = {w(cid:48) }, a N ×M\nki ij\nweight matrix (hidden-output). t = {t ,··· ,t }, a M-dimension vector, which is the\n1 M\ngold-standard labels of output. To obtain the update equations for w and w(cid:48) , we simply need to take the derivative\nki ij\nof the error function E with regard to the weights respectively."
  },
  {
    "chunk_id": "doc_1_p18_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "To make the derivation\nstraightforward, we do start computing the derivative for the right-most layer (i.e., the\noutput layer), and then move left. For each layer, we split the computation into three\nsteps, computing the derivative of the error with regard to the output, net input, and\nweight respectively. This process is shown below. 18"
  },
  {
    "chunk_id": "doc_1_p19_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "We start with the output layer. The first step is to compute the derivative of the error\nw.r.t. the output:\n∂E\n= y −t . (77)\nj j\n∂y\nj\nThe second step is to compute the derivative of the error with regard to the net input of\nthe output layer. Note that when taking derivatives with regard to something, we need to\nkeep everything else fixed. Also note that this value is very important because it will be\nreused multiple times in subsequent computations. We denote it as EI(cid:48) for simplicity. j\n∂E ∂E ∂y\n= · j = (y −t )·y (1−y ) := EI(cid:48) (78)\n∂u(cid:48) ∂y ∂u(cid:48) j j j j j\nj j j\nThe third step is to compute the derivative of the error with regard to the weight between\nthe hidden layer and the output layer. ∂E ∂E\n∂u(cid:48)\n= · j = EI(cid:48) ·h (79)\n∂w(cid:48) ∂u(cid:48) ∂w(cid:48) j i\nij j ij\nSo far, we have obtained the update equation for weights between the hidden layer and the\noutput layer. ∂E\nw(cid:48) (new) = w(cid:48) (old) −η· (80)\nij ij ∂w(cid:48)\nij\n= w(cid:48) (old) −η·EI(cid:48) ·h . (81)\nij j i\nwhere η > 0 is the learning rate. We can repeat the same three steps to obtain the update equation for weights of the\nprevious layer, which is essentially the idea of back propagation."
  },
  {
    "chunk_id": "doc_1_p19_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "We repeat the first step and compute the derivative of the error with regard to the\noutput of the hidden layer. Note that the output of the hidden layer is related to all units\nin the output layer. ∂E = (cid:88)\nM\n∂E\n∂u(cid:48)\nj = (cid:88)\nM\nEI(cid:48) ·w(cid:48) . (82)\n∂h ∂u(cid:48) ∂h j ij\ni j=1 j i j=1\nThen we repeat the second step above to compute the derivative of the error with regard\nto the net input of the hidden layer. This value is again very important, and we denote it\nas EI . i\nM\n∂E = ∂E · ∂h i = (cid:88) EI(cid:48) ·w(cid:48) ·h (1−h ) := EI (83)\n∂u ∂h ∂u j ij i i i\ni i i\nj=1\nNext we repeat the third step above to compute the derivative of the error with regard to\nthe weights between the input layer and the hidden layer. ∂E ∂E ∂u\ni\n= · = EI ·x , (84)\ni k\n∂w ∂u ∂w\nki i ki\n19"
  },
  {
    "chunk_id": "doc_1_p20_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "Finally, we can obtain the update equation for weights between the input layer and the\nhidden layer. w (new) = w (old)−η·EI ·x . (85)\nki ki i k\nFromtheaboveexample,wecanseethattheintermediateresults(EI(cid:48))whencomputing\nj\nthederivativesforonelayercanbereusedforthepreviouslayer. Imaginetherewereanother\nlayer prior to the input layer, then EI can also be reused to continue computing the chain\ni\nof derivatives efficiently. Compare Equations (78) and (83), we may find that in (83), the\nfactor (cid:80)M EI(cid:48)w(cid:48) is just like the “error” of the hidden layer unit h . We may interpret\nj=1 j ij i\nthis term as the error “back-propagated” from the next layer, and this propagation may\ngo back further if the network has more hidden layers. B wevi: Word Embedding Visual Inspector\nAn interactive visual interface, wevi (word embedding visual inspector), is available online\nto demonstrate the working mechanism of the models described in this paper. See Figure 7\nfor a screenshot of wevi. Thedemoallowstheusertovisuallyexaminethemovementofinputvectorsandoutput\nvectorsaseachtraininginstanceisconsumed. Thetrainingprocesscanbealsoruninbatch\nmode (e.g., consuming 500 training instances in a row), which can reveal the emergence of\npatterns in the weight matrices and the corresponding word vectors. Principal component\nanalysis (PCA) is employed to visualize the “high”-dimensional vectors in a 2D scatter\nplot. The demo supports both CBOW and skip-gram models."
  },
  {
    "chunk_id": "doc_1_p20_fixed_1",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "After training the model, the user can manually activate one or multiple input-layer\nunits, and inspect which hidden-layer units and output-layer units become active. The\nuser can also customize training data, hidden layer size, and learning rate. Several preset\ntraining datasets are provided, which can generate different results that seem interesting,\nsuch as using a toy vocabulary to reproduce the famous word analogy: king - queen = man\n- woman. It is hoped that by interacting with this demo one can quickly gain insights of the\nworkingmechanismofthemodel. Thesystemisavailableathttp://bit.ly/wevi-online. The source code is available at http://github.com/ronxin/wevi. 20"
  },
  {
    "chunk_id": "doc_1_p21_fixed_0",
    "doc_id": "doc_1",
    "pdf_name": "1.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": "Figure 7: wevi screenshot (http://bit.ly/wevi-online)\n21"
  },
  {
    "chunk_id": "doc_2_p1_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederikP.Kingma* JimmyLeiBa∗\nUniversityofAmsterdam,OpenAI UniversityofToronto\ndpkingma@openai.com jimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for first-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. Themethodisstraightforwardtoimplement,iscomputationallyefficient,\nhaslittlememoryrequirements,isinvarianttodiagonalrescalingofthegradients,\nandiswellsuitedforproblemsthatarelargeintermsofdataand/orparameters. The method is also appropriate for non-stationary objectives and problems with\nverynoisyand/orsparsegradients. Thehyper-parametershaveintuitiveinterpre-\ntationsandtypicallyrequirelittletuning.Someconnectionstorelatedalgorithms,\nonwhichAdamwasinspired,arediscussed. Wealsoanalyzethetheoreticalcon-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimizationframework. EmpiricalresultsdemonstratethatAdamworkswellin\npracticeandcomparesfavorablytootherstochasticoptimizationmethods.Finally,\nwediscussAdaMax,avariantofAdambasedontheinfinitynorm."
  },
  {
    "chunk_id": "doc_2_p1_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "1 INTRODUCTION\nStochasticgradient-basedoptimizationisofcorepracticalimportanceinmanyfieldsofscienceand\nengineering.Manyproblemsinthesefieldscanbecastastheoptimizationofsomescalarparameter-\nizedobjectivefunctionrequiringmaximizationorminimizationwithrespecttoitsparameters.Ifthe\nfunctionisdifferentiablew.r.t. itsparameters,gradientdescentisarelativelyefficientoptimization\nmethod,sincethecomputationoffirst-orderpartialderivativesw.r.t.alltheparametersisofthesame\ncomputationalcomplexityasjustevaluatingthefunction. Often,objectivefunctionsarestochastic. Forexample,manyobjectivefunctionsarecomposedofasumofsubfunctionsevaluatedatdifferent\nsubsamples of data; in this case optimization can be made more efficient by taking gradient steps\nw.r.t. individualsubfunctions,i.e. stochasticgradientdescent(SGD)orascent. SGDproveditself\nasanefficientandeffectiveoptimizationmethodthatwascentralinmanymachinelearningsuccess\nstories,suchasrecentadvancesindeeplearning(Dengetal.,2013;Krizhevskyetal.,2012;Hinton\n&Salakhutdinov,2006;Hintonetal.,2012a;Gravesetal.,2013). Objectivesmayalsohaveother\nsourcesofnoisethandatasubsampling,suchasdropout(Hintonetal.,2012b)regularization."
  },
  {
    "chunk_id": "doc_2_p1_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "For\nallsuchnoisyobjectives,efficientstochasticoptimizationtechniquesarerequired.Thefocusofthis\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestrictedtofirst-ordermethods. WeproposeAdam,amethodforefficientstochasticoptimizationthatonlyrequiresfirst-ordergra-\ndientswithlittlememoryrequirement. Themethodcomputesindividualadaptivelearningratesfor\ndifferentparametersfromestimatesoffirstandsecondmomentsofthegradients; thenameAdam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\noftworecentlypopularmethods: AdaGrad(Duchietal.,2011),whichworkswellwithsparsegra-\ndients,andRMSProp(Tieleman&Hinton,2012),whichworkswellinon-lineandnon-stationary\nsettings; important connections to these and other stochastic optimization methods are clarified in\nsection5.SomeofAdam’sadvantagesarethatthemagnitudesofparameterupdatesareinvariantto\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nitdoesnotrequireastationaryobjective,itworkswithsparsegradients,anditnaturallyperformsa\nformofstepsizeannealing. ∗Equalcontribution.AuthororderingdeterminedbycoinflipoveraGoogleHangout. 1\n7102\nnaJ\n03\n]GL.sc[\n9v0896.2141:viXra"
  },
  {
    "chunk_id": "doc_2_p2_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nAlgorithm1:Adam,ourproposedalgorithmforstochasticoptimization. Seesection2fordetails,\nandforaslightlymoreefficient(butlessclear)orderofcomputation. g2 indicatestheelementwise\nt\nsquare g g . Good default settings for the tested machine learning problems are α = 0.001,\nt t\nβ = 0.9, (cid:12) β = 0.999and(cid:15) = 10 8. Alloperationsonvectorsareelement-wise. Withβt andβt\n1 2 − 1 2\nwedenoteβ andβ tothepowert. 1 2\nRequire: α: Stepsize\nRequire: β 1 ,β 2 [0,1): Exponentialdecayratesforthemomentestimates\n∈\nRequire: f(θ): Stochasticobjectivefunctionwithparametersθ\nRequire: θ 0 : Initialparametervector\nm 0(Initialize1stmomentvector)\n0\nv ← 0(Initialize2ndmomentvector)\n0\n←\nt 0(Initializetimestep)\n←\nwhileθ\nt\nnotconvergeddo\nt t+1\n←\ng f (θ )(Getgradientsw.r.t."
  },
  {
    "chunk_id": "doc_2_p2_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "stochasticobjectiveattimestept)\nt θ t t 1\nm ←∇ β m− +(1 β ) g (Updatebiasedfirstmomentestimate)\nt 1 t 1 1 t\nv ← β · v −+(1 − β ) g ·2(Updatebiasedsecondrawmomentestimate)\nm t ← m 2 · /( t 1− 1 βt)( − Com 2 p · ute t bias-correctedfirstmomentestimate)\nv t ← v / t (1 − βt) 1 (Computebias-correctedsecondrawmomentestimate)\nt ← t − 2\nθ θ α m /(√v +(cid:15))(Updateparameters)\nbt t 1 t t\n← − − ·\nendbwhile\nreturn θ t (Resultin b gparam b eters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nourinitializationbiascorrectiontechnique,andsection4providesatheoreticalanalysisofAdam’s\nconvergenceinonlineconvexprogramming.Empirically,ourmethodconsistentlyoutperformsother\nmethodsforavarietyofmodelsanddatasets,asshowninsection6. Overall,weshowthatAdamis\naversatilealgorithmthatscalestolarge-scalehigh-dimensionalmachinelearningproblems. 2 ALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ."
  },
  {
    "chunk_id": "doc_2_p2_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "With\nf (θ),...,,f (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1 T\n1,...,T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nofdatapoints,orarisefrominherentfunctionnoise. Withg = f (θ)wedenotethegradient,i.e. t θ t\n∇\nthevectorofpartialderivativesoff ,w.r.tθevaluatedattimestept. t\nThealgorithmupdatesexponentialmovingaveragesofthegradient(m )andthesquaredgradient\nt\n(v )wherethehyper-parametersβ ,β [0,1)controltheexponentialdecayratesofthesemoving\nt 1 2\naverages. The moving averages thems ∈ elves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitializedas(vectorsof)0’s,leadingtomomentestimatesthatarebiasedtowardszero,especially\nduringtheinitialtimesteps,andespeciallywhenthedecayratesaresmall(i.e.theβsarecloseto1). Thegoodnewsisthatthisinitializationbiascanbeeasilycounteracted,resultinginbias-corrected\nestimatesm andv . Seesection3formoredetails. t t\nNotethattheefficiencyofalgorithm1can,attheexpenseofclarity,beimproveduponbychanging\ntheorderobfcompbutation,e.g."
  },
  {
    "chunk_id": "doc_2_p2_fixed_3",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "byreplacingthelastthreelinesintheloopwiththefollowinglines:\nα =α 1 βt/(1 βt)andθ θ α m /(√v +(cid:15)ˆ). t · − 2 − 1 t ← t − 1 − t · t t\np\n2.1 ADAM’SUPDATERULE\nAnimportantpropertyofAdam’supdateruleisitscarefulchoiceofstepsizes.Assuming(cid:15)=0,the\neffectivesteptakeninparameterspaceattimesteptis∆ =α m /√v .Theeffectivestepsizehas\nt t t\n·\ntwoupperbounds: ∆ α (1 β )/√1 β inthecase(1 β ) > √1 β ,and ∆ α\nt 1 2 1 2 t\n| | ≤ · − − − − | | ≤\nb b\n2"
  },
  {
    "chunk_id": "doc_2_p3_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\notherwise. The first case only happens in the most severe case of sparsity: when a gradient has\nbeenzeroatalltimestepsexceptatthecurrenttimestep. Forlesssparsecases,theeffectivestepsize\nwill be smaller. When (1 β ) = √1 β we have that m /√v < 1 therefore ∆ < α. In\n1 2 t t t\n− − | | | |\nmorecommonscenarios,wewillhavethatm\nt\n/√v\nt\n1since E[g]/ E[g2] 1. Theeffective\n≈± | |≤\nmagnitude of the steps taken in parameter space at each timbestepbare approximately bounded by\np\nthestepsizesettingα,i.e., | ∆ t | / α. Thisc b anbe b understoodasestablishingatrustregionaround\nthecurrentparametervalue,beyondwhichthecurrentgradientestimatedoesnotprovidesufficient\ninformation. This typically makes it relatively easy to know the right scale of α in advance. For\nmanymachinelearningmodels,forinstance,weoftenknowinadvancethatgoodoptimaarewith\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since α sets (an upper bound of) the magnitude of\nstepsinparameterspace, wecanoftendeducetherightorderofmagnitudeofαsuchthatoptima\ncan be reached from θ within some number of iterations. With a slight abuse of terminology,\n0\nwe will call the ratio m /√v the signal-to-noise ratio (SNR)."
  },
  {
    "chunk_id": "doc_2_p3_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "With a smaller SNR the effective\nt t\nstepsize ∆ will be closer to zero. This is a desirable property, since a smaller SNR means that\nt\nthereisgreateruncertaintyaboutwhetherthedirectionofm correspondstothedirectionofthetrue\nb b t\ngradient. Forexample, theSNRvaluetypicallybecomescloserto0towardsanoptimum, leading\ntosmallereffectivestepsinparameterspace: aformofautomaticannealing. Theeffectivestepsize\nb\n∆ isalsoinvarianttothescaleofthegradients;rescalingthegradientsgwithfactorcwillscalem\nt t\nwithafactorcandv withafactorc2,whichcancelout: (c m )/(√c2 v )=m /√v . t t t t t\n· ·\nb\nb b b b b\n3 INITIALIZATION BIAS CORRECTION\nAs explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthetermforthesecondmomentestimate;thederivationforthefirstmomentestimateiscompletely\nanalogous. Letg bethegradientofthestochasticobjectivef, andwewishtoestimateitssecond\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate β . Let g ,...,g be the gradients at subsequent timesteps, each a draw from an\n2 1 T\nunderlying gradient distribution g p(g )."
  },
  {
    "chunk_id": "doc_2_p3_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Let us initialize the exponential moving average as\nt t\n∼\nv =0(avectorofzeros).Firstnotethattheupdateattimesteptoftheexponentialmovingaverage\n0\nv =β v +(1 β ) g2(whereg2indicatestheelementwisesquareg g )canbewrittenas\na t funct 2 io · no t −f 1 thegra − dien 2 ts · at t allpreviou t stimesteps: t (cid:12) t\nt\nv t =(1 − β 2 ) β 2 t − i · g i 2 (1)\ni=1\nX\nWe wish to know how E[v\nt\n], the expected value of the exponential moving average at timestep t,\nrelates to the true second moment E[g\nt\n2], so we can correct for the discrepancy between the two. Takingexpectationsoftheleft-handandright-handsidesofeq. (1):\nt\nE[v t ]=E \" (1 − β 2 ) β 2 t − i · g i 2 # (2)\ni=1\nX\nt\n=E[g t 2] · (1 − β 2 ) β 2 t − i+ζ (3)\ni=1\nX\n=E[g\nt\n2]\n·\n(1\n−\nβ\n2\nt)+ζ (4)\nwhere ζ = 0 if the true second moment E[g2] is stationary; otherwise ζ can be kept small since\ni\ntheexponentialdecayrateβ can(andshould)bechosensuchthattheexponentialmovingaverage\n1\nassigns small weights to gradients too far in the past. What is left is the term (1 βt) which is\n− 2\ncaused by initializing the running average with zeros."
  },
  {
    "chunk_id": "doc_2_p3_fixed_3",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "In algorithm 1 we therefore divide by this\ntermtocorrecttheinitializationbias. Incaseofsparsegradients,forareliableestimateofthesecondmomentoneneedstoaverageover\nmanygradientsbychosingasmallvalueofβ ;howeveritisexactlythiscaseofsmallβ wherea\n2 2\nlackofinitialisationbiascorrectionwouldleadtoinitialstepsthataremuchlarger. 3"
  },
  {
    "chunk_id": "doc_2_p4_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\n4 CONVERGENCE ANALYSIS\nWeanalyzetheconvergenceofAdamusingtheonlinelearningframeworkproposedin(Zinkevich,\n2003). Givenanarbitrary, unknownsequenceofconvexcostfunctionsf (θ), f (θ),..., f (θ). At\n1 2 T\neach time t, our goal is to predict the parameter θ and evaluate it on a previously unknown cost\nt\nfunction f . Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nt\nusingtheregret,thatisthesumofallthepreviousdifferencebetweentheonlinepredictionf (θ )\nt t\nandthebestfixedpointparameterf (θ )fromafeasibleset foralltheprevioussteps.Concretely,\nt ∗\nX\ntheregretisdefinedas:\nT\nR(T)= [f\nt\n(θ\nt\n) f\nt\n(θ∗)] (5)\n−\nt=1\nX\nwhereθ =argmin T f (θ).WeshowAdamhasO(√T)regretboundandaproofisgiven\n∗ θ t=1 t\nintheappendix. Our∈rXesultiscomparabletothebestknownboundforthisgeneralconvexonline\nP\nlearningproblem. Wealsousesomedefinitionssimplifyournotation,whereg t , f t (θ t )andg t,i\n∇\nastheith element. Wedefineg\n1:t,i\nRt asavectorthatcontainstheith dimensionofthegradients\n∈\nover all iterations till t, g 1:t,i = [g 1,i ,g 2,i , ··· ,g t,i ]."
  },
  {
    "chunk_id": "doc_2_p4_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Also, we define γ , √ β β 1 2 2 . Our following\ntheorem holds when the learning rate α t is decaying at a rate of t − 1 2 and first moment running\naveragecoefficientβ decayexponentiallywithλ,thatistypicallycloseto1,e.g. 1 10 8. 1,t −\n−\nTheorem4.1. Assumethatthefunctionf t hasboundedgradients, f t (θ) 2 G, f t (θ)\nG forallθ Rdanddistancebetweenanyθ generatedbyAdam k∇ isboun k ded ≤ , θ k∇ θ k∞D ≤ ,\nt n m 2\nθ ∞ θ ∈ D foranym,n 1,...,T ,andβ ,β [0,1)satisfy β 1 2 k < 1 − . Letα k ≤ = α\nk m − n k∞ ≤ ∞ ∈ { } 1 2 ∈ √β2 t √t\nandβ =β λt 1,λ (0,1). Adamachievesthefollowingguarantee,forallT 1."
  },
  {
    "chunk_id": "doc_2_p4_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "1,t 1 −\n∈ ≥\nD2 d α(1+β )G d d D2 G √1 β\nR(T) ≤ 2α(1 β ) Tv T,i + (1 β )√1 1 β ( ∞ 1 γ)2 k g 1:T,i k 2 + 2α( ∞ 1 ∞ β )(1 − λ 2 )2\n− 1 i=1 − 1 − 2 − i=1 i=1 − 1 −\nXp X X\nb\nOur Theorem 4.1 implies when the data features are sparse and bounded gradients, the sum-\nmation term can be much smaller than its upper bound d g << dG √T and\nd Tv <<dG √T,inparticulariftheclassoffunction i= an 1 d k da 1 t :T a , f i e k a 2 turesareint ∞ heformof\nP\nsec i t = io 1 n\np\n1.2i T n , ( i Duchiet ∞ al.,2011).Theirresultsfortheexpecte P dvalueE[ d\ni=1k\ng\n1:T,i k 2\n]alsoapply\ntoAdam. Ibnparticular,theadaptivemethod,suchasAdamandAdagrad,canachieveO(logd√T),\nP\nanimprovementoverO(√dT)forthenon-adaptivemethod. Decayingβ towardszeroisimpor-\n1,t\ntantinourtheoreticalanalysisandalsomatchespreviousempiricalfindings,e.g. (Sutskeveretal.,\n2013)suggestsreducingthemomentumcoefficientintheendoftrainingcanimproveconvergence."
  },
  {
    "chunk_id": "doc_2_p4_fixed_3",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Finally,wecanshowtheaverageregretofAdamconverges,\nCorollary4.2. Assumethatthefunctionf t hasboundedgradients, f t (θ) 2 G, f t (θ)\nG forallθ Rdanddistancebetweenanyθ generatedbyAdam k∇ isboun k ded ≤ , θ k∇ θ k∞D ≤ ,\nt n m 2\nθ∞ θ ∈ D for any m,n 1,...,T . Adam achieves the following k guar − antee k , fo ≤ r all\nm n\nk T − 1. k∞ ≤ ∞ ∈ { }\n≥ R(T) 1\n=O( )\nT √T\nThis result can be obtained by using Theorem 4.1 and d g dG √T. Thus,\nlim R(T) =0. i=1k 1:T,i k 2 ≤ ∞\nT →∞ T P\n5 RELATED WORK\nOptimizationmethodsbearingadirectrelationtoAdamareRMSProp(Tieleman&Hinton,2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochasticoptimizationmethodsincludevSGD(Schauletal.,2012),AdaDelta(Zeiler,2012)andthe\nnaturalNewtonmethodfromRoux&Fitzgibbon(2010),allsettingstepsizesbyestimatingcurvature\n4"
  },
  {
    "chunk_id": "doc_2_p5_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nfromfirst-orderinformation. TheSum-of-FunctionsOptimizer(SFO)(Sohl-Dicksteinetal.,2014)\nisaquasi-Newtonmethodbasedonminibatches,but(unlikeAdam)hasmemoryrequirementslinear\ninthenumberofminibatchpartitionsofadataset,whichisofteninfeasibleonmemory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditionerthatadaptstothegeometryofthedata,sincev isanapproximationtothediagonal\nt\noftheFisherinformationmatrix(Pascanu&Bengio,2013);however,Adam’spreconditioner(like\nAdaGrad’s)ismoreconservativeinitsadaptionthanvanillaNGDbypreconditioningwiththesquare\nb\nrootoftheinverseofthediagonalFisherinformationmatrixapproximation. RMSProp: An optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012).Aversionwithmomentumhassometimesbeenused(Graves,2013).Thereareafewimpor-\ntantdifferencesbetweenRMSPropwithmomentumandAdam: RMSPropwithmomentumgener-\natesitsparameterupdatesusingamomentumontherescaledgradient,whereasAdamupdatesare\ndirectly estimated using a running average of first and second moment of the gradient."
  },
  {
    "chunk_id": "doc_2_p5_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "RMSProp\nalsolacksabias-correctionterm; thismattersmostincaseofavalueofβ closeto1(requiredin\n2\ncaseofsparsegradients),sinceinthatcasenotcorrectingthebiasleadstoverylargestepsizesand\noftendivergence,aswealsoempiricallydemonstrateinsection6.4. AdaGrad: AnalgorithmthatworkswellforsparsegradientsisAdaGrad(Duchietal.,2011). Its\nbasicversionupdatesparametersasθ =θ α g / t g2.Notethatifwechooseβ tobe\nt+1 t − · t i=1 t 2\ninfinitesimallycloseto1frombelow,thenlim v =qt 1 t g2. AdaGradcorrespondstoa\nversionofAdamwithβ =0,infinitesimal(1\nβ2→β 1\n)a\nt\nnda\nP−\nrep\n·\nlace\ni\nm\n=1\nent\nt\nofαbyanannealedversion\n1 2\n− P\nα =α t 1/2,namelyθ α t 1/2 m / lim b v =θ α t 1/2 g / t 1 t g2 =\nt · − t − · − · t β2→ 1 t t − · − · t − · i=1 t\nθ α g / t g2. Note that this dirpect correspondence between Adam q and AdPagrad does\nt − · t i=1 t b b\nnotholdwheqnr\nP\nemovingthebias-correctionterms;withoutbiascorrection,likeinRMSProp,aβ\n2\ninfinitesimallycloseto1wouldleadtoinfinitelylargebias,andinfinitelylargeparameterupdates."
  },
  {
    "chunk_id": "doc_2_p5_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "6 EXPERIMENTS\nTo empirically evaluate the proposed method, we investigated different popular machine learning\nmodels,includinglogisticregression,multilayerfullyconnectedneuralnetworksanddeepconvolu-\ntionalneuralnetworks.Usinglargemodelsanddatasets,wedemonstrateAdamcanefficientlysolve\npracticaldeeplearningproblems. We use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresultsarereportedusingthebesthyper-parametersetting. 6.1 EXPERIMENT: LOGISTICREGRESSION\nWeevaluateourproposedmethodonL2-regularizedmulti-classlogisticregressionusingtheMNIST\ndataset. Logisticregressionhasawell-studiedconvexobjective,makingitsuitableforcomparison\nofdifferentoptimizerswithoutworryingaboutlocalminimumissues.Thestepsizeαinourlogistic\nregressionexperimentsisadjustedby1/√tdecay,namelyα = α thatmatcheswithourtheorat-\nt √t\nicalpredictionfromsection4. Thelogisticregressionclassifiestheclasslabeldirectlyonthe784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagradusingminibatchsizeof128.AccordingtoFigure1,wefoundthattheAdamyieldssimilar\nconvergenceasSGDwithmomentumandbothconvergefasterthanAdagrad."
  },
  {
    "chunk_id": "doc_2_p5_fixed_3",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "As discussed in (Duchi et al., 2011), Adagrad can efficiently deal with sparse features and gradi-\nentsasoneofitsmaintheoreticalresultswhereasSGDislowatlearningrarefeatures. Adamwith\n1/√tdecayonitsstepsizeshouldtheoraticallymatchtheperformanceofAdagrad.Weexaminethe\nsparsefeatureproblemusingIMDBmoviereviewdatasetfrom(Maasetal.,2011). Wepre-process\ntheIMDBmoviereviewsintobag-of-words(BoW)featurevectorsincludingthefirst10,000most\nfrequentwords.The10,000dimensionBoWfeaturevectorforeachreviewishighlysparse.Assug-\ngestedin(Wang&Manning,2013),50%dropoutnoisecanbeappliedtotheBoWfeaturesduring\n5"
  },
  {
    "chunk_id": "doc_2_p6_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2 0 5 10 15 20 25 30 35 40 45\niterations over entire dataset\ntsoc\ngniniart\nMNIST Logistic Regression\n0.50\nAdaGrad\nSGDNesterov\nAdam 0.45\n0.40\n0.35\n0.30\n0.25\n0.20 0 20 40 60 80 100 120 140 160\niterations over entire dataset\ntsoc\ngniniart\nIMDB BoW feature Logistic Regression\nAdagrad+dropout\nRMSProp+dropout\nSGDNesterov+dropout\nAdam+dropout\nFigure1: LogisticregressiontrainingnegativeloglikelihoodonMNISTimagesandIMDBmovie\nreviewswith10,000bag-of-words(BoW)featurevectors. training to prevent over-fitting. In figure 1, Adagrad outperforms SGD with Nesterov momentum\nbyalargemarginbothwithandwithoutdropoutnoise. AdamconvergesasfastasAdagrad. The\nempiricalperformanceofAdamisconsistentwithourtheoreticalfindingsinsections2and4. Sim-\nilartoAdagrad,Adamcantakeadvantageofsparsefeaturesandobtainfasterconvergenceratethan\nnormalSGDwithmomentum. 6.2 EXPERIMENT: MULTI-LAYERNEURALNETWORKS\nMulti-layer neural network are powerful models with non-convex objective functions."
  },
  {
    "chunk_id": "doc_2_p6_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Although\nourconvergenceanalysisdoesnotapplytonon-convexproblems,weempiricallyfoundthatAdam\noftenoutperformsothermethodsinsuchcases.Inourexperiments,wemademodelchoicesthatare\nconsistentwithpreviouspublicationsinthearea;aneuralnetworkmodelwithtwofullyconnected\nhidden layers with 1000 hidden units each and ReLU activation are used for this experiment with\nminibatchsizeof128. First, we study different optimizers using the standard deterministic cross-entropy objective func-\ntion with L weight decay on the parameters to prevent over-fitting. The sum-of-functions (SFO)\n2\nmethod(Sohl-Dicksteinetal.,2014)isarecentlyproposedquasi-Newtonmethodthatworkswith\nminibatches of data and has shown good performance on optimization of multi-layer neural net-\nworks. We used their implementation and compared with Adam to train such models. Figure 2\nshows that Adam makes faster progress in terms of both the number of iterations and wall-clock\ntime. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com-\nparedtoAdam,andhasamemoryrequirementthatislinearinthenumberminibatches. Stochasticregularizationmethods,suchasdropout,areaneffectivewaytopreventover-fittingand\noftenusedinpracticeduetotheirsimplicity. SFOassumesdeterministicsubfunctions,andindeed\nfailedtoconvergeoncostfunctionswithstochasticregularization. Wecomparetheeffectivenessof\nAdam to other stochastic first order methods on multi-layer neural networks trained with dropout\nnoise."
  },
  {
    "chunk_id": "doc_2_p6_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Figure2showsourresults;Adamshowsbetterconvergencethanothermethods. 6.3 EXPERIMENT: CONVOLUTIONALNEURALNETWORKS\nConvolutionalneuralnetworks(CNNs)withseverallayersofconvolution, poolingandnon-linear\nunitshaveshownconsiderablesuccessincomputervisiontasks.Unlikemostfullyconnectedneural\nnets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller\nlearningratefortheconvolutionlayersisoftenusedinpracticewhenapplyingSGD.Weshowthe\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nconvolutionfiltersand3x3maxpoolingwithstrideof2thatarefollowedbyafullyconnectedlayer\nof1000rectifiedlinearhiddenunits(ReLU’s).Theinputimagearepre-processedbywhitening,and\n6"
  },
  {
    "chunk_id": "doc_2_p7_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\n10-1\n10-2\n0 50 100 150 200\niterations over entire dataset\ntsoc\ngniniart\nMNIST Multilayer Neural Network + dropout\nAdaGrad\nRMSProp\nSGDNesterov\nAdaDelta\nAdam\n(a) (b)\nFigure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using\ndropoutstochasticregularization. (b)Neuralnetworkswithdeterministiccostfunction.Wecompare\nwiththesum-of-functions(SFO)optimizer(Sohl-Dicksteinetal.,2014)\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\niterations over entire dataset\ntsoc\ngniniart\nCIFAR10 ConvNet First 3 Epoches\nAdaGrad\nAdaGrad+dropout\n102\nSGDNesterov\nSGDNesterov+dropout 101\nAdam\nAdam+dropout\n100\n10-1\n10-2\n10-3\n10-4\n0 5 10 15 20 25 30 35 40 45\niterations over entire dataset\ntsoc\ngniniart\nCIFAR10 ConvNet\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\nFigure3:Convolutionalneuralnetworkstrainingcost.(left)Trainingcostforthefirstthreeepochs. (right)Trainingcostover45epochs. CIFAR-10withc64-c64-c128-1000architecture. dropoutnoiseisappliedtotheinputlayerandfullyconnectedlayer. Theminibatchsizeisalsoset\nto128similartopreviousexperiments."
  },
  {
    "chunk_id": "doc_2_p7_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "Interestingly,althoughbothAdamandAdagradmakerapidprogressloweringthecostintheinitial\nstage of the training, shown in Figure 3 (left), Adam and SGD eventually converge considerably\nfasterthanAdagradforCNNsshowninFigure3(right). Wenoticethesecondmomentestimatev\nt\nvanishestozerosafterafewepochsandisdominatedbythe(cid:15)inalgorithm1. Thesecondmoment\nestimateisthereforeapoorapproximationtothegeometryofthecostfunctioninCNNscomparing\nb\nto fully connected network from Section 6.2. Whereas, reducing the minibatch variance through\nthefirstmomentismoreimportantinCNNsandcontributestothespeed-up. Asaresult,Adagrad\nconverges much slower than others in this particular experiment. Though Adam shows marginal\nimprovementoverSGDwithmomentum,itadaptslearningratescalefordifferentlayersinsteadof\nhandpickingmanuallyasinSGD. 7"
  },
  {
    "chunk_id": "doc_2_p8_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nβ2=0.99 β2=0.999 β2=0.9999 β2=0.99 β2=0.999 β2=0.9999\nβ1=0\nβ1=0.9\nlog10(α)\n(a) after 10 epochs (b) after 100 epochs\nssoL\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter10epochs(left)and100epochs(right)ontheloss(y-axes)whenlearningaVariationalAuto-\nEncoder(VAE)(Kingma&Welling,2013),fordifferentsettingsofstepsizeα(x-axes)andhyper-\nparametersβ andβ . 1 2\n6.4 EXPERIMENT: BIAS-CORRECTIONTERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3. Discussedinsection5,removalofthebiascorrectiontermsresultsinaversionofRMSProp(Tiele-\nman & Hinton, 2012) with momentum. We vary the β and β when training a variational auto-\n1 2\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e."
  },
  {
    "chunk_id": "doc_2_p8_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "β [0,0.9] and\n1\n∈\nβ [0.99,0.999,0.9999],andlog (α) [ 5,..., 1].Valuesofβ closeto1,requiredforrobust-\n2 ∈ 10 ∈ − − 2\nnesstosparsegradients,resultsinlargerinitializationbias;thereforeweexpectthebiascorrection\ntermisimportantinsuchcasesofslowdecay,preventinganadverseeffectonoptimization. InFigure4,valuesβ closeto1indeedleadtoinstabilitiesintrainingwhennobiascorrectionterm\n2\nwaspresent,especiallyatfirstfewepochsofthetraining.Thebestresultswereachievedwithsmall\nvaluesof(1 β )andbiascorrection;thiswasmoreapparenttowardstheendofoptimizationwhen\n2\n−\ngradientstendstobecomesparserashiddenunitsspecializetospecificpatterns.Insummary,Adam\nperformedequalorbetterthanRMSProp,regardlessofhyper-parametersetting. 7 EXTENSIONS\n7.1 ADAMAX\nInAdam,theupdateruleforindividualweightsistoscaletheirgradientsinverselyproportionaltoa\n(scaled)L2normoftheirindividualcurrentandpastgradients.WecangeneralizetheL2normbased\nupdateruletoaLp normbasedupdaterule. Suchvariantsbecomenumericallyunstableforlarge\np. However, in the special case where we let p , a surprisingly simple and stable algorithm\nemerges;seealgorithm2. We’llnowderivethea → lgor ∞ ithm."
  },
  {
    "chunk_id": "doc_2_p8_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Let,incaseoftheLpnorm,thestepsize\nattimetbeinverselyproportionaltov1/p\n,where:\nt\nv =βpv +(1 βp)g p (6)\nt 2 t − 1 − 2 | t |\nt\n=(1 − β 2 p) β 2 p(t − i) ·| g i | p (7)\ni=1\nX\n8"
  },
  {
    "chunk_id": "doc_2_p9_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nAlgorithm2:AdaMax, avariantofAdambasedontheinfinitynorm. Seesection7.1fordetails. Good default settings for the tested machine learning problems are α = 0.002, β = 0.9 and\n1\nβ =0.999. Withβt wedenoteβ tothepowert. Here,(α/(1 βt))isthelearningratewiththe\n2 1 1 − 1\nbias-correctiontermforthefirstmoment. Alloperationsonvectorsareelement-wise. Require: α: Stepsize\nRequire: β 1 ,β 2 [0,1): Exponentialdecayrates\n∈\nRequire: f(θ): Stochasticobjectivefunctionwithparametersθ\nRequire: θ 0 : Initialparametervector\nm 0(Initialize1stmomentvector)\n0\n←\nu 0(Initializetheexponentiallyweightedinfinitynorm)\n0\n←\nt 0(Initializetimestep)\n←\nwhileθ\nt\nnotconvergeddo\nt t+1\n←\ng f (θ )(Getgradientsw.r.t."
  },
  {
    "chunk_id": "doc_2_p9_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "stochasticobjectiveattimestept)\nt θ t t 1\nm ←∇ β m− +(1 β ) g (Updatebiasedfirstmomentestimate)\nt 1 t 1 1 t\nu ← max · (β −u , − g )(U · pdatetheexponentiallyweightedinfinitynorm)\nt 2 t 1 t\nθ ← θ (α · /(−1 | βt| )) m /u (Updateparameters)\nt ← t − 1 − − 1 · t t\nendwhile\nreturn θ t (Resultingparameters)\nNotethatthedecaytermishereequivalentlyparameterisedasβp insteadofβ . Nowletp ,\n2 2 → ∞\nanddefineu =lim (v )1/p,then:\nt p t\n→∞\nt 1/p\nu t = p lim(v t )1/p = p lim (1 − β 2 p) β 2 p(t − i) ·| g i | p ! (8)\n→∞ →∞ i=1\nX\nt 1/p\n= p lim(1 − β 2 p)1/p β 2 p(t − i) ·| g i | p ! (9)\n→∞ i=1\nX\nt 1/p\np\n= p lim β 2 (t − i) ·| g i | !"
  },
  {
    "chunk_id": "doc_2_p9_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "(10)\n→∞ X i=1(cid:16) (cid:17)\n=max β 2 t − 1 | g 1 | ,β 2 t − 2 | g 2 | ,...,β 2 | g t − 1 | , | g t | (11)\nWhichcorrespondstotheremarkablysimp(cid:0)lerecursiveformula: (cid:1)\nu =max(β u , g ) (12)\nt 2 t 1 t\n· − | |\nwithinitialvalueu =0.Notethat,convenientlyenough,wedon’tneedtocorrectforinitialization\n0\nbias in this case. Also note that the magnitude of parameter updates has a simpler bound with\nAdaMaxthanAdam,namely: ∆ α.\nt\n| |≤\n7.2 TEMPORALAVERAGING\nSincethelastiterateisnoisyduetostochasticapproximation,bettergeneralizationperformanceis\noften achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging\n(Polyak&Juditsky,1992;Ruppert,1988)hasbeenshowntoimprovetheconvergenceofstandard\nSGD,whereθ¯ = 1 n θ .Alternatively,anexponentialmovingaverageovertheparameterscan\nt t k=1 k\nbeused, givinghigherweighttomorerecentparametervalues. Thiscanbetriviallyimplemented\nbyaddingonelinetoPtheinnerloopofalgorithms1and2:θ¯ β θ¯ +(1 β )θ ,withθ¯ =0. t 2 t 1 2 t 0\nInitalizationbiascanagainbecorrectedbytheestimatorθ = ← θ¯/(1 · − βt)."
  },
  {
    "chunk_id": "doc_2_p9_fixed_3",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "−\nt t − 2\nb\n8 CONCLUSION\nWehaveintroducedasimpleandcomputationallyefficientalgorithmforgradient-basedoptimiza-\ntionofstochasticobjectivefunctions.Ourmethodisaimedtowardsmachinelearningproblemswith\n9"
  },
  {
    "chunk_id": "doc_2_p10_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nlargedatasetsand/orhigh-dimensionalparameterspaces. Themethodcombinestheadvantagesof\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nandtheabilityofRMSProptodealwithnon-stationaryobjectives. Themethodisstraightforward\ntoimplementandrequireslittlememory. Theexperimentsconfirmtheanalysisontherateofcon-\nvergenceinconvexproblems.Overall,wefoundAdamtoberobustandwell-suitedtoawiderange\nofnon-convexoptimizationproblemsinthefieldmachinelearning. 9 ACKNOWLEDGMENTS\nThispaperwouldprobablynothaveexistedwithoutthesupportofGoogleDeepmind. Wewould\nliketogivespecialthankstoIvoDanihelka,andTomSchaulforcoiningthenameAdam.Thanksto\nKaiFanfromDukeUniversityforspottinganerrorintheoriginalAdaMaxderivation.Experiments\ninthisworkwerepartlycarriedoutontheDutchnationale-infrastructurewiththesupportofSURF\nFoundation. DiederikKingmaissupportedbytheGoogleEuropeanDoctorateFellowshipinDeep\nLearning. REFERENCES\nAmari,Shun-Ichi. Naturalgradientworksefficientlyinlearning. Neuralcomputation,10(2):251–276,1998. Deng,Li,Li,Jinyu,Huang,Jui-Ting,Yao,Kaisheng,Yu,Dong,Seide,Frank,Seltzer,Michael,Zweig,Geoff,\nHe,Xiaodong,Williams,Jason,etal."
  },
  {
    "chunk_id": "doc_2_p10_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "Recentadvancesindeeplearningforspeechresearchatmicrosoft. ICASSP2013,2013. Duchi,John,Hazan,Elad,andSinger,Yoram.Adaptivesubgradientmethodsforonlinelearningandstochastic\noptimization. TheJournalofMachineLearningResearch,12:2121–2159,2011. Graves,Alex. Generatingsequenceswithrecurrentneuralnetworks. arXivpreprintarXiv:1308.0850,2013. Graves,Alex,Mohamed,Abdel-rahman,andHinton,Geoffrey. Speechrecognitionwithdeeprecurrentneural\nnetworks. InAcoustics,SpeechandSignalProcessing(ICASSP),2013IEEEInternationalConferenceon,\npp.6645–6649.IEEE,2013. Hinton,G.E.andSalakhutdinov,R.R.Reducingthedimensionalityofdatawithneuralnetworks.Science,313\n(5786):504–507,2006. Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, GeorgeE, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew,Vanhoucke,Vincent,Nguyen,Patrick,Sainath,TaraN,etal. Deepneuralnetworksforacoustic\nmodelinginspeechrecognition: Thesharedviewsoffourresearchgroups. SignalProcessingMagazine,\nIEEE,29(6):82–97,2012a. Hinton,GeoffreyE,Srivastava,Nitish,Krizhevsky,Alex,Sutskever,Ilya,andSalakhutdinov,RuslanR."
  },
  {
    "chunk_id": "doc_2_p10_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "Im-\nprovingneuralnetworksbypreventingco-adaptationoffeaturedetectors. arXivpreprintarXiv:1207.0580,\n2012b. Kingma,DiederikPandWelling,Max.Auto-EncodingVariationalBayes.InThe2ndInternationalConference\nonLearningRepresentations(ICLR),2013. Krizhevsky, Alex, Sutskever, Ilya, andHinton, GeoffreyE. Imagenetclassificationwithdeepconvolutional\nneuralnetworks. InAdvancesinneuralinformationprocessingsystems,pp.1097–1105,2012. Maas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher. Learningwordvectorsforsentimentanalysis.InProceedingsofthe49thAnnualMeetingoftheAssociation\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142–150. Association for\nComputationalLinguistics,2011. Moulines, Eric and Bach, Francis R. Non-asymptotic analysis of stochastic approximation algorithms for\nmachinelearning. InAdvancesinNeuralInformationProcessingSystems,pp.451–459,2011. Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient for deep networks. arXiv preprint\narXiv:1301.3584,2013. Polyak,BorisTandJuditsky,AnatoliB.Accelerationofstochasticapproximationbyaveraging.SIAMJournal\nonControlandOptimization,30(4):838–855,1992. 10"
  },
  {
    "chunk_id": "doc_2_p11_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternationalConferenceonMachineLearning(ICML-10),pp.623–630,2010. Ruppert, David. Efficient estimations from a slowly convergent robbins-monro process. Technical report,\nCornellUniversityOperationsResearchandIndustrialEngineering,1988. Schaul,Tom,Zhang,Sixin,andLeCun,Yann. Nomorepeskylearningrates. arXivpreprintarXiv:1206.1106,\n2012. Sohl-Dickstein, Jascha, Poole, Ben, andGanguli, Surya. Fastlarge-scaleoptimizationbyunifyingstochas-\nticgradientandquasi-newtonmethods. InProceedingsofthe31stInternationalConferenceonMachine\nLearning(ICML-14),pp.604–612,2014. Sutskever,Ilya,Martens,James,Dahl,George,andHinton,Geoffrey. Ontheimportanceofinitializationand\nmomentumindeeplearning. InProceedingsofthe30thInternationalConferenceonMachineLearning\n(ICML-13),pp.1139–1147,2013. Tieleman,T.andHinton,G. Lecture6.5-RMSProp,COURSERA:NeuralNetworksforMachineLearning. Technicalreport,2012. Wang,SidaandManning,Christopher.Fastdropouttraining.InProceedingsofthe30thInternationalConfer-\nenceonMachineLearning(ICML-13),pp.118–126,2013."
  },
  {
    "chunk_id": "doc_2_p11_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "Zeiler,MatthewD. Adadelta:Anadaptivelearningratemethod. arXivpreprintarXiv:1212.5701,2012. Zinkevich,Martin. Onlineconvexprogrammingandgeneralizedinfinitesimalgradientascent. 2003. 11"
  },
  {
    "chunk_id": "doc_2_p12_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\n10 APPENDIX\n10.1 CONVERGENCEPROOF\nDefinition10.1. Afunctionf :Rd Risconvexifforallx,y Rd,forallλ [0,1],\n→ ∈ ∈\nλf(x)+(1 λ)f(y) f(λx+(1 λ)y)\n− ≥ −\nAlso,noticethataconvexfunctioncanbelowerboundedbyahyperplaneatitstangent. Lemma10.2. Ifafunctionf :Rd Risconvex,thenforallx,y Rd,\n→ ∈\nf(y) f(x)+ f(x)T(y x)\n≥ ∇ −\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructedbysubstitutingthehyperplanewiththeAdamupdaterules. Thefollowingtwolemmasareusedtosupportourmaintheorem.Wealsousesomedefinitionssim-\nplifyournotation,whereg t , f t (θ t )andg t,i astheithelement. Wedefineg 1:t,i Rtasavector\n∇ ∈\nthatcontainstheithdimensionofthegradientsoveralliterationstillt,g =[g ,g , ,g ]\n1:t,i 1,i 2,i t,i\n···\nLemma10.3. Letg t = f t (θ t )andg 1:t bedefinedasaboveandbounded, g t 2 G, g t\nG . Then,\n∇ k k ≤ k k∞ ≤\n∞\nT g2\nt,i 2G g\n1:T,i 2\ns t ≤ ∞k k\nt=1\nX\nProof."
  },
  {
    "chunk_id": "doc_2_p12_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "WewillprovetheinequalityusinginductionoverT. ThebasecaseforT =1,wehave g2 2G g . 1,i ≤ ∞k 1,i k 2\nq\nFortheinductivestep,\nT g t 2 ,i = T − 1 g t 2 ,i + g T 2 ,i\ns t s t s T\nt=1 t=1\nX X\ng2\n2G g + T,i\n1:T 1,i 2\n≤ ∞k − k s T\ng2\n=2G g 2 g2 + T,i\n∞ k 1:T,i k2− T s T\nq\nFrom, g 2 g2 + g T 4 ,i g 2 g2 , we can take square root of both side and\nk 1:T,i k2 − T,i 4 k g1:T,ik 2 2 ≥ k 1:T,i k2 − T,i\nhave,\ng2\ng 2 g2 g T,i\nk 1:T,i k2− T,i ≤k 1:T,i k 2 − 2 g\n1:T,i 2\nq k k\ng2\ng T,i\n1:T,i 2\n≤k k − 2 TG2\n∞\np\nRearrangetheinequalityandsubstitutethe g 2 g2 term,\nk 1:T,i k2− T,i\nq\ng2\nG g 2 g2 + T,i 2G g\n∞ k 1:T,i k2− T s T ≤ ∞k 1:T,i k 2\nq\n12"
  },
  {
    "chunk_id": "doc_2_p13_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nLemma10.4. Letγ , √ β β 1 2 2 . Forβ 1 ,β 2 ∈ [0,1)thatsatisfy √ β β 1 2 2 <1andboundedg t , k g t k 2 ≤ G,\ng G ,thefollowinginequalityholds\nt\nk k∞ ≤ ∞\nT m2 2 1\nt,i g\nt=1 tv t,i ≤ 1 − γ√1 − β 2 k 1:T,i k 2\nX b\np\nProof. Undertheassumption, √1 − β 2 tb 1 ."
  },
  {
    "chunk_id": "doc_2_p13_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "Wecanexpandthelastterminthesummation\n(1 − β 1 t)2 ≤ (1 − β1)2\nusingtheupdaterulesinAlgorithm1,\nT m2 t,i = T − 1 m2 t,i + 1 − β 2 T ( T k=1 (1 − β 1 )β 1 T − kg k,i )2\nX t=1 b tv t,i X t=1 b tv t,i ( p 1 − β 1 T)2 PT T j=1 (1 − β 2 )β 2 T − jg j 2 ,i\np b T − 1p m2 tb,i + 1 − β 2 T qT PT((1 − β 1 )β 1 T − kg k,i )2\n≤ X t=1 b tv t,i ( p 1 − β 1 T)2 k X =1 T T j=1 (1 − β 2 )β 2 T − jg j 2 ,i\nT − 1p m2 tb,i + 1 − β 2 T T T q ((1P − β 1 )β 1 T − kg k,i )2\n≤ X t=1 b tv t,i ( p 1 − β 1 T)2 k X =1 T(1 − β 2 )β 2 T − kg k 2 ,i\nT − 1p m2 tb,i + 1 − β 2 T (1 − q β 1 )2 T T β 1 2 T − k g\n≤ X t=1 b tv t,i ( p 1 − β 1 T)2 T(1 − β 2 ) k X =1 (cid:18) √β 2(cid:19) k k,i k 2\nT − 1pm2 tb,i + T p T γT − k g k,i 2\n≤ tv T(1 β ) k k\nX t=1 b t,i − 2 k X =1\np p\nb\nSimilarly,wecanupperboundtherestofthetermsinthesummation."
  },
  {
    "chunk_id": "doc_2_p13_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "T m2 t,i T k g t,i k 2 T − t tγj\ntv ≤ t(1 β )\nX t=1 b t,i X t=1 − 2 X j=0\np T p g T\nb k t,i k 2 tγj\n≤ t(1 β )\nX t=1 − 2 X j=0\np\nForγ <1,usingtheupperboundonthearithmetic-geometricseries, tγt < 1 :\nt (1 γ)2\n−\nT T TP\ng 1 g\nk t,i k 2 tγj k t,i k 2\nX t=1 t(1 − β 2 ) X j=0 ≤ (1 − γ)2√1 − β 2 X t=1 √t\np\nApplyLemma10.3,\nT m2 2G\nt=1 tv t, t i ,i ≤ (1 − γ)2√ ∞ 1 − β 2 k g 1:T,i k 2\nX b\np\nb\nTo simplify the notation, we define γ , β 1 2 . Intuitively, our following theorem holds when the\n√β2\nlearningrateα t isdecayingatarateoft − 1 2 andfirstmomentrunningaveragecoefficientβ 1,t decay\nexponentiallywithλ,thatistypicallycloseto1,e.g. 1 10 8. −\n−\nTheorem10.5. Assumethatthefunctionf t hasboundedgradients, f t (θ) 2 G, f t (θ)\nG forallθ Rdanddistancebetweenanyθ generatedbyAdam k∇ isboun k ded ≤ , θ k∇ θ k∞D ≤ ,\nt n m 2\n∞ ∈ k − k ≤\n13"
  },
  {
    "chunk_id": "doc_2_p14_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nθ θ D foranym,n 1,...,T ,andβ ,β [0,1)satisfy β 1 2 < 1. Letα = α\nk m − n k∞ ≤ ∞ ∈ { } 1 2 ∈ √β2 t √t\nandβ =β λt 1,λ (0,1). Adamachievesthefollowingguarantee,forallT 1. 1,t 1 −\n∈ ≥\nD2 d α(β +1)G d d D2 G √1 β\nR(T) ≤ 2α(1 β ) Tv T,i + (1 β )√ 1 1 β ( ∞ 1 γ)2 k g 1:T,i k 2 + 2α( ∞ 1 ∞ β )(1 − λ 2 )2\n− 1 i=1 − 1 − 2 − i=1 i=1 − 1 −\nXp X X\nb\nProof. UsingLemma10.2,wehave,\nd\nf t (θ t ) − f t (θ∗) ≤ g t T(θ t − θ∗)= g t,i (θ t,i − θ ,∗i )\ni=1\nX\nFromtheupdaterulespresentedinalgorithm1,\nθ =θ α m / v\nt+1 t t t t\n−\n=θ\nα\nt\npβ\n1,tm +\n(1\n−\nβ\n1,t\n)\ng\nt − 1 − bβ 1 t (cid:18) b√v t t − 1 √v t t (cid:19)\nWefocusontheith dimensionoftheparametervectorθ Rd."
  },
  {
    "chunk_id": "doc_2_p14_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "Subtractthescalarθ andsquare\nb t\n∈\nb ,∗i\nbothsidesoftheaboveupdaterule,wehave,\n2α β β ) m\n(θ t+1,i − θ ,∗i )2 =(θ t,i − θ ,∗i )2 − 1 − β t 1 t ( 1 v , t t ,i m t − 1,i +(1 − 1 v ,t t,i g t,i )(θ t,i − θ ,∗i )+α t 2( v t, t i ,i )2\nb\np p p\nWecanrearrangetheaboveequationandusbeYoung’sinequality,abb a2/2+b2/2.Also,itcbanbe\n≤\nshownthat v t,i = t j=1 (1 − β 2 )β 2 t − jg j 2 ,i / 1 − β 2 t ≤k g 1:t,i k 2 andβ 1,t ≤ β 1 ."
  },
  {
    "chunk_id": "doc_2_p14_fixed_2",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "Then\np q P p\nb (1 βt) v\ng t,i (θ t,i\n−\nθ ,∗i )=\n2α\n−\n(1\n1\nβ\nt,\n)\ni (θ t,i\n−\nθ ,∗t )2\n−\n(θ t+1,i\n−\nθ ,∗i )2\nt −p1,t (cid:18) (cid:19)\nb 1\nβ v4 m α (1 βt) v m\n+ (1 − 1 β ,t 1,t )√ t α − t 1 − ,i 1 (θ ,∗i− θ t,i )√α t − 1 v t 1 4 t − 1 1 , , i i + t 2(1 − − 1 β 1p,t ) t,i ( v t, t i ,i )2\nb − b b\n1 β p\n≤2α t ( β 1 − α β 1 ) (cid:18) m\n(θ\n2\nt,i\n−\nθ ,∗t )2\n− α\n(θ t+1,i\nm − 2\nθ b,∗i )2\n(cid:19) p\nv t,i +\n2α t − 1 (1\n1,\n−\nt\nβ 1,t )\nb(θ ,∗i− θ t,i )2\np\nv t\n−\n1,i\n+ 1 t − 1 t − 1,i + t t,i b b\n2(1 − β 1 ) v t − 1,i 2(1 − β 1 ) b v t,i\np p\nWeapplyLemma10.4totheaboveinequalityandderivetheregretboundbysummingacrossall\nb b\nthe dimensions for i 1,...,d in the upper bound of f (θ ) f (θ ) and the sequence of convex\nt t t ∗\n∈ −\nfunctionsfort 1,...,T:\n∈\nd 1 d T 1 v v\nR(T)\n≤ 2α (1 β )\n(θ 1,i\n−\nθ ,∗i )2 v 1,i +\n2(1 β )\n(θ t,i\n−\nθ ,∗i )2(\nα\nt,i\n− α\nt − 1,i )\nX i=1 1 − 1 p X i=1 X t=2 − 1 p b t p b t − 1\nβ αG\ndb\nαG\nd\n+ (1 β )√ 1 1 β ∞ (1 γ)2 k g 1:T,i k 2 + (1 β )√1 ∞ β (1 γ)2 k g 1:T,i k 2\n− 1 − 2 − i=1 − 1 − 2 − i=1\nX X\nd T\nβ\n+\n2α (1\n1,t\nβ )\n(θ ,∗i− θ t,i )2 v t,i\nt 1,t\ni=1t=1 −\nXX p\nb\n14"
  },
  {
    "chunk_id": "doc_2_p15_fixed_0",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_2_p15_fixed_1",
    "doc_id": "doc_2",
    "pdf_name": "2.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "PublishedasaconferencepaperatICLR2015\nFromtheassumption, θ θ D, θ θ D ,wehave:\nt ∗ 2 m n\nk − k ≤ k − k∞ ≤ ∞\nD2 d α(1+β )G d D2 d t β\nR(T) ≤2α(1 β ) Tv T,i + (1 β )√1 1 β ( ∞ 1 γ)2 k g 1:T,i k 2 + 2α ∞ (1 1 β ,t ) tv t,i\n− 1 i=1 − 1 − 2 − i=1 i=1t=1 − 1,t\nXp X XX p\nD2 d b α(1+β )G d b\n≤2α(1 β ) Tv T,i + (1 β )√1 1 β ( ∞ 1 γ)2 k g 1:T,i k 2\n− 1 i=1 − 1 − 2 − i=1\nXp X\nD2 G √1 β bd t β\n+ ∞ ∞ − 2 1,t √t\n2α (1 β )\n1,t\ni=1t=1 −\nXX\nWecanusearithmeticgeometricseriesupperboundforthelastterm:\nt t\nβ 1\n1,t √t λt\n−\n1√t\n(1 β ) ≤ (1 β )\n1,t 1\nt=1 − t=1 −\nX X\nt\n1\nλt\n−\n1t\n≤ (1 β )\n1\nt=1 −\nX\n1\n≤ (1 β )(1 λ)2\n1\n− −\nTherefore,wehavethefollowingregretbound:\nD2 d α(1+β )G d d D2 G √1 β\nR(T) ≤2α(1 β ) Tv T,i + (1 β )√1 1 β ( ∞ 1 γ)2 k g 1:T,i k 2 + 2 ∞ αβ ∞ (1 λ − )2 2\n− 1 i=1 − 1 − 2 − i=1 i=1 1 −\nXp X X\nb\n15"
  },
  {
    "chunk_id": "doc_3_p1_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "You Only Look Once:\nUnified, Real-Time Object Detection\nJosephRedmon∗,SantoshDivvala∗†,RossGirshick¶,AliFarhadi∗†\nUniversityofWashington∗,AllenInstituteforAI†,FacebookAIResearch¶\nhttp://pjreddie.com/yolo/\nAbstract\nPerson: 0.64\nWe present YOLO, a new approach to object detection. Horse: 0.28\nPriorworkonobjectdetectionrepurposesclassifierstoper- 1. Resize image. Dog: 0.30\n2. Run convolutional network. formdetection. Instead,weframeobjectdetectionasare- 3. Non-max suppression. gressionproblemtospatiallyseparatedboundingboxesand\nassociatedclassprobabilities. Asingleneuralnetworkpre- Figure 1: The YOLO Detection System. Processing images\ndicts bounding boxes and class probabilities directly from withYOLOissimpleandstraightforward. Oursystem(1)resizes\nfull images in one evaluation. Since the whole detection theinputimageto448×448,(2)runsasingleconvolutionalnet-\npipelineisasinglenetwork,itcanbeoptimizedend-to-end workontheimage,and(3)thresholdstheresultingdetectionsby\ndirectlyondetectionperformance. themodel’sconfidence. Our unified architecture is extremely fast. Our base\nYOLO model processes images in real-time at 45 frames\nmethodstofirstgeneratepotentialboundingboxesinanim-\npersecond. Asmallerversionofthenetwork, FastYOLO,\nageandthenrunaclassifierontheseproposedboxes."
  },
  {
    "chunk_id": "doc_3_p1_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "After\nprocesses an astounding 155 frames per second while\nclassification, post-processing is used to refine the bound-\nstill achieving double the mAP of other real-time detec-\ning boxes, eliminate duplicate detections, and rescore the\ntors.Comparedtostate-of-the-artdetectionsystems,YOLO\nboxesbasedonotherobjectsinthescene[13]. Thesecom-\nmakes more localization errors but is less likely to predict\nplex pipelines are slow and hard to optimize because each\nfalse positives on background. Finally, YOLO learns very\nindividualcomponentmustbetrainedseparately. generalrepresentationsofobjects. Itoutperformsotherde-\nWereframeobjectdetectionasasingleregressionprob-\ntectionmethods, includingDPMandR-CNN,whengener-\nlem, straight from image pixels to bounding box coordi-\nalizingfromnaturalimagestootherdomainslikeartwork. nates and class probabilities. Using our system, you only\nlookonce(YOLO)atanimagetopredictwhatobjectsare\npresentandwheretheyare. YOLO is refreshingly simple: see Figure 1. A sin-\n1.Introduction\ngle convolutional network simultaneously predicts multi-\nHumansglanceatanimageandinstantlyknowwhatob- pleboundingboxesandclassprobabilitiesforthoseboxes. jects are in the image, where they are, and how they inter- YOLO trains on full images and directly optimizes detec-\nact. The human visual system is fast and accurate, allow- tion performance."
  },
  {
    "chunk_id": "doc_3_p1_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "This unified model has several benefits\ningustoperformcomplextaskslikedrivingwithlittlecon- overtraditionalmethodsofobjectdetection. scious thought. Fast, accurate algorithms for object detec- First,YOLOisextremelyfast. Sinceweframedetection\ntion would allow computers to drive cars without special- asaregressionproblemwedon’tneedacomplexpipeline. ized sensors, enable assistive devices to convey real-time We simply run our neural network on a new image at test\nsceneinformationtohumanusers,andunlockthepotential time to predict detections. Our base network runs at 45\nforgeneralpurpose,responsiveroboticsystems. frames per second with no batch processing on a Titan X\nCurrent detection systems repurpose classifiers to per- GPU and a fast version runs at more than 150 fps. This\nform detection. To detect an object, these systems take a means we can process streaming video in real-time with\nclassifierforthatobjectandevaluateitatvariouslocations less than 25 milliseconds of latency. Furthermore, YOLO\nand scales in a test image. Systems like deformable parts achieves more than twice the mean average precision of\nmodels (DPM) use a sliding window approach where the otherreal-timesystems. Forademoofoursystemrunning\nclassifier is run at evenly spaced locations over the entire in real-time on a webcam please see our project webpage:\nimage[10]. http://pjreddie.com/yolo/. MorerecentapproacheslikeR-CNNuseregionproposal Second, YOLO reasons globally about the image when\n1\n6102\nyaM\n9\n]VC.sc[\n5v04620.6051:viXra"
  },
  {
    "chunk_id": "doc_3_p2_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "making predictions. Unlike sliding window and region onesetofclassprobabilitiespergridcell,regardlessofthe\nproposal-based techniques, YOLO sees the entire image numberofboxesB. duringtrainingandtesttimesoitimplicitlyencodescontex- Attesttimewemultiplytheconditionalclassprobabili-\ntual information about classes as well as their appearance. tiesandtheindividualboxconfidencepredictions,\nFast R-CNN, a top detection method [14], mistakes back-\ntruth truth\ngroundpatchesinanimageforobjectsbecauseitcan’tsee\nPr(Classi|Object)∗Pr(Object)∗IOUpred =Pr(Classi)∗IOUpred (1)\nthelargercontext. YOLOmakeslessthanhalfthenumber which gives us class-specific confidence scores for each\nofbackgrounderrorscomparedtoFastR-CNN. box. Thesescoresencodeboththeprobabilityofthatclass\nThird,YOLOlearnsgeneralizablerepresentationsofob- appearingintheboxandhowwellthepredictedboxfitsthe\njects. When trained on natural images and tested on art- object. work,YOLOoutperformstopdetectionmethodslikeDPM\nandR-CNNbyawidemargin. SinceYOLOishighlygen-\neralizable it is less likely to break down when applied to\nnewdomainsorunexpectedinputs. YOLOstilllagsbehindstate-of-the-artdetectionsystems\nin accuracy. While it can quickly identify objects in im-\nages it struggles to precisely localize some objects, espe- Bounding boxes + confidence\nciallysmallones."
  },
  {
    "chunk_id": "doc_3_p2_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Weexaminethesetradeoffsfurtherinour\nexperiments. All of our training and testing code is open source. A S × S grid on input Final detections\nvarietyofpretrainedmodelsarealsoavailabletodownload. 2.UnifiedDetection Class probability map\nWe unify the separate components of object detection\nFigure2: TheModel.Oursystemmodelsdetectionasaregres-\ninto a single neural network. Our network uses features\nsionproblem.ItdividestheimageintoanS×Sgridandforeach\nfromtheentireimagetopredicteachboundingbox. Italso\ngridcellpredictsB boundingboxes,confidenceforthoseboxes,\npredicts all bounding boxes across all classes for an im-\nand C class probabilities. These predictions are encoded as an\nagesimultaneously. Thismeansournetworkreasonsglob-\nS×S×(B∗5+C)tensor. ally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-\nForevaluatingYOLOon PASCALVOC,weuseS = 7,\ntimespeedswhilemaintaininghighaverageprecision. B = 2. PASCAL VOChas20labelledclassessoC = 20. OursystemdividestheinputimageintoanS ×S grid. Ourfinalpredictionisa7×7×30tensor. Ifthecenterofanobjectfallsintoagridcell,thatgridcell\n2.1.NetworkDesign\nisresponsiblefordetectingthatobject. EachgridcellpredictsBboundingboxesandconfidence Weimplementthismodelasaconvolutionalneuralnet-\nscoresforthoseboxes."
  },
  {
    "chunk_id": "doc_3_p2_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Theseconfidencescoresreflecthow workandevaluateitonthePASCALVOCdetectiondataset\nconfident the model is that the box contains an object and [9]. Theinitialconvolutionallayersofthenetworkextract\nalso how accurate it thinks the box is that it predicts. For- features from the image while the fully connected layers\nmallywedefineconfidenceasPr(Object)∗IOUt p r r u e t d h. Ifno predicttheoutputprobabilitiesandcoordinates. object exists in that cell, the confidence scores should be Our network architecture is inspired by the GoogLeNet\nzero. Otherwisewewanttheconfidencescoretoequalthe model for image classification [34]. Our network has 24\nintersection over union (IOU) between the predicted box convolutional layers followed by 2 fully connected layers. andthegroundtruth. Instead of the inception modules used by GoogLeNet, we\nEachboundingboxconsistsof5predictions: x,y,w,h, simplyuse1×1reductionlayersfollowedby3×3convo-\nandconfidence. The(x,y)coordinatesrepresentthecenter lutionallayers,similartoLinetal[22]. Thefullnetworkis\noftheboxrelativetotheboundsofthegridcell. Thewidth showninFigure3. andheightarepredictedrelativetothewholeimage.Finally We also train a fast version of YOLO designed to push\nthe confidence prediction represents the IOU between the the boundaries of fast object detection. Fast YOLO uses a\npredictedboxandanygroundtruthbox."
  },
  {
    "chunk_id": "doc_3_p2_fixed_3",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "neural network with fewer convolutional layers (9 instead\nEach grid cell also predicts C conditional class proba- of24)andfewerfiltersinthoselayers. Otherthanthesize\nbilities, Pr(Class |Object). These probabilities are condi- of the network, all training and testing parameters are the\ni\ntionedonthegridcellcontaininganobject.Weonlypredict samebetweenYOLOandFastYOLO."
  },
  {
    "chunk_id": "doc_3_p3_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "448\n7\n7\n112\n3\n56\n3 3\n448 3 283\n3 143 73 7 7\n112 56 28 14 3 7 3 7 7\n3 192 256 512 1024 1024 1024 4096 30\nConv. Layer Conv. Layer Conv. Layers Conv. Layers Conv. Layers Conv. Layers Conn. Layer Conn. Layer\n7x7x64-s-2 3x3x192 1x1x128 1x1x256}×4 1x1x512 }×2 3x3x1024\nMaxpool Layer Maxpool Layer 3x3x256 3x3x512 3x3x1024 3x3x1024\n2x2-s-2 2x2-s-2 1x1x256 1x1x512 3x3x1024\n3x3x512 3x3x1024 3x3x1024-s-2\nMaxpool Layer Maxpool Layer\n2x2-s-2 2x2-s-2\nFigure3: TheArchitecture.Ourdetectionnetworkhas24convolutionallayersfollowedby2fullyconnectedlayers.Alternating1×1\nconvolutionallayersreducethefeaturesspacefromprecedinglayers. WepretraintheconvolutionallayersontheImageNetclassification\ntaskathalftheresolution(224×224inputimage)andthendoubletheresolutionfordetection. Thefinaloutputofournetworkisthe7×7×30tensor model. Weusesum-squarederrorbecauseitiseasytoop-\nofpredictions. timize,howeveritdoesnotperfectlyalignwithourgoalof\nmaximizing average precision."
  },
  {
    "chunk_id": "doc_3_p3_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "It weights localization er-\n2.2.Training\nrorequallywithclassificationerrorwhichmaynotbeideal. We pretrain our convolutional layers on the ImageNet Also, in every image many grid cells do not contain any\n1000-classcompetitiondataset[30]. Forpretrainingweuse object. This pushes the “confidence” scores of those cells\nthefirst20convolutionallayersfromFigure3followedbya towards zero, often overpowering the gradient from cells\naverage-poolinglayerandafullyconnectedlayer. Wetrain that do contain objects. This can lead to model instability,\nthisnetworkforapproximatelyaweekandachieveasingle causingtrainingtodivergeearlyon. croptop-5accuracyof88%ontheImageNet2012valida-\ntion set, comparable to the GoogLeNet models in Caffe’s Toremedythis,weincreasethelossfromboundingbox\nModel Zoo [24]. We use the Darknet framework for all coordinate predictions and decrease the loss from confi-\ntrainingandinference[26]. dencepredictionsforboxesthatdon’tcontainobjects. We\nWethenconvertthemodeltoperformdetection. Renet usetwoparameters,λ andλ toaccomplishthis.We\ncoord noobj\nal. showthataddingbothconvolutionalandconnectedlay- setλ =5andλ =.5. coord noobj\ners to pretrained networks can improve performance [29]."
  },
  {
    "chunk_id": "doc_3_p3_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Following their example, we add four convolutional lay-\nSum-squared error also equally weights errors in large\nersandtwofullyconnectedlayerswithrandomlyinitialized\nboxesandsmallboxes. Ourerrormetricshouldreflectthat\nweights. Detectionoftenrequiresfine-grainedvisualinfor-\nsmall deviations in large boxes matter less than in small\nmation so we increase the input resolution of the network\nboxes. To partially address this we predict the square root\nfrom224×224to448×448. oftheboundingboxwidthandheightinsteadofthewidth\nOur final layer predicts both class probabilities and\nandheightdirectly. boundingboxcoordinates. Wenormalizetheboundingbox\nwidthandheightbytheimagewidthandheightsothatthey\nfall between 0 and 1. We parametrize the bounding box x YOLO predicts multiple bounding boxes per grid cell. andycoordinatestobeoffsetsofaparticulargridcellloca- Attrainingtimeweonlywantoneboundingboxpredictor\nto be responsible for each object. We assign one predictor\ntionsotheyarealsoboundedbetween0and1. tobe“responsible”forpredictinganobjectbasedonwhich\nWeusealinearactivationfunctionforthefinallayerand\nprediction has the highest current IOU with the ground\nallotherlayersusethefollowingleakyrectifiedlinearacti-\ntruth.Thisleadstospecializationbetweentheboundingbox\nvation:\npredictors. Each predictor gets better at predicting certain\n(cid:40)\nx, ifx>0 sizes, aspect ratios, or classes of object, improving overall\nφ(x)= (2) recall."
  },
  {
    "chunk_id": "doc_3_p3_fixed_3",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "0.1x, otherwise\nWe optimize for sum-squared error in the output of our During training we optimize the following, multi-part"
  },
  {
    "chunk_id": "doc_3_p4_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "lossfunction: theborderofmultiplecellscanbewelllocalizedbymulti-\nplecells.Non-maximalsuppressioncanbeusedtofixthese\nλcoord (cid:88) S2 (cid:88) B 1o\ni\nb\nj\nj(cid:104) (xi−xˆi)2+(yi−yˆi)2(cid:105) m\nis\nu\nf\nl\no\nti\nr\np\nR\nle\n-C\nde\nN\nte\nN\nct\no\nio\nr\nn\nD\ns.\nP\nW\nM\nh\n,\ni\nn\nle\non\nn\n-\no\nm\nt\na\nc\nx\nri\ni\nt\nm\nic\na\na\nl\nl\ns\nto\nup\np\np\ne\nr\nr\ne\nf\ns\no\ns\nr\ni\nm\non\nan\na\nc\nd\ne\nd\na\ns\ns\n2\ni\n-\nt\ni=0j=0\n+λcoord (cid:88) S2 (cid:88) B 1o i b j j (cid:20)(cid:16)√ wi− (cid:112) wˆi (cid:17)2 + (cid:18) (cid:112) hi− (cid:113) hˆ i (cid:19)2(cid:21) 3%inmAP. i=0j=0 2.4.LimitationsofYOLO\n+ (cid:88) S2 (cid:88) B 1o i b j j(cid:16) Ci−Cˆ i (cid:17)2 YOLO imposes strong spatial constraints on bounding\ni=0j=0\nboxpredictionssinceeachgridcellonlypredictstwoboxes\n+λnoobj (cid:88) S2 (cid:88) B 1n i o j obj(cid:16) Ci−Cˆ i (cid:17)2 and can only have one class."
  },
  {
    "chunk_id": "doc_3_p4_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "This spatial constraint lim-\ni=0j=0 its the number of nearby objects that our model can pre-\nS2 dict. Ourmodelstruggleswithsmallobjectsthatappearin\n+ (cid:88)1o i bj (cid:88) (pi(c)−pˆi(c))2 (3) groups,suchasflocksofbirds. i=0 c∈classes\nSince our model learns to predict bounding boxes from\nwhere 1obj denotes if object appears in cell i and 1obj de- data,itstrugglestogeneralizetoobjectsinneworunusual\ni ij\nnotes that the jth bounding box predictor in cell i is “re- aspect ratios or configurations. Our model also uses rela-\nsponsible”forthatprediction. tively coarse features for predicting bounding boxes since\nNote that the loss function only penalizes classification ourarchitecturehasmultipledownsamplinglayersfromthe\nerrorifanobjectispresentinthatgridcell(hencethecon- inputimage. ditionalclassprobabilitydiscussedearlier). Italsoonlype- Finally, while we train on a loss function that approxi-\nnalizes bounding box coordinate error if that predictor is matesdetectionperformance,ourlossfunctiontreatserrors\n“responsible”forthegroundtruthbox(i.e. hasthehighest the same in small bounding boxes versus large bounding\nIOUofanypredictorinthatgridcell). boxes. Asmallerrorinalargeboxisgenerallybenignbuta\nWetrainthenetworkforabout135epochsonthetrain- smallerrorinasmallboxhasamuchgreatereffectonIOU."
  },
  {
    "chunk_id": "doc_3_p4_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "ing and validation data sets from PASCAL VOC 2007 and Ourmainsourceoferrorisincorrectlocalizations. 2012. Whentestingon2012wealsoincludetheVOC2007\ntest data for training. Throughout training we use a batch 3.ComparisontoOtherDetectionSystems\nsizeof64,amomentumof0.9andadecayof0.0005. Our learning rate schedule is as follows: For the first Object detection is a core problem in computer vision. epochsweslowlyraisethelearningratefrom10−3to10−2. Detection pipelines generally start by extracting a set of\nIfwestartatahighlearningrateourmodeloftendiverges robust features from input images (Haar [25], SIFT [23],\nduetounstablegradients. Wecontinuetrainingwith10−2 HOG [4], convolutional features [6]). Then, classifiers\nfor 75 epochs, then 10−3 for 30 epochs, and finally 10−4 [36, 21, 13, 10] or localizers [1, 32] are used to identify\nfor30epochs. objects in the feature space. These classifiers or localizers\nTo avoid overfitting we use dropout and extensive data areruneitherinslidingwindowfashionoverthewholeim-\naugmentation. Adropoutlayerwithrate=.5afterthefirst ageoronsomesubsetofregionsintheimage[35,15,39]. connectedlayerpreventsco-adaptationbetweenlayers[18]."
  },
  {
    "chunk_id": "doc_3_p4_fixed_3",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "WecomparetheYOLOdetectionsystemtoseveraltopde-\nFor data augmentation we introduce random scaling and tectionframeworks,highlightingkeysimilaritiesanddiffer-\ntranslations of up to 20% of the original image size. We ences. alsorandomlyadjusttheexposureandsaturationoftheim- Deformable parts models. Deformable parts models\nagebyuptoafactorof1.5intheHSVcolorspace. (DPM) use a sliding window approach to object detection\n[10]. DPMusesadisjointpipelinetoextractstaticfeatures,\n2.3.Inference\nclassify regions, predict bounding boxes for high scoring\nJustlikeintraining,predictingdetectionsforatestimage regions,etc.Oursystemreplacesallofthesedisparateparts\nonlyrequiresonenetworkevaluation.OnPASCALVOCthe with a single convolutional neural network. The network\nnetwork predicts 98 bounding boxes per image and class performsfeatureextraction,boundingboxprediction,non-\nprobabilities for each box. YOLO is extremely fast at test maximalsuppression,andcontextualreasoningallconcur-\ntimesinceitonlyrequiresasinglenetworkevaluation,un- rently. Insteadofstaticfeatures,thenetworktrainsthefea-\nlikeclassifier-basedmethods. turesin-lineandoptimizesthemforthedetectiontask. Our\nThe grid design enforces spatial diversity in the bound- unified architecture leads to a faster, more accurate model\ning box predictions. Often it is clear which grid cell an thanDPM."
  },
  {
    "chunk_id": "doc_3_p4_fixed_4",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "objectfallsintoandthenetworkonlypredictsoneboxfor R-CNN.R-CNNanditsvariantsuseregionproposalsin-\neach object. However, some large objects or objects near steadofslidingwindowstofindobjectsinimages.Selective"
  },
  {
    "chunk_id": "doc_3_p5_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "Search[35]generatespotentialboundingboxes,aconvolu- graspdetectionbyRedmonetal[27]. Ourgridapproachto\ntionalnetworkextractsfeatures,anSVMscorestheboxes,a boundingboxpredictionisbasedontheMultiGraspsystem\nlinearmodeladjuststheboundingboxes,andnon-maxsup- forregressiontograsps.However,graspdetectionisamuch\npressioneliminatesduplicatedetections. Eachstageofthis simpler task than object detection. MultiGrasp only needs\ncomplex pipeline must be precisely tuned independently topredictasinglegraspableregionforanimagecontaining\nandtheresultingsystemisveryslow, takingmorethan40 one object. It doesn’t have to estimate the size, location,\nsecondsperimageattesttime[14]. orboundariesoftheobjectorpredictit’sclass,onlyfinda\nYOLOsharessomesimilaritieswithR-CNN.Eachgrid regionsuitableforgrasping. YOLOpredictsbothbounding\ncell proposes potential bounding boxes and scores those boxesandclassprobabilitiesformultipleobjectsofmulti-\nboxes using convolutional features. However, our system pleclassesinanimage. puts spatial constraints on the grid cell proposals which\n4.Experiments\nhelps mitigate multiple detections of the same object. Our\nsystem also proposes far fewer bounding boxes, only 98\nFirst we compare YOLO with other real-time detection\nper image compared to about 2000 from Selective Search. systems on PASCAL VOC 2007."
  },
  {
    "chunk_id": "doc_3_p5_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "To understand the differ-\nFinally, our system combines these individual components\nencesbetweenYOLOandR-CNNvariantsweexplorethe\nintoasingle,jointlyoptimizedmodel. errorsonVOC2007madebyYOLOandFastR-CNN,one\nOtherFastDetectorsFastandFasterR-CNNfocuson\nof the highest performing versions of R-CNN [14]. Based\nspeeding up the R-CNN framework by sharing computa-\non the different error profiles we show that YOLO can be\ntion and using neural networks to propose regions instead\nused to rescore Fast R-CNN detections and reduce the er-\nof Selective Search [14] [28]. While they offer speed and\nrors from background false positives, giving a significant\naccuracyimprovementsoverR-CNN,bothstillfallshortof\nperformanceboost. WealsopresentVOC2012resultsand\nreal-timeperformance. compare mAP to current state-of-the-art methods. Finally,\nMany research efforts focus on speeding up the DPM\nweshowthatYOLOgeneralizestonewdomainsbetterthan\npipeline [31] [38] [5]. They speed up HOG computation,\notherdetectorsontwoartworkdatasets. use cascades, and push computation to GPUs. However,\nonly30HzDPM[31]actuallyrunsinreal-time. 4.1.ComparisontoOtherReal-TimeSystems\nInstead of trying to optimize individual components of\nManyresearcheffortsinobjectdetectionfocusonmak-\na large detection pipeline, YOLO throws out the pipeline\ningstandarddetectionpipelinesfast. [5][38][31][14][17]\nentirelyandisfastbydesign. [28] However, only Sadeghi et al."
  },
  {
    "chunk_id": "doc_3_p5_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "actually produce a de-\nDetectors for single classes like faces or people can be tectionsystemthatrunsinreal-time(30framespersecond\nhighly optimized since they have to deal with much less or better) [31]. We compare YOLO to their GPU imple-\nvariation [37]. YOLO is a general purpose detector that mentation of DPM which runs either at 30Hz or 100Hz. learnstodetectavarietyofobjectssimultaneously. While the other efforts don’t reach the real-time milestone\nDeepMultiBox. UnlikeR-CNN,Szegedyetal. traina we also compare their relative mAP and speed to examine\nconvolutional neural network to predict regions of interest the accuracy-performance tradeoffs available in object de-\n[8] instead of using Selective Search. MultiBox can also tectionsystems. performsingleobjectdetectionbyreplacingtheconfidence Fast YOLO is the fastest object detection method on\nprediction with a single class prediction. However, Multi- PASCAL; as far as we know, it is the fastest extant object\nBoxcannotperformgeneralobjectdetectionandisstilljust detector.With52.7%mAP,itismorethantwiceasaccurate\napieceinalargerdetectionpipeline, requiringfurtherim- aspriorworkonreal-timedetection.YOLOpushesmAPto\nage patch classification. Both YOLO and MultiBox use a 63.4%whilestillmaintainingreal-timeperformance. convolutionalnetworktopredictboundingboxesinanim- WealsotrainYOLOusingVGG-16. Thismodelismore\nagebutYOLOisacompletedetectionsystem."
  },
  {
    "chunk_id": "doc_3_p5_fixed_3",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "accuratebutalsosignificantlyslowerthanYOLO.Itisuse-\nOverFeat. Sermanet et al. train a convolutional neural ful for comparison to other detection systems that rely on\nnetwork to perform localization and adapt that localizer to VGG-16butsinceitisslowerthanreal-timetherestofthe\nperformdetection[32]. OverFeatefficientlyperformsslid- paperfocusesonourfastermodels. ingwindowdetectionbutitisstilladisjointsystem. Over- Fastest DPM effectively speeds up DPM without sacri-\nFeatoptimizesforlocalization, notdetectionperformance. ficing much mAP but it still misses real-time performance\nLike DPM, the localizer only sees local information when byafactorof2[38]. ItalsoislimitedbyDPM’srelatively\nmakingaprediction. OverFeatcannotreasonaboutglobal lowaccuracyondetectioncomparedtoneuralnetworkap-\ncontextandthusrequiressignificantpost-processingtopro- proaches. ducecoherentdetections. R-CNN minus R replaces Selective Search with static\nMultiGrasp. Our work is similar in design to work on boundingboxproposals[20]. Whileitismuchfasterthan"
  },
  {
    "chunk_id": "doc_3_p6_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Real-TimeDetectors Train mAP FPS Fast R-CNN YOLO\n100HzDPM[31] 2007 16.0 100\n30HzDPM[31] 2007 26.1 30 Background: 13.6% Background: 4.75%\nOther: 4.0%\nFastYOLO 2007+2012 52.7 155\nOther: 1.9% Sim: 6.75%\nYOLO 2007+2012 63.4 45\nSim: 4.3%\nLessThanReal-Time\nFastestDPM[38] 2007 30.4 15 Loc: 8.6% Loc: 19.0%\nR-CNNMinusR[20] 2007 53.5 6\nFastR-CNN[14] 2007+2012 70.0 0.5 Correct: 71.6% Correct: 65.5%\nFasterR-CNNVGG-16[28] 2007+2012 73.2 7\nFasterR-CNNZF[28] 2007+2012 62.1 18\nYOLOVGG-16 2007+2012 66.4 21 Figure 4: Error Analysis: Fast R-CNN vs. YOLO These\nchartsshowthepercentageoflocalizationandbackgrounderrors\ninthetopNdetectionsforvariouscategories(N=#objectsinthat\nTable1: Real-TimeSystemsonPASCALVOC2007. Compar- category). ing the performance and speed of fast detectors. Fast YOLO is\nthe fastest detector on record for PASCAL VOC detection and is\nstill twice as accurate as any other real-time detector. YOLO is • Other: classiswrong,IOU>.1\n10mAPmoreaccuratethanthefastversionwhilestillwellabove • Background: IOU<.1foranyobject\nreal-timeinspeed. Figure 4 shows the breakdown of each error type aver-\nagedacrossall20classes."
  },
  {
    "chunk_id": "doc_3_p6_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "R-CNN,itstillfallsshortofreal-timeandtakesasignificant YOLOstrugglestolocalizeobjectscorrectly. Localiza-\naccuracyhitfromnothavinggoodproposals. tionerrorsaccountformoreofYOLO’serrorsthanallother\nFastR-CNNspeedsuptheclassificationstageofR-CNN sources combined. Fast R-CNN makes much fewer local-\nbutitstillreliesonselectivesearchwhichcantakearound ization errors but far more background errors. 13.6% of\n2 seconds per image to generate bounding box proposals. it’stopdetectionsarefalsepositivesthatdon’tcontainany\nThusithashighmAPbutat0.5fpsitisstillfarfromreal- objects. Fast R-CNN is almost 3x more likely to predict\ntime. backgrounddetectionsthanYOLO. TherecentFasterR-CNNreplacesselectivesearchwith\n4.3.CombiningFastR-CNNandYOLO\na neural network to propose bounding boxes, similar to\nSzegedy et al. [8] In our tests, their most accurate model YOLO makes far fewer background mistakes than Fast\nachieves 7 fps while a smaller, less accurate one runs at R-CNN. By using YOLO to eliminate background detec-\n18fps. TheVGG-16 versionof FasterR-CNN is10 mAP tionsfromFastR-CNNwegetasignificantboostinperfor-\nhigher but is also 6 times slower than YOLO. The Zeiler- mance. For every bounding box that R-CNN predicts we\nFergusFasterR-CNNisonly2.5timesslowerthanYOLO checktoseeifYOLOpredictsasimilarbox."
  },
  {
    "chunk_id": "doc_3_p6_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Ifitdoes,we\nbutisalsolessaccurate. give that prediction a boost based on the probability pre-\ndictedbyYOLOandtheoverlapbetweenthetwoboxes. 4.2.VOC2007ErrorAnalysis\nThe best Fast R-CNN model achieves a mAP of 71.8%\nTo further examine the differences between YOLO and ontheVOC2007testset. WhencombinedwithYOLO,its\nstate-of-the-art detectors, we look at a detailed breakdown\nof results on VOC 2007. We compare YOLO to Fast R- mAP Combined Gain\nCNN since Fast R-CNN is one of the highest performing FastR-CNN 71.8 - -\ndetectorson PASCAL andit’sdetectionsarepubliclyavail- FastR-CNN(2007data) 66.9 72.4 .6\nable. FastR-CNN(VGG-M) 59.2 72.4 .6\nWeusethemethodologyandtoolsofHoiemetal. [19] FastR-CNN(CaffeNet) 57.1 72.1 .3\nForeachcategoryattesttimewelookatthetopNpredic- YOLO 63.4 75.0 3.2\ntions for that category. Each prediction is either correct or\nitisclassifiedbasedonthetypeoferror:\nTable2: ModelcombinationexperimentsonVOC2007. We\n• Correct: correctclassandIOU>.5 examinetheeffectofcombiningvariousmodelswiththebestver-\nsionofFastR-CNN.OtherversionsofFastR-CNNprovideonly\n• Localization: correctclass,.1<IOU<.5\na small benefit while YOLO provides a significant performance\n• Similar: classissimilar,IOU>.1 boost."
  },
  {
    "chunk_id": "doc_3_p7_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_3_p7_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "VOC2012test mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbikepersonplant sheep sofa train tv\nMRCNNMOREDATA[11] 73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0\nHyperNetVGG 71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7\nHyperNetSP 71.3 84.1 78.3 73.3 55.5 53.6 78.6 79.6 87.5 49.5 74.9 52.1 85.6 81.6 83.2 81.6 48.4 73.2 59.3 79.7 65.6\nFastR-CNN+YOLO 70.7 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2\nMRCNNSCNN[11] 70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.1\nFasterR-CNN[28] 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nDEEPENSCOCO 70.1 84.0 79.4 71.6 51.9 51.1 74.1 72.1 88.6 48.3 73.4 57.8 86.1 80.0 80.7 70.4 46.6 69.6 68.8 75.9 71.4\nNoC[29] 68.8 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1\nFastR-CNN[14] 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2\nUMICHFGSSTRUCT 66.4 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2\nNUSNINC2000[7] 63.8 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.9 69.7 51.7 78.2 75.2 76.9 65.1 38.6 68.3 58.0 68.7 63.3\nBabyLearning[7] 63.2 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6\nNUSNIN 62.4 77.9 73.1 62.6 39.5 43.3 69.1 66.4 78.9 39.1 68.1 50.0 77.2 71.3 76.1 64.7 38.4 66.9 56.2 66.9 62.7\nR-CNNVGGBB[13] 62.4 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3\nR-CNNVGG[13] 59.2 76.8 70.9 56.6 37.5 36.9 62.9 63.6 81.1 35.7 64.3 43.9 80.4 71.6 74.0 60.0 30.8 63.4 52.0 63.5 58.7\nYOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8\nFeatureEdit[33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8\nR-CNNBB[13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\nSDS[16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7\nR-CNN[13] 49.6 68.1 63.8 46.1 29.4 27.9 56.6 57.0 65.9 26.5 48.7 39.5 66.2 57.3 65.4 53.2 26.2 54.5 38.1 50.6 51.6\nTable 3: PASCAL VOC 2012 Leaderboard."
  },
  {
    "chunk_id": "doc_3_p7_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "YOLO compared with the full comp4 (outside data allowed) public leaderboard as of\nNovember6th,2015. Meanaverageprecisionandper-classaverageprecisionareshownforavarietyofdetectionmethods. YOLOisthe\nonlyreal-timedetector.FastR-CNN+YOLOistheforthhighestscoringmethod,witha2.3%boostoverFastR-CNN. mAPincreasesby3.2%to75.0%. Wealsotriedcombining thetestdatacandivergefromwhatthesystemhasseenbe-\nthe top Fast R-CNN model with several other versions of fore[3]. WecompareYOLOtootherdetectionsystemson\nFastR-CNN.Thoseensemblesproducedsmallincreasesin thePicassoDataset[12]andthePeople-ArtDataset[3],two\nmAPbetween.3and.6%,seeTable2fordetails. datasetsfortestingpersondetectiononartwork. The boost from YOLO is not simply a byproduct of Figure 5 shows comparative performance between\nmodelensemblingsincethereislittlebenefitfromcombin- YOLOandotherdetectionmethods. Forreference,wegive\ningdifferentversionsofFastR-CNN.Rather,itisprecisely VOC2007detectionAPonpersonwhereallmodelsare\nbecause YOLO makes different kinds of mistakes at test trained only on VOC 2007 data. On Picasso models are\ntime that it is so effective at boosting Fast R-CNN’s per- trainedonVOC2012whileonPeople-Arttheyaretrained\nformance. onVOC2010."
  },
  {
    "chunk_id": "doc_3_p7_fixed_3",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "Unfortunately,thiscombinationdoesn’tbenefitfromthe R-CNN has high AP on VOC 2007. However, R-CNN\nspeed of YOLO since we run each model seperately and drops off considerably when applied to artwork. R-CNN\nthencombinetheresults. However, sinceYOLOissofast usesSelectiveSearchforboundingboxproposalswhichis\nitdoesn’taddanysignificantcomputationaltimecompared tunedfornaturalimages.TheclassifierstepinR-CNNonly\ntoFastR-CNN. seessmallregionsandneedsgoodproposals. DPM maintains its AP well when applied to artwork. 4.4.VOC2012Results\nPriorworktheorizesthatDPMperformswellbecauseithas\nOn the VOC 2012 test set, YOLO scores 57.9% mAP. strong spatial models of the shape and layout of objects. This is lower than the current state of the art, closer to ThoughDPMdoesn’tdegradeasmuchasR-CNN,itstarts\nthe original R-CNN using VGG-16, see Table 3. Our sys- fromalowerAP. tem struggles with small objects compared to its closest YOLOhasgoodperformanceonVOC2007anditsAP\ncompetitors. On categories like bottle, sheep, and degradeslessthanothermethodswhenappliedtoartwork. tv/monitorYOLOscores8-10%lowerthanR-CNNor\nLike DPM, YOLO models the size and shape of objects,\nFeature Edit. However, on other categories like cat and aswellasrelationshipsbetweenobjectsandwhereobjects\ntrainYOLOachieveshigherperformance. commonly appear."
  },
  {
    "chunk_id": "doc_3_p7_fixed_4",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "Artwork and natural images are very\nOurcombinedFastR-CNN+YOLOmodelisoneofthe different on a pixel level but they are similar in terms of\nhighest performing detection methods. Fast R-CNN gets the size and shape of objects, thus YOLO can still predict\na 2.3% improvement from the combination with YOLO, goodboundingboxesanddetections. boostingit5spotsuponthepublicleaderboard. 4.5.Generalizability: PersonDetectioninArtwork 5.Real-TimeDetectionInTheWild\nAcademicdatasetsforobjectdetectiondrawthetraining YOLOisafast,accurateobjectdetector,makingitideal\nand testing data from the same distribution. In real-world for computer vision applications. We connect YOLO to a\napplications it is hard to predict all possible use cases and webcamandverifythatitmaintainsreal-timeperformance,"
  },
  {
    "chunk_id": "doc_3_p8_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Humans\nYOLO\nVOC2007 Picasso People-Art\nDPM\nAP AP BestF AP\n1\nPoselets\nRCNN YOLO 59.2 53.3 0.590 45\nR-CNN 54.2 10.4 0.226 26\nDPM 43.2 37.8 0.458 32\nD&T Poselets[2] 36.5 17.8 0.271\nD&T[4] - 1.9 0.051\n(b)QuantitativeresultsontheVOC2007,Picasso,andPeople-ArtDatasets. (a)PicassoDatasetprecision-recallcurves. ThePicassoDatasetevaluatesonbothAPandbestF score. 1\nFigure5: GeneralizationresultsonPicassoandPeople-Artdatasets. Figure6: QualitativeResults.YOLOrunningonsampleartworkandnaturalimagesfromtheinternet.Itismostlyaccuratealthoughit\ndoesthinkonepersonisanairplane. includingthetimetofetchimagesfromthecameraanddis- directlyonfullimages. Unlikeclassifier-basedapproaches,\nplaythedetections. YOLOistrainedonalossfunctionthatdirectlycorresponds\nTheresultingsystemisinteractiveandengaging. While to detection performance and the entire model is trained\nYOLO processes images individually, when attached to a jointly. webcam it functions like a tracking system, detecting ob-\nFast YOLO is the fastest general-purpose object detec-\njects as they move around and change in appearance. A\ntorintheliteratureandYOLOpushesthestate-of-the-artin\ndemo of the system and the source code can be found on\nreal-time object detection."
  },
  {
    "chunk_id": "doc_3_p8_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "YOLO also generalizes well to\nourprojectwebsite: http://pjreddie.com/yolo/. new domains making it ideal for applications that rely on\nfast,robustobjectdetection. 6.Conclusion\nAcknowledgements: This work is partially supported by\nWe introduce YOLO, a unified model for object detec- ONRN00014-13-1-0720,NSFIIS-1338054,andTheAllen\ntion. Our model is simple to construct and can be trained DistinguishedInvestigatorAward."
  },
  {
    "chunk_id": "doc_3_p9_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "References [16] B.Hariharan,P.Arbela´ez,R.Girshick,andJ.Malik. Simul-\ntaneous detection and segmentation. In Computer Vision–\n[1] M.B.BlaschkoandC.H.Lampert. Learningtolocalizeob- ECCV2014,pages297–312.Springer,2014. 7\njectswithstructuredoutputregression. InComputerVision–\n[17] K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpooling\nECCV2008,pages2–15.Springer,2008. 4\nindeepconvolutionalnetworksforvisualrecognition. arXiv\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors preprintarXiv:1406.4729,2014. 5\ntrained using 3d human pose annotations. In International\n[18] G.E.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,and\nConferenceonComputerVision(ICCV),2009. 8\nR. R. Salakhutdinov. Improving neural networks by pre-\n[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross- venting co-adaptation of feature detectors. arXiv preprint\ndepiction problem: Computer vision algorithms for recog- arXiv:1207.0580,2012. 4\nnisingobjectsinartworkandinphotographs. arXivpreprint\n[19] D.Hoiem,Y.Chodpathumwan,andQ.Dai.Diagnosingerror\narXiv:1505.00110,2015. 7\ninobjectdetectors."
  },
  {
    "chunk_id": "doc_3_p9_fixed_1",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "InComputerVision–ECCV2012,pages\n[4] N.DalalandB.Triggs. Histogramsoforientedgradientsfor 340–353.Springer,2012. 6\nhumandetection. InComputerVisionandPatternRecogni-\n[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint\ntion,2005.CVPR2005.IEEEComputerSocietyConference\narXiv:1506.06981,2015. 5,6\non,volume1,pages886–893.IEEE,2005. 4,8\n[21] R.LienhartandJ.Maydt. Anextendedsetofhaar-likefea-\n[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya- turesforrapidobjectdetection. InImageProcessing.2002. narasimhan, J. Yagnik, et al. Fast, accurate detection of Proceedings. 2002 International Conference on, volume 1,\n100,000 object classes on a single machine. In Computer pagesI–900.IEEE,2002. 4\nVisionandPatternRecognition(CVPR),2013IEEEConfer-\n[22] M.Lin,Q.Chen,andS.Yan. Networkinnetwork. CoRR,\nenceon,pages1814–1821.IEEE,2013. 5\nabs/1312.4400,2013. 2\n[6] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\n[23] D. G. Lowe. Object recognition from local scale-invariant\nE.Tzeng,andT.Darrell."
  },
  {
    "chunk_id": "doc_3_p9_fixed_2",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Decaf: Adeepconvolutionalacti-\nfeatures. InComputervision,1999.Theproceedingsofthe\nvationfeatureforgenericvisualrecognition. arXivpreprint\nseventhIEEEinternationalconferenceon,volume2,pages\narXiv:1310.1531,2013. 4\n1150–1157.Ieee,1999. 4\n[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards unified\n[24] D. Mishkin. Models accuracy on imagenet 2012\nobject detection and semantic segmentation. In Computer val. https://github.com/BVLC/caffe/wiki/\nVision–ECCV2014,pages299–314.Springer,2014. 7 Models-accuracy-on-ImageNet-2012-val. Ac-\n[8] D.Erhan,C.Szegedy,A.Toshev,andD.Anguelov.Scalable cessed:2015-10-2. 3\nobject detection using deep neural networks. In Computer\n[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general\nVisionandPatternRecognition(CVPR),2014IEEEConfer-\nframeworkforobjectdetection. InComputervision, 1998.\nenceon,pages2155–2162.IEEE,2014. 5,6 sixth international conference on, pages 555–562. IEEE,\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. 1998. 4\nWilliams,J.Winn,andA.Zisserman. Thepascalvisualob- [26] J. Redmon."
  },
  {
    "chunk_id": "doc_3_p9_fixed_3",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Darknet: Open source neural networks in c.\njectclasseschallenge:Aretrospective.InternationalJournal http://pjreddie.com/darknet/,2013–2016. 3\nofComputerVision,111(1):98–136,Jan.2015. 2\n[27] J.RedmonandA.Angelova.Real-timegraspdetectionusing\n[10] P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ra- convolutionalneuralnetworks.CoRR,abs/1412.3128,2014. manan. Object detection with discriminatively trained part 5\nbasedmodels. IEEETransactionsonPatternAnalysisand\n[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\nMachineIntelligence,32(9):1627–1645,2010. 1,4\nwards real-time object detection with region proposal net-\n[11] S.GidarisandN.Komodakis. Objectdetectionviaamulti- works. arXivpreprintarXiv:1506.01497,2015. 5,6,7\nregion&semanticsegmentation-awareCNNmodel. CoRR,\n[29] S.Ren,K.He,R.B.Girshick,X.Zhang,andJ.Sun. Object\nabs/1505.01749,2015. 7\ndetection networks on convolutional feature maps. CoRR,\n[12] S.Ginosar,D.Haas,T.Brown,andJ.Malik. Detectingpeo- abs/1504.06066,2015."
  },
  {
    "chunk_id": "doc_3_p9_fixed_4",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "3,7\npleincubistart.InComputerVision-ECCV2014Workshops,\n[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\npages101–116.Springer,2014. 7 S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\n[13] R.Girshick,J.Donahue,T.Darrell,andJ.Malik. Richfea- A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nture hierarchies for accurate object detection and semantic RecognitionChallenge. InternationalJournalofComputer\nsegmentation. InComputerVisionandPatternRecognition Vision(IJCV),2015. 3\n(CVPR),2014IEEEConferenceon, pages580–587.IEEE, [31] M.A.SadeghiandD.Forsyth. 30hzobjectdetectionwith\n2014. 1,4,7 dpm v5. In Computer Vision–ECCV 2014, pages 65–79. [14] R.B.Girshick. FastR-CNN. CoRR,abs/1504.08083,2015. Springer,2014. 5,6\n2,5,6,7 [32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\n[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta- and Y. LeCun. Overfeat: Integrated recognition, localiza-\ntionandobjectdetection. InAdvancesinneuralinformation tion and detection using convolutional networks."
  },
  {
    "chunk_id": "doc_3_p9_fixed_5",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "CoRR,\nprocessingsystems,pages655–663,2009. 4 abs/1312.6229,2013. 4,5"
  },
  {
    "chunk_id": "doc_3_p10_fixed_0",
    "doc_id": "doc_3",
    "pdf_name": "3.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "[33] Z.ShenandX.Xue.Domoredropoutsinpool5featuremaps\nforbetterobjectdetection. arXivpreprintarXiv:1409.6911,\n2014. 7\n[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD.Anguelov, D.Erhan, V.Vanhoucke, andA.Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842,\n2014. 2\n[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\nSmeulders. Selective search for object recognition. Inter-\nnationaljournalofcomputervision,104(2):154–171,2013. 4\n[36] P. Viola and M. Jones. Robust real-time object detection. InternationalJournalofComputerVision,4:34–47,2001. 4\n[37] P. Viola and M. J. Jones. Robust real-time face detection. International journal of computer vision, 57(2):137–154,\n2004. 5\n[38] J.Yan,Z.Lei,L.Wen,andS.Z.Li. Thefastestdeformable\npartmodelforobjectdetection.InComputerVisionandPat-\nternRecognition(CVPR),2014IEEEConferenceon,pages\n2497–2504.IEEE,2014. 5,6\n[39] C.L.ZitnickandP.Dolla´r.Edgeboxes:Locatingobjectpro-\nposalsfromedges. InComputerVision–ECCV2014,pages\n391–405.Springer,2014. 4"
  },
  {
    "chunk_id": "doc_4_p1_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "L O N G S H O R T - T E R M M E M O R Y\nN e u r a l C o m p u t a t io n 9 ( 8 ) :1 7 3 5 { 1 7 8 0 , 1 9 9 7\nJ (cid:127)u r g e n S c h m i d h u b e r S e p p H o c h r e i t e r\nI D S I A F a k u l t (cid:127)a t f (cid:127)u r I n f o r m a t i k\nC o r s o E l v e z i a 3 6 T e c h n i s c h e U n i v e r s i t (cid:127)a t M (cid:127)u n c h e n\n6 9 0 0 L u g a n o , S w i t z e r l a n d 8 0 2 9 0 M (cid:127)u n c h e n , G e r m a n y\nj u e r g e n @ i d s i a . c h h o c h r e i t @ i n f o r m a t i k . t u - m u e n c h e n ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "d e\nh t t p :/ / w w w .id s ia .c h / ~ ju e r g e n h t t p :/ / w w w 7 .in fo r m a t ik .t u - m u e n c h e n .d e / ~ h o c h r e it\nA b s t r a c t\nL e a r n in g t o s t o r e in fo r m a t io n o v e r e x t e n d e d t im e in t e r v a ls v ia r e c u r r e n t b a c k p r o p a g a t io n\nt a k e s a v e r y lo n g t im e , m o s t ly d u e t o in s u (cid:14) c ie n t , d e c a y in g e r r o r b a c k (cid:13) o w . W e b r ie (cid:13) y r e v ie w\nH o c h r e it e r 's 1 9 9 1 a n a ly s is o f t h is p r o b le m , t h e n a d d r e s s it b y in t r o d u c in g a n o v e l, e (cid:14) c ie n t ,\ng r a d ie n t - b a s e d m e t h o d c a lle d \\ L o n g S h o r t - T e r m M e m o r y \" ( L S T M ) ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "T r u n c a t in g t h e g r a d ie n t\nw h e r e t h is d o e s n o t d o h a r m , L S T M c a n le a r n t o b r id g e m in im a l t im e la g s in e x c e s s o f 1 0 0 0\nd is c r e t e t im e s t e p s b y e n fo r c in g c o n s ta n t e r r o r (cid:13) o w t h r o u g h \\ c o n s t a n t e r r o r c a r r o u s e ls \" w it h in\ns p e c ia l u n it s . M u lt ip lic a t iv e g a t e u n it s le a r n t o o p e n a n d c lo s e a c c e s s t o t h e c o n s t a n t e r r o r\n(cid:13) o w . L S T M is lo c a l in s p a c e a n d t im e ; it s c o m p u t a t io n a l c o m p le x it y p e r t im e s t e p a n d w e ig h t\nis O ( 1 ) ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "O u r e x p e r im e n t s w it h a r t i(cid:12) c ia l d a t a in v o lv e lo c a l, d is t r ib u t e d , r e a l- v a lu e d , a n d n o is y\np a t t e r n r e p r e s e n t a t io n s . I n c o m p a r is o n s w it h R T R L , B P T T , R e c u r r e n t C a s c a d e - C o r r e la t io n ,\nE lm a n n e t s , a n d N e u r a l S e q u e n c e C h u n k in g , L S T M le a d s t o m a n y m o r e s u c c e s s fu l r u n s , a n d\nle a r n s m u c h fa s t e r . L S T M a ls o s o lv e s c o m p le x , a r t i(cid:12) c ia l lo n g t im e la g t a s k s t h a t h a v e n e v e r\nb e e n s o lv e d b y p r e v io u s r e c u r r e n t n e t w o r k a lg o r it h m s ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "1 I N T R O D U C T I O N\nR e c u r r e n t n e t w o r k s c a n in p r in c ip le u s e t h e ir fe e d b a c k c o n n e c t io n s t o s t o r e r e p r e s e n t a t io n s o f\nr e c e n t in p u t e v e n t s in fo r m o f a c t iv a t io n s ( \\ s h o r t - t e r m m e m o r y \" , a s o p p o s e d t o \\ lo n g - t e r m m e m -\no r y \" e m b o d ie d b y s lo w ly c h a n g in g w e ig h t s ) . T h is is p o t e n t ia lly s ig n i(cid:12) c a n t fo r m a n y a p p lic a t io n s ,\nin c lu d in g s p e e c h p r o c e s s in g , n o n - M a r k o v ia n c o n t r o l, a n d m u s ic c o m p o s it io n ( e .g ., M o z e r 1 9 9 2 ) ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "T h e m o s t w id e ly u s e d a lg o r it h m s fo r le a r n in g w h a t t o p u t in s h o r t - t e r m m e m o r y , h o w e v e r , t a k e\nt o o m u c h t im e o r d o n o t w o r k w e ll a t a ll, e s p e c ia lly w h e n m in im a l t im e la g s b e t w e e n in p u t s a n d\nc o r r e s p o n d in g t e a c h e r s ig n a ls a r e lo n g . A lt h o u g h t h e o r e t ic a lly fa s c in a t in g , e x is t in g m e t h o d s d o\nn o t p r o v id e c le a r p r a c t ic a l a d v a n t a g e s o v e r , s a y , b a c k p r o p in fe e d fo r w a r d n e t s w it h lim it e d t im e\nw in d o w s . T h is p a p e r w ill r e v ie w a n a n a ly s is o f t h e p r o b le m a n d s u g g e s t a r e m e d y . T h e p r o b l e m ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "W it h c o n v e n t io n a l \\ B a c k - P r o p a g a t io n T h r o u g h T im e \" ( B P T T , e .g ., W illia m s\na n d Z ip s e r 1 9 9 2 , W e r b o s 1 9 8 8 ) o r \\ R e a l- T im e R e c u r r e n t L e a r n in g \" ( R T R L , e .g ., R o b in s o n a n d\nF a lls id e 1 9 8 7 ) , e r r o r s ig n a ls \\ (cid:13) o w in g b a c k w a r d s in t im e \" t e n d t o e it h e r ( 1 ) b lo w u p o r ( 2 ) v a n is h :\nt h e t e m p o r a l e v o lu t io n o f t h e b a c k p r o p a g a t e d e r r o r e x p o n e n t ia lly d e p e n d s o n t h e s iz e o f t h e\nw e ig h t s ( H o c h r e it e r 1 9 9 1 ) ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "C a s e ( 1 ) m a y le a d t o o s c illa t in g w e ig h t s , w h ile in c a s e ( 2 ) le a r n in g t o\nb r id g e lo n g t im e la g s t a k e s a p r o h ib it iv e a m o u n t o f t im e , o r d o e s n o t w o r k a t a ll ( s e e s e c t io n 3 ) . T h e r e m e d y . T h is p a p e r p r e s e n t s \\ L o n g S h o r t - T e r m M e m o r y \" ( L S T M ) , a n o v e l r e c u r r e n t\nn e t w o r k a r c h it e c t u r e in c o n ju n c t io n w it h a n a p p r o p r ia t e g r a d ie n t - b a s e d le a r n in g a lg o r it h m . L S T M\nis d e s ig n e d t o o v e r c o m e t h e s e e r r o r b a c k - (cid:13) o w p r o b le m s ."
  },
  {
    "chunk_id": "doc_4_p1_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "I t c a n le a r n t o b r id g e t im e in t e r v a ls in\ne x c e s s o f 1 0 0 0 s t e p s e v e n in c a s e o f n o is y , in c o m p r e s s ib le in p u t s e q u e n c e s , w it h o u t lo s s o f s h o r t\nt im e la g c a p a b ilit ie s . T h is is a c h ie v e d b y a n e (cid:14) c ie n t , g r a d ie n t - b a s e d a lg o r it h m fo r a n a r c h it e c t u r e\n1"
  },
  {
    "chunk_id": "doc_4_p2_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "e n fo r c in g c o n s t a n t ( t h u s n e it h e r e x p lo d in g n o r v a n is h in g ) e r r o r (cid:13) o w t h r o u g h in t e r n a l s t a t e s o f\ns p e c ia l u n it s ( p r o v id e d t h e g r a d ie n t c o m p u t a t io n is t r u n c a t e d a t c e r t a in a r c h it e c t u r e - s p e c i(cid:12) c p o in t s\n| t h is d o e s n o t a (cid:11) e c t lo n g - t e r m e r r o r (cid:13) o w t h o u g h ) . O u t l i n e o f p a p e r . S e c t io n 2 w ill b r ie (cid:13) y r e v ie w p r e v io u s w o r k . S e c t io n 3 b e g in s w it h a n o u t lin e\no f t h e d e t a ile d a n a ly s is o f v a n is h in g e r r o r s d u e t o H o c h r e it e r ( 1 9 9 1 ) ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "I t w ill t h e n in t r o d u c e a n a iv e\na p p r o a c h t o c o n s t a n t e r r o r b a c k p r o p fo r d id a c t ic p u r p o s e s , a n d h ig h lig h t it s p r o b le m s c o n c e r n in g\nin fo r m a t io n s t o r a g e a n d r e t r ie v a l. T h e s e p r o b le m s w ill le a d t o t h e L S T M a r c h it e c t u r e a s d e s c r ib e d\nin S e c t io n 4 . S e c t io n 5 w ill p r e s e n t n u m e r o u s e x p e r im e n t s a n d c o m p a r is o n s w it h c o m p e t in g\nm e t h o d s . L S T M o u t p e r fo r m s t h e m , a n d a ls o le a r n s t o s o lv e c o m p le x , a r t i(cid:12) c ia l t a s k s n o o t h e r\nr e c u r r e n t n e t a lg o r it h m h a s s o lv e d . S e c t io n 6 w ill d is c u s s L S T M 's lim it a t io n s a n d a d v a n t a g e s ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "T h e\na p p e n d ix c o n t a in s a d e t a ile d d e s c r ip t io n o f t h e a lg o r it h m ( A .1 ) , a n d e x p lic it e r r o r (cid:13) o w fo r m u la e\n( A .2 ) . 2 P R E V I O U S W O R K\nT h is s e c t io n w ill fo c u s o n r e c u r r e n t n e t s w it h t im e - v a r y in g in p u t s ( a s o p p o s e d t o n e t s w it h s t a -\nt io n a r y in p u t s a n d (cid:12) x p o in t - b a s e d g r a d ie n t c a lc u la t io n s , e .g ., A lm e id a 1 9 8 7 , P in e d a 1 9 8 7 ) . G r a d i e n t - d e s c e n t v a r i a n t s ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "T h e a p p r o a c h e s o f E lm a n ( 1 9 8 8 ) , F a h lm a n ( 1 9 9 1 ) , W illia m s\n( 1 9 8 9 ) , S c h m id h u b e r ( 1 9 9 2 a ) , P e a r lm u t t e r ( 1 9 8 9 ) , a n d m a n y o f t h e r e la t e d a lg o r it h m s in P e a r l-\nm u t t e r 's c o m p r e h e n s iv e o v e r v ie w ( 1 9 9 5 ) s u (cid:11) e r fr o m t h e s a m e p r o b le m s a s B P T T a n d R T R L ( s e e\nS e c t io n s 1 a n d 3 ) . T i m e - d e l a y s ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "O t h e r m e t h o d s t h a t s e e m p r a c t ic a l fo r s h o r t t im e la g s o n ly a r e T im e - D e la y\nN e u r a l N e t w o r k s ( L a n g e t a l. 1 9 9 0 ) a n d P la t e 's m e t h o d ( P la t e 1 9 9 3 ) , w h ic h u p d a t e s u n it a c t iv a -\nt io n s b a s e d o n a w e ig h t e d s u m o f o ld a c t iv a t io n s ( s e e a ls o d e V r ie s a n d P r in c ip e 1 9 9 1 ) . L in e t a l.\n( 1 9 9 5 ) p r o p o s e v a r ia n t s o f t im e - d e la y n e t w o r k s c a lle d N A R X n e t w o r k s . T i m e c o n s t a n t s ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "T o d e a l w it h lo n g t im e la g s , M o z e r ( 1 9 9 2 ) u s e s t im e c o n s t a n t s in (cid:13) u e n c in g\nc h a n g e s o f u n it a c t iv a t io n s ( d e V r ie s a n d P r in c ip e 's a b o v e - m e n t io n e d a p p r o a c h ( 1 9 9 1 ) m a y in fa c t\nb e v ie w e d a s a m ix t u r e o f T D N N a n d t im e c o n s t a n t s ) . F o r lo n g t im e la g s , h o w e v e r , t h e t im e\nc o n s t a n t s n e e d e x t e r n a l (cid:12) n e t u n in g ( M o z e r 1 9 9 2 ) . S u n e t a l.'s a lt e r n a t iv e a p p r o a c h ( 1 9 9 3 ) u p d a t e s\nt h e a c t iv a t io n o f a r e c u r r e n t u n it b y a d d in g t h e o ld a c t iv a t io n a n d t h e ( s c a le d ) c u r r e n t n e t in p u t ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "T h e n e t in p u t , h o w e v e r , t e n d s t o p e r t u r b t h e s t o r e d in fo r m a t io n , w h ic h m a k e s lo n g - t e r m s t o r a g e\nim p r a c t ic a l.\nR i n g ' s a p p r o a c h . R in g ( 1 9 9 3 ) a ls o p r o p o s e d a m e t h o d fo r b r id g in g lo n g t im e la g s . W h e n e v e r\na u n it in h is n e t w o r k r e c e iv e s c o n (cid:13) ic t in g e r r o r s ig n a ls , h e a d d s a h ig h e r o r d e r u n it in (cid:13) u e n c in g\na p p r o p r ia t e c o n n e c t io n s . A lt h o u g h h is a p p r o a c h c a n s o m e t im e s b e e x t r e m e ly fa s t , t o b r id g e a\nt im e la g in v o lv in g 1 0 0 s t e p s m a y r e q u ir e t h e a d d it io n o f 1 0 0 u n it s ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "A ls o , R in g 's n e t d o e s n o t\ng e n e r a liz e t o u n s e e n la g d u r a t io n s . B e n g i o e t a l . ' s a p p r o a c h e s . B e n g io e t a l. ( 1 9 9 4 ) in v e s t ig a t e m e t h o d s s u c h a s s im u la t e d\na n n e a lin g , m u lt i- g r id r a n d o m s e a r c h , t im e - w e ig h t e d p s e u d o - N e w t o n o p t im iz a t io n , a n d d is c r e t e\ne r r o r p r o p a g a t io n . T h e ir \\ la t c h \" a n d \\ 2 - s e q u e n c e \" p r o b le m s a r e v e r y s im ila r t o p r o b le m 3 a w it h\nm in im a l t im e la g 1 0 0 ( s e e E x p e r im e n t 3 ) . B e n g io a n d F r a s c o n i ( 1 9 9 4 ) a ls o p r o p o s e a n E M a p p r o a c h\nfo r p r o p a g a t in g t a r g e t s ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "W it h n s o - c a lle d \\ s t a t e n e t w o r k s \" , a t a g iv e n t im e , t h e ir s y s t e m c a n b e\nin o n e o f o n ly n d i(cid:11) e r e n t s t a t e s . S e e a ls o b e g in n in g o f S e c t io n 5 . B u t t o s o lv e c o n t in u o u s p r o b le m s\ns u c h a s t h e \\ a d d in g p r o b le m \" ( S e c t io n 5 .4 ) , t h e ir s y s t e m w o u ld r e q u ir e a n u n a c c e p t a b le n u m b e r\no f s t a t e s ( i.e ., s t a t e n e t w o r k s ) . K a l m a n (cid:12) l t e r s . P u s k o r iu s a n d F e ld k a m p ( 1 9 9 4 ) u s e K a lm a n (cid:12) lt e r t e c h n iq u e s t o im p r o v e\nr e c u r r e n t n e t p e r fo r m a n c e ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "S in c e t h e y u s e \\ a d e r iv a t iv e d is c o u n t fa c t o r im p o s e d t o d e c a y e x p o -\nn e n t ia lly t h e e (cid:11) e c t s o f p a s t d y n a m ic d e r iv a t iv e s ,\" t h e r e is n o r e a s o n t o b e lie v e t h a t t h e ir K a lm a n\nF ilt e r T r a in e d R e c u r r e n t N e t w o r k s w ill b e u s e fu l fo r v e r y lo n g m in im a l t im e la g s . S e c o n d o r d e r n e t s . W e w ill s e e t h a t L S T M u s e s m u lt ip lic a t iv e u n it s ( M U s ) t o p r o t e c t e r r o r\n(cid:13) o w fr o m u n w a n t e d p e r t u r b a t io n s . I t is n o t t h e (cid:12) r s t r e c u r r e n t n e t m e t h o d u s in g M U s t h o u g h . F o r in s t a n c e , W a t r o u s a n d K u h n ( 1 9 9 2 ) u s e M U s in s e c o n d o r d e r n e t s ."
  },
  {
    "chunk_id": "doc_4_p2_fixed_10",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "S o m e d i(cid:11) e r e n c e s t o L S T M\na r e : ( 1 ) W a t r o u s a n d K u h n 's a r c h it e c t u r e d o e s n o t e n fo r c e c o n s t a n t e r r o r (cid:13) o w a n d is n o t d e s ig n e d\n2"
  },
  {
    "chunk_id": "doc_4_p3_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "t o s o lv e lo n g t im e la g p r o b le m s . ( 2 ) I t h a s fu lly c o n n e c t e d s e c o n d - o r d e r s ig m a - p i u n it s , w h ile t h e\nL S T M a r c h it e c t u r e 's M U s a r e u s e d o n ly t o g a t e a c c e s s t o c o n s t a n t e r r o r (cid:13) o w . ( 3 ) W a t r o u s a n d\n2\nK u h n 's a lg o r it h m c o s t s O ( W ) o p e r a t io n s p e r t im e s t e p , o u r s o n ly O ( W ) , w h e r e W is t h e n u m b e r\no f w e ig h t s . S e e a ls o M ille r a n d G ile s ( 1 9 9 3 ) fo r a d d it io n a l w o r k o n M U s . S i m p l e w e i g h t g u e s s i n g ."
  },
  {
    "chunk_id": "doc_4_p3_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "T o a v o id lo n g t im e la g p r o b le m s o f g r a d ie n t - b a s e d a p p r o a c h e s w e\nm a y s im p ly r a n d o m ly in it ia liz e a ll n e t w o r k w e ig h t s u n t il t h e r e s u lt in g n e t h a p p e n s t o c la s s ify a ll\nt r a in in g s e q u e n c e s c o r r e c t ly . I n fa c t , r e c e n t ly w e d is c o v e r e d ( S c h m id h u b e r a n d H o c h r e it e r 1 9 9 6 ,\nH o c h r e it e r a n d S c h m id h u b e r 1 9 9 6 , 1 9 9 7 ) t h a t s im p le w e ig h t g u e s s in g s o lv e s m a n y o f t h e p r o b le m s\nin ( B e n g io 1 9 9 4 , B e n g io a n d F r a s c o n i 1 9 9 4 , M ille r a n d G ile s 1 9 9 3 , L in e t a l. 1 9 9 5 ) fa s t e r t h a n\nt h e a lg o r it h m s p r o p o s e d t h e r e in ."
  },
  {
    "chunk_id": "doc_4_p3_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "T h is d o e s n o t m e a n t h a t w e ig h t g u e s s in g is a g o o d a lg o r it h m . I t ju s t m e a n s t h a t t h e p r o b le m s a r e v e r y s im p le . M o r e r e a lis t ic t a s k s r e q u ir e e it h e r m a n y fr e e\np a r a m e t e r s ( e .g ., in p u t w e ig h t s ) o r h ig h w e ig h t p r e c is io n ( e .g ., fo r c o n t in u o u s - v a lu e d p a r a m e t e r s ) ,\ns u c h t h a t g u e s s in g b e c o m e s c o m p le t e ly in fe a s ib le . A d a p t i v e s e q u e n c e c h u n k e r s ."
  },
  {
    "chunk_id": "doc_4_p3_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "S c h m id h u b e r 's h ie r a r c h ic a l c h u n k e r s y s t e m s ( 1 9 9 2 b , 1 9 9 3 )\nd o h a v e a c a p a b ilit y t o b r id g e a r b it r a r y t im e la g s , b u t o n ly if t h e r e is lo c a l p r e d ic t a b ilit y a c r o s s t h e\ns u b s e q u e n c e s c a u s in g t h e t im e la g s ( s e e a ls o M o z e r 1 9 9 2 ) . F o r in s t a n c e , in h is p o s t d o c t o r a l t h e s is\n( 1 9 9 3 ) , S c h m id h u b e r u s e s h ie r a r c h ic a l r e c u r r e n t n e t s t o r a p id ly s o lv e c e r t a in g r a m m a r le a r n in g\nt a s k s in v o lv in g m in im a l t im e la g s in e x c e s s o f 1 0 0 0 s t e p s ."
  },
  {
    "chunk_id": "doc_4_p3_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "T h e p e r fo r m a n c e o f c h u n k e r s y s t e m s ,\nh o w e v e r , d e t e r io r a t e s a s t h e n o is e le v e l in c r e a s e s a n d t h e in p u t s e q u e n c e s b e c o m e le s s c o m p r e s s ib le . L S T M d o e s n o t s u (cid:11) e r fr o m t h is p r o b le m . 3 C O N S T A N T E R R O R B A C K P R O P\n3 . 1 E X P O N E N T I A L L Y D E C A Y I N G E R R O R\nC o n v e n t i o n a l B P T T ( e .g . W illia m s a n d Z ip s e r 1 9 9 2 ) . O u t p u t u n it k 's t a r g e t a t t im e t is\nd e n o t e d b y d ( t ) ."
  },
  {
    "chunk_id": "doc_4_p3_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "U s in g m e a n s q u a r e d e r r o r , k 's e r r o r s ig n a l is k\n0 k\n# ( t ) = f ( n e t ( t ) ) ( d ( t ) (cid:0) y ( t ) ) ; k k k k\nw h e r e\ni\ny ( t ) = f ( n e t ( t ) ) i i\nis t h e a c t iv a t io n o f a n o n - in p u t u n it i w it h d i(cid:11) e r e n t ia b le a c t iv a t io n fu n c t io n f , i\nX\nj\nn e t ( t ) = w y ( t (cid:0) 1 ) i ij\nj\nis u n it i 's c u r r e n t n e t in p u t , a n d w is t h e w e ig h t o n t h e c o n n e c t io n fr o m u n it j t o i ."
  },
  {
    "chunk_id": "doc_4_p3_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "S o m e ij\nn o n - o u t p u t u n it j 's b a c k p r o p a g a t e d e r r o r s ig n a l is\nX\n0\n# ( t ) = f ( n e t ( t ) ) w # ( t + 1 ) : j j ij i j\ni\nl\nT h e c o r r e s p o n d in g c o n t r ib u t io n t o w 's t o t a l w e ig h t u p d a t e is (cid:11) # ( t ) y ( t (cid:0) 1 ) , w h e r e (cid:11) is t h e j l j\nle a r n in g r a t e , a n d l s t a n d s fo r a n a r b it r a r y u n it c o n n e c t e d t o u n it j . O u t l i n e o f H o c h r e i t e r ' s a n a l y s i s ( 1 9 9 1 , p a g e 1 9 - 2 1 ) . S u p p o s e w e h a v e a fu lly c o n n e c t e d\nn e t w h o s e n o n - in p u t u n it in d ic e s r a n g e fr o m 1 t o n ."
  },
  {
    "chunk_id": "doc_4_p3_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "L e t u s fo c u s o n lo c a l e r r o r (cid:13) o w fr o m u n it u\nt o u n it v ( la t e r w e w ill s e e t h a t t h e a n a ly s is im m e d ia t e ly e x t e n d s t o g lo b a l e r r o r (cid:13) o w ) . T h e e r r o r\no c c u r r in g a t a n a r b it r a r y u n it u a t t im e s t e p t is p r o p a g a t e d \\ b a c k in t o t im e \" fo r q t im e s t e p s , t o\na n a r b it r a r y u n it v . T h is w ill s c a le t h e e r r o r b y t h e fo llo w in g fa c t o r :\n(\n0\nf ( n e t ( t (cid:0) 1 ) ) w q = 1 @ # ( t (cid:0) q ) v u v v v\nP = : ( 1 ) n @ # ( t(cid:0) q + 1 ) l 0\nw q > 1 f ( n e t ( t (cid:0) q ) ) @ # ( t ) lv v u v l= 1 @ # ( t) u\n3"
  },
  {
    "chunk_id": "doc_4_p4_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "W it h l = v a n d l = u , w e o b t a in : q 0\nq n n X X Y @ # ( t (cid:0) q ) v 0\n= : : : f ( n e t ( t (cid:0) m ) ) w ( 2 ) l l l m m m (cid:0) 1 lm\n@ # ( t ) u\nm = 1 l = 1 l = 1 1 q (cid:0) 1\nQ q q (cid:0) 1 0\n( p r o o f b y in d u c t io n ) . T h e s u m o f t h e n t e r m s f ( n e t ( t (cid:0) m ) ) w d e t e r m in e s t h e l l l m m m (cid:0) 1 l m = 1 m\nt o t a l e r r o r b a c k (cid:13) o w ( n o t e t h a t s in c e t h e s u m m a t io n t e r m s m a y h a v e d i(cid:11) e r e n t s ig n s , in c r e a s in g\nt h e n u m b e r o f u n it s n d o e s n o t n e c e s s a r ily in c r e a s e e r r o r (cid:13) o w ) . I n t u i t i v e e x p l a n a t i o n o f e q u a t i o n ( 2 ) ."
  },
  {
    "chunk_id": "doc_4_p4_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "I f\n0\njf ( n e t ( t (cid:0) m ) ) w j > 1 :0 l l l m m m (cid:0) 1 lm\nfo r a ll m ( a s c a n h a p p e n , e .g ., w it h lin e a r f ) t h e n t h e la r g e s t p r o d u c t in c r e a s e s e x p o n e n t ia lly lm\nw it h q . T h a t is , t h e e r r o r b lo w s u p , a n d c o n (cid:13) ic t in g e r r o r s ig n a ls a r r iv in g a t u n it v c a n le a d t o\no s c illa t in g w e ig h t s a n d u n s t a b le le a r n in g ( fo r e r r o r b lo w - u p s o r b ifu r c a t io n s s e e a ls o P in e d a 1 9 8 8 ,\nB a ld i a n d P in e d a 1 9 9 1 , D o y a 1 9 9 2 ) . O n t h e o t h e r h a n d , if\n0\njf ( n e t ( t (cid:0) m ) ) w j < 1 :0 l l l m m m (cid:0) 1 lm\nfo r a ll m , t h e n t h e la r g e s t p r o d u c t d e c r e a s e s e x p o n e n t ia lly w it h q ."
  },
  {
    "chunk_id": "doc_4_p4_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "T h a t is , t h e e r r o r v a n is h e s , a n d\nn o t h in g c a n b e le a r n e d in a c c e p t a b le t im e . 0 lm (cid:0) 1 is c o n s t a n t I f f is t h e lo g is t ic s ig m o id fu n c t io n , t h e n t h e m a x im a l v a lu e o f f is 0 .2 5 . I f y lm lm\n0\n( n e t ) w j t a k e s o n m a x im a l v a lu e s w h e r e a n d n o t e q u a l t o z e r o , t h e n jf l l l m m m (cid:0) 1 lm\n1 1\nc o t h ( n e t ) ; w = l l l m m m (cid:0) 1 lm (cid:0) 1 y 2\ng o e s t o z e r o fo r jw j ! 1 , a n d is le s s t h a n 1 :0 fo r jw j < 4 :0 ( e .g ., if t h e a b s o lu t e m a x - l l l l m m (cid:0) 1 m m (cid:0) 1\nim a l w e ig h t v a lu e w is s m a lle r t h a n 4 .0 ) ."
  },
  {
    "chunk_id": "doc_4_p4_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "H e n c e w it h c o n v e n t io n a l lo g is t ic s ig m o id a c t iv a t io n m a x\nfu n c t io n s , t h e e r r o r (cid:13) o w t e n d s t o v a n is h a s lo n g a s t h e w e ig h t s h a v e a b s o lu t e v a lu e s b e lo w 4 .0 ,\ne s p e c ia lly in t h e b e g in n in g o f t h e t r a in in g p h a s e . I n g e n e r a l t h e u s e o f la r g e r in it ia l w e ig h t s w ill\nn o t h e lp t h o u g h | a s s e e n a b o v e , fo r jw j ! 1 t h e r e le v a n t d e r iv a t iv e g o e s t o z e r o \\ fa s t e r \" l l m m (cid:0) 1\nt h a n t h e a b s o lu t e w e ig h t c a n g r o w ( a ls o , s o m e w e ig h t s w ill h a v e t o c h a n g e t h e ir s ig n s b y c r o s s in g\nz e r o ) ."
  },
  {
    "chunk_id": "doc_4_p4_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "L ik e w is e , in c r e a s in g t h e le a r n in g r a t e d o e s n o t h e lp e it h e r | it w ill n o t c h a n g e t h e r a t io o f\nlo n g - r a n g e e r r o r (cid:13) o w a n d s h o r t - r a n g e e r r o r (cid:13) o w . B P T T is t o o s e n s it iv e t o r e c e n t d is t r a c t io n s . ( A\nv e r y s im ila r , m o r e r e c e n t a n a ly s is w a s p r e s e n t e d b y B e n g io e t a l. 1 9 9 4 ) . G l o b a l e r r o r (cid:13) o w . T h e lo c a l e r r o r (cid:13) o w a n a ly s is a b o v e im m e d ia t e ly s h o w s t h a t g lo b a l e r r o r\n(cid:13) o w v a n is h e s , t o o . T o s e e t h is , c o m p u t e\nX @ # ( t (cid:0) q ) v\n:\n@ # ( t ) u\nu : u o u t p u t u n it\nW e a k u p p e r b o u n d f o r s c a l i n g f a c t o r ."
  },
  {
    "chunk_id": "doc_4_p4_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "T h e fo llo w in g , s lig h t ly e x t e n d e d v a n is h in g e r r o r\na n a ly s is a ls o t a k e s n , t h e n u m b e r o f u n it s , in t o a c c o u n t ."
  },
  {
    "chunk_id": "doc_4_p4_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "F o r q > 1 , fo r m u la ( 2 ) c a n b e r e w r it t e n\na s\nq (cid:0) 1\nY\n0 0 T 0\n( n e t ( t (cid:0) q ) ) ; ( W T ) F ( t (cid:0) 1 ) ( W F ( t (cid:0) m ) ) W f v v u v\nm = 2\nw h e r e t h e w e ig h t m a t r ix W is d e (cid:12) n e d b y [W ] := w , v 's o u t g o in g w e ig h t v e c t o r W is d e (cid:12) n e d b y ij ij v\n[W ] := [W ] = w , u 's in c o m in g w e ig h t v e c t o r W T is d e (cid:12) n e d b y [W T ] := [W ] = w , a n d fo r v i iv iv i u i u i u u\n0 0\nm = 1 ; : : : ; q , F ( t (cid:0) m ) is t h e d ia g o n a l m a t r ix o f (cid:12) r s t o r d e r d e r iv a t iv e s d e (cid:12) n e d a s : [F ( t (cid:0) m ) ] := 0 ij\n0 0\nif i 6= j , a n d [F ( t (cid:0) m ) ] := f ( n e t ( t (cid:0) m ) ) o t h e r w is e ."
  },
  {
    "chunk_id": "doc_4_p4_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "H e r e T is t h e t r a n s p o s it io n o p e r a t o r , ij i i\n[A ] is t h e e le m e n t in t h e i - t h c o lu m n a n d j - t h r o w o f m a t r ix A , a n d [x ] is t h e i - t h c o m p o n e n t ij i\no f v e c t o r x . 4"
  },
  {
    "chunk_id": "doc_4_p5_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_4_p5_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "F\nw\nT\na\nw\nN\nt\nu\nk\na\nw\nt\nw\nW\n3\nA\na\ne\nr\nN\na\nf\nv\nU s in g a m a t r ix n o r m k : k c o m p a t ib le w it h v e c t o r n o r m k : k , w e d e (cid:12) n e A x\n0 0\nf := m a x f k F ( t (cid:0) m ) k g : m = 1 ;:::;q A m a x\nT\no r m a x f jx jg (cid:20) k x k w e g e t jx y j (cid:20) n k x k k y k : S in c e i= 1 ;:::;n i x x x\n0 0 0\njf ( n e t ( t (cid:0) q ) ) j (cid:20) k F ( t (cid:0) q ) k (cid:20) f ; v A v m a x\ne o b t a in t h e fo llo w in g in e q u a lit y :\n@ # ( t (cid:0) q ) q v q (cid:0) 2 0 q 0\nj j (cid:20) n ( f ) k W k k W T k k W k (cid:20) n ( f k W k ) : v x x A u m a x m a x A\n@ # ( t ) u\nh is in e q u a lit y r e s u lt s fr o m\nk W k = k W e k (cid:20) k W k k e k (cid:20) k W k v x v x A v x A\nn d\nk W T k = k e W k (cid:20) k W k k e k (cid:20) k W k ; x u x A u x A u\nh e r e e is t h e u n it v e c t o r w h o s e c o m p o n e n t s a r e 0 e x c e p t fo r t h e k - t h c o m p o n e n t , w h ic h is 1 ."
  },
  {
    "chunk_id": "doc_4_p5_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "k\n0\no t e t h a t t h is is a w e a k , e x t r e m e c a s e u p p e r b o u n d | it w ill b e r e a c h e d o n ly if a ll k F ( t (cid:0) m ) k A\na k e o n m a x im a l v a lu e s , a n d if t h e c o n t r ib u t io n s o f a ll p a t h s a c r o s s w h ic h e r r o r (cid:13) o w s b a c k fr o m\nn it u t o u n it v h a v e t h e s a m e s ig n . L a r g e k W k , h o w e v e r , t y p ic a lly r e s u lt in s m a ll v a lu e s o f A\n0\nF ( t (cid:0) m ) k , a s c o n (cid:12) r m e d b y e x p e r im e n t s ( s e e , e .g ., H o c h r e it e r 1 9 9 1 ) . A\nF o r e x a m p le , w it h n o r m s\nX\nk W k := m a x jw j A r r s\ns\nn d\nk x k := m a x jx j; x r r\n0\ne h a v e f = 0 :2 5 fo r t h e lo g is t ic s ig m o id ."
  },
  {
    "chunk_id": "doc_4_p5_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "W e o b s e r v e t h a t if m a x\n4 :0\njw j (cid:20) w < 8 i ; j ; ij m a x\nn\n(cid:0) (cid:1)\nn w m a x\nh e n k W k (cid:20) n w < 4 :0 w ill r e s u lt in e x p o n e n t ia l d e c a y | b y s e t t in g (cid:28) := < 1 :0 , A m a x 4 :0\ne o b t a in\n@ # ( t (cid:0) q ) v q\nj j (cid:20) n ( (cid:28) ) :\n@ # ( t ) u\ne r e fe r t o H o c h r e it e r 's 1 9 9 1 t h e s is fo r a d d it io n a l r e s u lt s . . 2 C O N S T A N T E R R O R F L O W : N A I V E A P P R O A C H\ns i n g l e u n i t . T o a v o id v a n is h in g e r r o r s ig n a ls , h o w c a n w e a c h ie v e c o n s t a n t e r r o r (cid:13) o w t h r o u g h\ns in g le u n it j w it h a s in g le c o n n e c t io n t o it s e lf ?"
  },
  {
    "chunk_id": "doc_4_p5_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "A c c o r d in g t o t h e r u le s a b o v e , a t t im e t , j 's lo c a l\n0\nr r o r b a c k (cid:13) o w is # ( t ) = f ( n e t ( t ) ) # ( t + 1 ) w . T o e n fo r c e c o n s t a n t e r r o r (cid:13) o w t h r o u g h j , w e j j j j j j\ne q u ir e\n0\n( n e t ( t ) ) w = 1 :0 : f j j j j\no t e t h e s im ila r it y t o M o z e r 's (cid:12) x e d t im e c o n s t a n t s y s t e m ( 1 9 9 2 ) | a t im e c o n s t a n t o f 1 :0 is\n1\np p r o p r ia t e fo r p o t e n t ia lly in (cid:12) n it e t im e la g s . T h e c o n s t a n t e r r o r c a r r o u s e l . I n t e g r a t in g t h e d i(cid:11) e r e n t ia l e q u a t io n a b o v e , w e o b t a in\nn e t ( t) j\n( n e t ( t ) ) = fo r a r b it r a r y n e t ( t ) ."
  },
  {
    "chunk_id": "doc_4_p5_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "T h is m e a n s : f h a s t o b e lin e a r , a n d u n it j 's a c t i- j j j j w j j\na t io n h a s t o r e m a in c o n s t a n t :\nj j\ny ( t + 1 ) = f ( n e t ( t + 1 ) ) = f ( w y ( t ) ) = y ( t ) : j j j j j j\n1 W e d o n o t u s e t h e e x p r e s s io n \\ t im e c o n s t a n t \" in t h e d i(cid:11) e r e n t ia l s e n s e , a s , e .g ., P e a r lm u t t e r ( 1 9 9 5 ) . 5"
  },
  {
    "chunk_id": "doc_4_p6_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "I n t h e e x p e r im e n t s , t h is w ill b e e n s u r e d b y u s in g t h e id e n t it y fu n c t io n f : f ( x ) = x ; 8 x , a n d b y j j\ns e t t in g w = 1 :0 . W e r e fe r t o t h is a s t h e c o n s t a n t e r r o r c a r r o u s e l ( C E C ) . C E C w ill b e L S T M 's j j\nc e n t r a l fe a t u r e ( s e e S e c t io n 4 ) . O f c o u r s e u n it j w ill n o t o n ly b e c o n n e c t e d t o it s e lf b u t a ls o t o o t h e r u n it s . T h is in v o k e s t w o\no b v io u s , r e la t e d p r o b le m s ( a ls o in h e r e n t in a ll o t h e r g r a d ie n t - b a s e d a p p r o a c h e s ) :\n1 . I n p u t w e i g h t c o n (cid:13) i c t : fo r s im p lic it y , le t u s fo c u s o n a s in g le a d d it io n a l in p u t w e ig h t w ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "j i\nA s s u m e t h a t t h e t o t a l e r r o r c a n b e r e d u c e d b y s w it c h in g o n u n it j in r e s p o n s e t o a c e r t a in in p u t ,\na n d k e e p in g it a c t iv e fo r a lo n g t im e ( u n t il it h e lp s t o c o m p u t e a d e s ir e d o u t p u t ) ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "P r o v id e d i is n o n -\nz e r o , s in c e t h e s a m e in c o m in g w e ig h t h a s t o b e u s e d fo r b o t h s t o r in g c e r t a in in p u t s a n d ig n o r in g\no t h e r s , w w ill o ft e n r e c e iv e c o n (cid:13) ic t in g w e ig h t u p d a t e s ig n a ls d u r in g t h is t im e ( r e c a ll t h a t j is j i\nlin e a r ) : t h e s e s ig n a ls w ill a t t e m p t t o m a k e w p a r t ic ip a t e in ( 1 ) s t o r in g t h e in p u t ( b y s w it c h in g j i\no n j ) a n d ( 2 ) p r o t e c t in g t h e in p u t ( b y p r e v e n t in g j fr o m b e in g s w it c h e d o (cid:11) b y ir r e le v a n t la t e r\nin p u t s ) ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "T h is c o n (cid:13) ic t m a k e s le a r n in g d i(cid:14) c u lt , a n d c a lls fo r a m o r e c o n t e x t - s e n s it iv e m e c h a n is m\nfo r c o n t r o llin g \\ w r it e o p e r a t io n s \" t h r o u g h in p u t w e ig h t s . 2 . O u t p u t w e i g h t c o n (cid:13) i c t : a s s u m e j is s w it c h e d o n a n d c u r r e n t ly s t o r e s s o m e p r e v io u s\nin p u t . F o r s im p lic it y , le t u s fo c u s o n a s in g le a d d it io n a l o u t g o in g w e ig h t w . T h e s a m e w h a s k j k j\nt o b e u s e d fo r b o t h r e t r ie v in g j 's c o n t e n t a t c e r t a in t im e s a n d p r e v e n t in g j fr o m d is t u r b in g k\na t o t h e r t im e s ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "A s lo n g a s u n it j is n o n - z e r o , w w ill a t t r a c t c o n (cid:13) ic t in g w e ig h t u p d a t e s ig n a ls k j\ng e n e r a t e d d u r in g s e q u e n c e p r o c e s s in g : t h e s e s ig n a ls w ill a t t e m p t t o m a k e w p a r t ic ip a t e in ( 1 ) k j\na c c e s s in g t h e in fo r m a t io n s t o r e d in j a n d | a t d i(cid:11) e r e n t t im e s | ( 2 ) p r o t e c t in g u n it k fr o m b e in g\np e r t u r b e d b y j . F o r in s t a n c e , w it h m a n y t a s k s t h e r e a r e c e r t a in \\ s h o r t t im e la g e r r o r s \" t h a t c a n b e\nr e d u c e d in e a r ly t r a in in g s t a g e s ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "H o w e v e r , a t la t e r t r a in in g s t a g e s j m a y s u d d e n ly s t a r t t o c a u s e\na v o id a b le e r r o r s in s it u a t io n s t h a t a lr e a d y s e e m e d u n d e r c o n t r o l b y a t t e m p t in g t o p a r t ic ip a t e in\nr e d u c in g m o r e d i(cid:14) c u lt \\ lo n g t im e la g e r r o r s \" . A g a in , t h is c o n (cid:13) ic t m a k e s le a r n in g d i(cid:14) c u lt , a n d\nc a lls fo r a m o r e c o n t e x t - s e n s it iv e m e c h a n is m fo r c o n t r o llin g \\ r e a d o p e r a t io n s \" t h r o u g h o u t p u t\nw e ig h t s . O f c o u r s e , in p u t a n d o u t p u t w e ig h t c o n (cid:13) ic t s a r e n o t s p e c i(cid:12) c fo r lo n g t im e la g s , b u t o c c u r fo r\ns h o r t t im e la g s a s w e ll."
  },
  {
    "chunk_id": "doc_4_p6_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "T h e ir e (cid:11) e c t s , h o w e v e r , b e c o m e p a r t ic u la r ly p r o n o u n c e d in t h e lo n g t im e\nla g c a s e : a s t h e t im e la g in c r e a s e s , ( 1 ) s t o r e d in fo r m a t io n m u s t b e p r o t e c t e d a g a in s t p e r t u r b a t io n\nfo r lo n g e r a n d lo n g e r p e r io d s , a n d | e s p e c ia lly in a d v a n c e d s t a g e s o f le a r n in g | ( 2 ) m o r e a n d\nm o r e a lr e a d y c o r r e c t o u t p u t s a ls o r e q u ir e p r o t e c t io n a g a in s t p e r t u r b a t io n ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "D u e t o t h e p r o b le m s a b o v e t h e n a iv e a p p r o a c h d o e s n o t w o r k w e ll e x c e p t in c a s e o f c e r t a in\ns im p le p r o b le m s in v o lv in g lo c a l in p u t / o u t p u t r e p r e s e n t a t io n s a n d n o n - r e p e a t in g in p u t p a t t e r n s\n( s e e H o c h r e it e r 1 9 9 1 a n d S ilv a e t a l. 1 9 9 6 ) . T h e n e x t s e c t io n s h o w s h o w t o d o it r ig h t . 4 L O N G S H O R T - T E R M M E M O R Y\nM e m o r y c e l l s a n d g a t e u n i t s ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "T o c o n s t r u c t a n a r c h it e c t u r e t h a t a llo w s fo r c o n s t a n t e r r o r (cid:13) o w\nt h r o u g h s p e c ia l, s e lf- c o n n e c t e d u n it s w it h o u t t h e d is a d v a n t a g e s o f t h e n a iv e a p p r o a c h , w e e x t e n d\nt h e c o n s t a n t e r r o r c a r r o u s e l C E C e m b o d ie d b y t h e s e lf- c o n n e c t e d , lin e a r u n it j fr o m S e c t io n 3 .2\nb y in t r o d u c in g a d d it io n a l fe a t u r e s . A m u lt ip lic a t iv e in p u t g a t e u n it is in t r o d u c e d t o p r o t e c t t h e\nm e m o r y c o n t e n t s s t o r e d in j fr o m p e r t u r b a t io n b y ir r e le v a n t in p u t s ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "L ik e w is e , a m u lt ip lic a t iv e\no u t p u t g a t e u n it is in t r o d u c e d w h ic h p r o t e c t s o t h e r u n it s fr o m p e r t u r b a t io n b y c u r r e n t ly ir r e le v a n t\nm e m o r y c o n t e n t s s t o r e d in j . T h e r e s u lt in g , m o r e c o m p le x u n it is c a lle d a m e m o r y c e ll ( s e e F ig u r e 1 ) . T h e j - t h m e m o r y c e ll\nis d e n o t e d c . E a c h m e m o r y c e ll is b u ilt a r o u n d a c e n t r a l lin e a r u n it w it h a (cid:12) x e d s e lf- c o n n e c t io n j\n( t h e C E C ) . I n a d d it io n t o n e t , c g e t s in p u t fr o m a m u lt ip lic a t iv e u n it o u t ( t h e \\ o u t p u t g a t e \" ) , c j j j\na n d fr o m a n o t h e r m u lt ip lic a t iv e u n it i n ( t h e \\ in p u t g a t e \" ) ."
  },
  {
    "chunk_id": "doc_4_p6_fixed_10",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "i n 's a c t iv a t io n a t t im e t is d e n o t e d j j\nin o u t j j b y y ( t ) , o u t 's b y y ( t ) . W e h a v e j\no u t in j j\n( t ) ) ; y ( t ) = f ( n e t ( t ) ) ; y ( t ) = f ( n e t o u t in in o u tj j j j\n6"
  },
  {
    "chunk_id": "doc_4_p7_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "w\na\nW\nT\nh\nt\na\n(\nw\nw\nT\no\nh e r e X\nu\nn e t ( t ) = w y ( t (cid:0) 1 ) ; o u t o u t u j j\nu\nn d X\nu\nn e t ( t ) = w y ( t (cid:0) 1 ) : in in u j j\nu\ne a ls o h a v e X\nu\nn e t ( t ) = w y ( t (cid:0) 1 ) : c c u j j\nu\nh e s u m m a t io n in d ic e s u m a y s t a n d fo r in p u t u n it s , g a t e u n it s , m e m o r y c e lls , o r e v e n c o n v e n t io n a l\nid d e n u n it s if t h e r e a r e a n y ( s e e a ls o p a r a g r a p h o n \\ n e t w o r k t o p o lo g y \" b e lo w ) . A ll t h e s e d i(cid:11) e r e n t\ny p e s o f u n it s m a y c o n v e y u s e fu l in fo r m a t io n a b o u t t h e c u r r e n t s t a t e o f t h e n e t ."
  },
  {
    "chunk_id": "doc_4_p7_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "F o r in s t a n c e ,\nn in p u t g a t e ( o u t p u t g a t e ) m a y u s e in p u t s fr o m o t h e r m e m o r y c e lls t o d e c id e w h e t h e r t o s t o r e\na c c e s s ) c e r t a in in fo r m a t io n in it s m e m o r y c e ll. T h e r e e v e n m a y b e r e c u r r e n t s e lf- c o n n e c t io n s lik e\n. I t is u p t o t h e u s e r t o d e (cid:12) n e t h e n e t w o r k t o p o lo g y . S e e F ig u r e 2 fo r a n e x a m p le ."
  },
  {
    "chunk_id": "doc_4_p7_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "c c j j\nc j A t t im e t , c 's o u t p u t y ( t ) is c o m p u t e d a s j\nc o u t j j\ny ( t ) = y ( t ) h ( s ( t ) ) ; c j\nh e r e t h e \\ in t e r n a l s t a t e \" s ( t ) is c j\n(cid:0) (cid:1)\nin j\ns ( 0 ) = 0 ; s ( t ) = s ( t (cid:0) 1 ) + y ( t ) g n e t ( t ) fo r t > 0 : c c c c j j j j\nh e d i(cid:11) e r e n t ia b le fu n c t io n g s q u a s h e s n e t ; t h e d i(cid:11) e r e n t ia b le fu n c t io n h s c a le s m e m o r y c e ll c j\nu t p u t s c o m p u t e d fr o m t h e in t e r n a l s t a t e s . c j\nnet s =s + g yin yc\nj j\nc c c\nj j j\n1.0\ng g yin h h yout\nj j\nw\nw ic\nyin yout j\nc i j j\nj\nnet net\nin out\nw j w j\nini out i\nj j\nF\nc\nt\na\nc\n(cid:13)\no\nw\n(cid:13)\ng\nig u r e 1 : A r c h it e c t u r e o f m e m o r y c e ll c ( t h e b o x ) a n d it s g a t e u n it s i n ; o u t ."
  },
  {
    "chunk_id": "doc_4_p7_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "T h e s e lf- r e c u r r e n t j j j\no n n e c t io n ( w it h w e ig h t 1 .0 ) in d ic a t e s fe e d b a c k w it h a d e la y o f 1 t im e s t e p . I t b u ild s t h e b a s is o f\nh e \\ c o n s t a n t e r r o r c a r r o u s e l\" C E C . T h e g a t e u n it s o p e n a n d c lo s e a c c e s s t o C E C . S e e t e x t a n d\np p e n d ix A .1 fo r d e t a ils . W h y g a t e u n i t s ? T o a v o id in p u t w e ig h t c o n (cid:13) ic t s , i n c o n t r o ls t h e e r r o r (cid:13) o w t o m e m o r y c e ll j\n's in p u t c o n n e c t io n s w . T o c ir c u m v e n t c 's o u t p u t w e ig h t c o n (cid:13) ic t s , o u t c o n t r o ls t h e e r r o r j c i j j j\no w fr o m u n it j 's o u t p u t c o n n e c t io n s ."
  },
  {
    "chunk_id": "doc_4_p7_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "I n o t h e r w o r d s , t h e n e t c a n u s e i n t o d e c id e w h e n t o k e e p j\nr o v e r r id e in fo r m a t io n in m e m o r y c e ll c , a n d o u t t o d e c id e w h e n t o a c c e s s m e m o r y c e ll c a n d j j j\nh e n t o p r e v e n t o t h e r u n it s fr o m b e in g p e r t u r b e d b y c ( s e e F ig u r e 1 ) . j\nE r r o r s ig n a ls t r a p p e d w it h in a m e m o r y c e ll's C E C c a n n o t c h a n g e { b u t d i(cid:11) e r e n t e r r o r s ig n a ls\no w in g in t o t h e c e ll ( a t d i(cid:11) e r e n t t im e s ) v ia it s o u t p u t g a t e m a y g e t s u p e r im p o s e d . T h e o u t p u t\na t e w ill h a v e t o le a r n w h ic h e r r o r s t o t r a p in it s C E C , b y a p p r o p r ia t e ly s c a lin g t h e m . T h e in p u t\n7"
  },
  {
    "chunk_id": "doc_4_p8_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "g a t e w ill h a v e t o le a r n w h e n t o r e le a s e e r r o r s , a g a in b y a p p r o p r ia t e ly s c a lin g t h e m . E s s e n t ia lly ,\nt h e m u lt ip lic a t iv e g a t e u n it s o p e n a n d c lo s e a c c e s s t o c o n s t a n t e r r o r (cid:13) o w t h r o u g h C E C . D is t r ib u t e d o u t p u t r e p r e s e n t a t io n s t y p ic a lly d o r e q u ir e o u t p u t g a t e s . N o t a lw a y s a r e b o t h\ng a t e t y p e s n e c e s s a r y , t h o u g h | o n e m a y b e s u (cid:14) c ie n t . F o r in s t a n c e , in E x p e r im e n t s 2 a a n d 2 b in\nS e c t io n 5 , it w ill b e p o s s ib le t o u s e in p u t g a t e s o n ly ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "I n fa c t , o u t p u t g a t e s a r e n o t r e q u ir e d in c a s e\no f lo c a l o u t p u t e n c o d in g | p r e v e n t in g m e m o r y c e lls fr o m p e r t u r b in g a lr e a d y le a r n e d o u t p u t s c a n\nb e d o n e b y s im p ly s e t t in g t h e c o r r e s p o n d in g w e ig h t s t o z e r o . E v e n in t h is c a s e , h o w e v e r , o u t p u t\ng a t e s c a n b e b e n e (cid:12) c ia l: t h e y p r e v e n t t h e n e t 's a t t e m p t s a t s t o r in g lo n g t im e la g m e m o r ie s ( w h ic h\na r e u s u a lly h a r d t o le a r n ) fr o m p e r t u r b in g a c t iv a t io n s r e p r e s e n t in g e a s ily le a r n a b le s h o r t t im e la g\nm e m o r ie s . ( T h is w ill p r o v e q u it e u s e fu l in E x p e r im e n t 1 , fo r in s t a n c e .)"
  },
  {
    "chunk_id": "doc_4_p8_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "N e t w o r k t o p o l o g y . W e u s e n e t w o r k s w it h o n e in p u t la y e r , o n e h id d e n la y e r , a n d o n e o u t p u t\nla y e r . T h e ( fu lly ) s e lf- c o n n e c t e d h id d e n la y e r c o n t a in s m e m o r y c e lls a n d c o r r e s p o n d in g g a t e u n it s\n( fo r c o n v e n ie n c e , w e r e fe r t o b o t h m e m o r y c e lls a n d g a t e u n it s a s b e in g lo c a t e d in t h e h id d e n\nla y e r ) . T h e h id d e n la y e r m a y a ls o c o n t a in \\ c o n v e n t io n a l\" h id d e n u n it s p r o v id in g in p u t s t o g a t e\nu n it s a n d m e m o r y c e lls ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "A ll u n it s ( e x c e p t fo r g a t e u n it s ) in a ll la y e r s h a v e d ir e c t e d c o n n e c t io n s\n( s e r v e a s in p u t s ) t o a ll u n it s in t h e la y e r a b o v e ( o r t o a ll h ig h e r la y e r s { E x p e r im e n t s 2 a a n d 2 b ) . M e m o r y c e l l b l o c k s . S m e m o r y c e lls s h a r in g t h e s a m e in p u t g a t e a n d t h e s a m e o u t p u t g a t e\nfo r m a s t r u c t u r e c a lle d a \\ m e m o r y c e ll b lo c k o f s iz e S \" . M e m o r y c e ll b lo c k s fa c ilit a t e in fo r m a t io n\ns t o r a g e | a s w it h c o n v e n t io n a l n e u r a l n e t s , it is n o t s o e a s y t o c o d e a d is t r ib u t e d in p u t w it h in a\ns in g le c e ll."
  },
  {
    "chunk_id": "doc_4_p8_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "S in c e e a c h m e m o r y c e ll b lo c k h a s a s m a n y g a t e u n it s a s a s in g le m e m o r y c e ll ( n a m e ly\nt w o ) , t h e b lo c k a r c h it e c t u r e c a n b e e v e n s lig h t ly m o r e e (cid:14) c ie n t ( s e e p a r a g r a p h \\ c o m p u t a t io n a l\nc o m p le x it y \" ) . A m e m o r y c e ll b lo c k o f s iz e 1 is ju s t a s im p le m e m o r y c e ll. I n t h e e x p e r im e n t s\n( S e c t io n 5 ) , w e w ill u s e m e m o r y c e ll b lo c k s o f v a r io u s s iz e s . L e a r n i n g . W e u s e a v a r ia n t o f R T R L ( e .g ., R o b in s o n a n d F a lls id e 1 9 8 7 ) w h ic h p r o p e r ly t a k e s\nin t o a c c o u n t t h e a lt e r e d , m u lt ip lic a t iv e d y n a m ic s c a u s e d b y in p u t a n d o u t p u t g a t e s ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "H o w e v e r , t o\ne n s u r e n o n - d e c a y in g e r r o r b a c k p r o p t h r o u g h in t e r n a l s t a t e s o f m e m o r y c e lls , a s w it h t r u n c a t e d\nB P T T ( e .g ., W illia m s a n d P e n g 1 9 9 0 ) , e r r o r s a r r iv in g a t \\ m e m o r y c e ll n e t in p u t s \" ( fo r c e ll c , t h is j\nin c lu d e s n e t , n e t , n e t ) d o n o t g e t p r o p a g a t e d b a c k fu r t h e r in t im e ( a lt h o u g h t h e y d o s e r v e c in o u t j j j\n2\nt o c h a n g e t h e in c o m in g w e ig h t s ) . O n ly w it h in m e m o r y c e lls , e r r o r s a r e p r o p a g a t e d b a c k t h r o u g h\np r e v io u s in t e r n a l s t a t e s s ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "T o v is u a liz e t h is : o n c e a n e r r o r s ig n a l a r r iv e s a t a m e m o r y c e ll o u t p u t , c j\n0\nit g e t s s c a le d b y o u t p u t g a t e a c t iv a t io n a n d h . T h e n it is w it h in t h e m e m o r y c e ll's C E C , w h e r e it\nc a n (cid:13) o w b a c k in d e (cid:12) n it e ly w it h o u t e v e r b e in g s c a le d . O n ly w h e n it le a v e s t h e m e m o r y c e ll t h r o u g h\n0\nt h e in p u t g a t e a n d g , it is s c a le d o n c e m o r e b y in p u t g a t e a c t iv a t io n a n d g . I t t h e n s e r v e s t o\nc h a n g e t h e in c o m in g w e ig h t s b e fo r e it is t r u n c a t e d ( s e e a p p e n d ix fo r e x p lic it fo r m u la e ) . C o m p u t a t i o n a l c o m p l e x i t y ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "A s w it h M o z e r 's fo c u s e d r e c u r r e n t b a c k p r o p a lg o r it h m ( M o z e r\n@ s c j\n1 9 8 9 ) , o n ly t h e d e r iv a t iv e s n e e d t o b e s t o r e d a n d u p d a t e d . H e n c e t h e L S T M a lg o r it h m is\n@ w il\nv e r y e (cid:14) c ie n t , w it h a n e x c e lle n t u p d a t e c o m p le x it y o f O ( W ) , w h e r e W t h e n u m b e r o f w e ig h t s ( s e e\nd e t a ils in a p p e n d ix A .1 ) . H e n c e , L S T M a n d B P T T fo r fu lly r e c u r r e n t n e t s h a v e t h e s a m e u p d a t e\nc o m p le x it y p e r t im e s t e p ( w h ile R T R L 's is m u c h w o r s e ) ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "U n lik e fu ll B P T T , h o w e v e r , L S T M is\n3\n: t h e r e is n o n e e d t o s t o r e a c t iv a t io n v a lu e s o b s e r v e d d u r in g s e q u e n c e lo c a l in s p a c e a n d t im e\np r o c e s s in g in a s t a c k w it h p o t e n t ia lly u n lim it e d s iz e . A b u s e p r o b l e m a n d s o l u t i o n s . I n t h e b e g in n in g o f t h e le a r n in g p h a s e , e r r o r r e d u c t io n\nm a y b e p o s s ib le w it h o u t s t o r in g in fo r m a t io n o v e r t im e . T h e n e t w o r k w ill t h u s t e n d t o a b u s e\nm e m o r y c e lls , e .g ., a s b ia s c e lls ( i.e ., it m ig h t m a k e t h e ir a c t iv a t io n s c o n s t a n t a n d u s e t h e o u t g o in g\nc o n n e c t io n s a s a d a p t iv e t h r e s h o ld s fo r o t h e r u n it s ) ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "T h e p o t e n t ia l d i(cid:14) c u lt y is : it m a y t a k e a\nlo n g t im e t o r e le a s e a b u s e d m e m o r y c e lls a n d m a k e t h e m a v a ila b le fo r fu r t h e r le a r n in g . A s im ila r\n\\ a b u s e p r o b le m \" a p p e a r s if t w o m e m o r y c e lls s t o r e t h e s a m e ( r e d u n d a n t ) in fo r m a t io n ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_10",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "T h e r e a r e\na t le a s t t w o s o lu t io n s t o t h e a b u s e p r o b le m : ( 1 ) S e q u e n t ia l n e t w o r k c o n s t r u c t io n ( e .g ., F a h lm a n\n1 9 9 1 ) : a m e m o r y c e ll a n d t h e c o r r e s p o n d in g g a t e u n it s a r e a d d e d t o t h e n e t w o r k w h e n e v e r t h e\n2 F o r in t r a -c e llu la r b a c k p r o p in a q u it e d i(cid:11) e r e n t c o n t e x t s e e a ls o D o y a a n d Y o s h iz a w a ( 1 9 8 9 ) . 3 F o llo w in g S c h m id h u b e r ( 1 9 8 9 ) , w e s a y t h a t a r e c u r r e n t n e t a lg o r it h m is lo c a l in s p a c e if t h e u p d a t e c o m p le x ity\np e r t im e s t e p a n d w e ig h t d o e s n o t d e p e n d o n n e tw o r k s iz e ."
  },
  {
    "chunk_id": "doc_4_p8_fixed_11",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "W e s a y t h a t a m e t h o d is lo c a l in tim e if it s s t o r a g e\nr e q u ir e m e n t s d o n o t d e p e n d o n in p u t s e q u e n c e le n g t h . F o r in s t a n c e , R T R L is lo c a l in t im e b u t n o t in s p a c e . B P T T\nis lo c a l in s p a c e b u t n o t in t im e . 8"
  },
  {
    "chunk_id": "doc_4_p9_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "output\nout 1 out 2\ncell1 cell2 cell1 cell 2\nhidden\nblock block block block\n1 1 2 2\nin 1 in 2\ninput\nF ig u r e 2 : E x a m p le o f a n e t w it h 8 in p u t u n it s , 4 o u t p u t u n it s , a n d 2 m e m o r y c e ll b lo c k s o f s iz e 2 . i n 1 m a r k s t h e in p u t g a t e , o u t 1 m a r k s t h e o u t p u t g a t e , a n d c e l l 1 = b l o c k 1 m a r k s t h e (cid:12) r s t m e m o r y\nc e ll o f b lo c k 1 . c e l l 1 = b l o c k 1 's a r c h it e c t u r e is id e n t ic a l t o t h e o n e in F ig u r e 1 , w it h g a t e u n it s\ni n 1 a n d o u t 1 ( n o t e t h a t b y r o t a t in g F ig u r e 1 b y 9 0 d e g r e e s a n t i- c lo c k w is e , it w ill m a t c h w it h t h e\nc o r r e s p o n d in g p a r t s o f F ig u r e 1 ) ."
  },
  {
    "chunk_id": "doc_4_p9_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "T h e e x a m p le a s s u m e s d e n s e c o n n e c t iv it y : e a c h g a t e u n it a n d\ne a c h m e m o r y c e ll s e e a ll n o n - o u t p u t u n it s . F o r s im p lic it y , h o w e v e r , o u t g o in g w e ig h t s o f o n ly\no n e t y p e o f u n it a r e s h o w n fo r e a c h la y e r . W it h t h e e (cid:14) c ie n t , t r u n c a t e d u p d a t e r u le , e r r o r (cid:13) o w s\no n ly t h r o u g h c o n n e c t io n s t o o u t p u t u n it s , a n d t h r o u g h (cid:12) x e d s e lf- c o n n e c t io n s w it h in c e ll b lo c k s ( n o t\ns h o w n h e r e | s e e F ig u r e 1 ) . E r r o r (cid:13) o w is t r u n c a t e d o n c e it \\ w a n t s \" t o le a v e m e m o r y c e lls o r\ng a t e u n it s ."
  },
  {
    "chunk_id": "doc_4_p9_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "T h e r e fo r e , n o c o n n e c t io n s h o w n a b o v e s e r v e s t o p r o p a g a t e e r r o r b a c k t o t h e u n it fr o m\nw h ic h t h e c o n n e c t io n o r ig in a t e s ( e x c e p t fo r c o n n e c t io n s t o o u t p u t u n it s ) , a lt h o u g h t h e c o n n e c t io n s\nt h e m s e lv e s a r e m o d i(cid:12) a b le . T h a t 's w h y t h e t r u n c a t e d L S T M a lg o r it h m is s o e (cid:14) c ie n t , d e s p it e it s\na b ilit y t o b r id g e v e r y lo n g t im e la g s . S e e t e x t a n d a p p e n d ix A .1 fo r d e t a ils . F ig u r e 2 a c t u a lly s h o w s\nt h e a r c h it e c t u r e u s e d fo r E x p e r im e n t 6 a | o n ly t h e b ia s o f t h e n o n - in p u t u n it s is o m it t e d ."
  },
  {
    "chunk_id": "doc_4_p9_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "e r r o r s t o p s d e c r e a s in g ( s e e E x p e r im e n t 2 in S e c t io n 5 ) . ( 2 ) O u t p u t g a t e b ia s : e a c h o u t p u t g a t e g e t s\na n e g a t iv e in it ia l b ia s , t o p u s h in it ia l m e m o r y c e ll a c t iv a t io n s t o w a r d s z e r o . M e m o r y c e lls w it h\nm o r e n e g a t iv e b ia s a u t o m a t ic a lly g e t \\ a llo c a t e d \" la t e r ( s e e E x p e r im e n t s 1 , 3 , 4 , 5 , 6 in S e c t io n 5 ) . I n t e r n a l s t a t e d r i f t a n d r e m e d i e s . I f m e m o r y c e ll c 's in p u t s a r e m o s t ly p o s it iv e o r m o s t ly j\nn e g a t iv e , t h e n it s in t e r n a l s t a t e s w ill t e n d t o d r ift a w a y o v e r t im e ."
  },
  {
    "chunk_id": "doc_4_p9_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "T h is is p o t e n t ia lly d a n g e r o u s , j\n0\nfo r t h e h ( s ) w ill t h e n a d o p t v e r y s m a ll v a lu e s , a n d t h e g r a d ie n t w ill v a n is h . O n e w a y t o c ir - j\nc u m v e n t t h is p r o b le m is t o c h o o s e a n a p p r o p r ia t e fu n c t io n h . B u t h ( x ) = x , fo r in s t a n c e , h a s t h e\nd is a d v a n t a g e o f u n r e s t r ic t e d m e m o r y c e ll o u t p u t r a n g e . O u r s im p le b u t e (cid:11) e c t iv e w a y o f s o lv in g\nd r ift p r o b le m s a t t h e b e g in n in g o f le a r n in g is t o in it ia lly b ia s t h e in p u t g a t e i n t o w a r d s z e r o ."
  },
  {
    "chunk_id": "doc_4_p9_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "j\n0 in j A lt h o u g h t h e r e is a t r a d e o (cid:11) b e t w e e n t h e m a g n it u d e s o f h ( s ) o n t h e o n e h a n d a n d o f y a n d j\n0\nf o n t h e o t h e r , t h e p o t e n t ia l n e g a t iv e e (cid:11) e c t o f in p u t g a t e b ia s is n e g lig ib le c o m p a r e d t o t h e o n e in j\no f t h e d r ift in g e (cid:11) e c t . W it h lo g is t ic s ig m o id a c t iv a t io n fu n c t io n s , t h e r e a p p e a r s t o b e n o n e e d fo r\n(cid:12) n e - t u n in g t h e in it ia l b ia s , a s c o n (cid:12) r m e d b y E x p e r im e n t s 4 a n d 5 in S e c t io n 5 .4 . 5 E X P E R I M E N T S\nI n t r o d u c t i o n . W h ic h t a s k s a r e a p p r o p r ia t e t o d e m o n s t r a t e t h e q u a lit y o f a n o v e l lo n g t im e la g\n9"
  },
  {
    "chunk_id": "doc_4_p10_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "a lg o r it h m ? F ir s t o f a ll, m in im a l t im e la g s b e t w e e n r e le v a n t in p u t s ig n a ls a n d c o r r e s p o n d in g t e a c h e r\ns ig n a ls m u s t b e lo n g fo r a ll t r a in in g s e q u e n c e s . I n fa c t , m a n y p r e v io u s r e c u r r e n t n e t a lg o r it h m s\ns o m e t im e s m a n a g e t o g e n e r a liz e fr o m v e r y s h o r t t r a in in g s e q u e n c e s t o v e r y lo n g t e s t s e q u e n c e s . S e e , e .g ., P o lla c k ( 1 9 9 1 ) . B u t a r e a l lo n g t im e la g p r o b le m d o e s n o t h a v e a n y s h o r t t im e la g\ne x e m p la r s in t h e t r a in in g s e t ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "F o r in s t a n c e , E lm a n 's t r a in in g p r o c e d u r e , B P T T , o (cid:15) in e R T R L ,\no n lin e R T R L , e t c ., fa il m is e r a b ly o n r e a l lo n g t im e la g p r o b le m s . S e e , e .g ., H o c h r e it e r ( 1 9 9 1 ) a n d\nM o z e r ( 1 9 9 2 ) . A s e c o n d im p o r t a n t r e q u ir e m e n t is t h a t t h e t a s k s s h o u ld b e c o m p le x e n o u g h s u c h\nt h a t t h e y c a n n o t b e s o lv e d q u ic k ly b y s im p le - m in d e d s t r a t e g ie s s u c h a s r a n d o m w e ig h t g u e s s in g . G u e s s i n g c a n o u t p e r f o r m m a n y l o n g t i m e l a g a l g o r i t h m s ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "R e c e n t ly w e d is c o v e r e d\n( S c h m id h u b e r a n d H o c h r e it e r 1 9 9 6 , H o c h r e it e r a n d S c h m id h u b e r 1 9 9 6 , 1 9 9 7 ) t h a t m a n y lo n g\nt im e la g t a s k s u s e d in p r e v io u s w o r k c a n b e s o lv e d m o r e q u ic k ly b y s im p le r a n d o m w e ig h t g u e s s in g\nt h a n b y t h e p r o p o s e d a lg o r it h m s . F o r in s t a n c e , g u e s s in g s o lv e d a v a r ia n t o f B e n g io a n d F r a s c o n i's\n4\n\\ p a r it y p r o b le m \" ( 1 9 9 4 ) p r o b le m m u c h fa s t e r t h a n t h e s e v e n m e t h o d s t e s t e d b y B e n g io e t a l.\n( 1 9 9 4 ) a n d B e n g io a n d F r a s c o n i ( 1 9 9 4 ) . S im ila r ly fo r s o m e o f M ille r a n d G ile s ' p r o b le m s ( 1 9 9 3 ) ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "O f\nc o u r s e , t h is d o e s n o t m e a n t h a t g u e s s in g is a g o o d a lg o r it h m . I t ju s t m e a n s t h a t s o m e p r e v io u s ly\nu s e d p r o b le m s a r e n o t e x t r e m e ly a p p r o p r ia t e t o d e m o n s t r a t e t h e q u a lit y o f p r e v io u s ly p r o p o s e d\na lg o r it h m s . W h a t ' s c o m m o n t o E x p e r i m e n t s 1 { 6 . A ll o u r e x p e r im e n t s ( e x c e p t fo r E x p e r im e n t 1 )\nin v o lv e lo n g m in im a l t im e la g s | t h e r e a r e n o s h o r t t im e la g t r a in in g e x e m p la r s fa c ilit a t in g\nle a r n in g . S o lu t io n s t o m o s t o f o u r t a s k s a r e s p a r s e in w e ig h t s p a c e ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "T h e y r e q u ir e e it h e r m a n y\np a r a m e t e r s / in p u t s o r h ig h w e ig h t p r e c is io n , s u c h t h a t r a n d o m w e ig h t g u e s s in g b e c o m e s in fe a s ib le . W e a lw a y s u s e o n - lin e le a r n in g ( a s o p p o s e d t o b a t c h le a r n in g ) , a n d lo g is t ic s ig m o id s a s a c t i-\nv a t io n fu n c t io n s . F o r E x p e r im e n t s 1 a n d 2 , in it ia l w e ig h t s a r e c h o s e n in t h e r a n g e [(cid:0) 0 :2 ; 0 :2 ], fo r\nt h e o t h e r e x p e r im e n t s in [(cid:0) 0 :1 ; 0 :1 ]. T r a in in g s e q u e n c e s a r e g e n e r a t e d r a n d o m ly a c c o r d in g t o t h e\nv a r io u s t a s k d e s c r ip t io n s ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "I n s lig h t d e v ia t io n fr o m t h e n o t a t io n in A p p e n d ix A 1 , e a c h d is c r e t e\nt im e s t e p o f e a c h in p u t s e q u e n c e in v o lv e s t h r e e p r o c e s s in g s t e p s : ( 1 ) u s e c u r r e n t in p u t t o s e t t h e\nin p u t u n it s . ( 2 ) C o m p u t e a c t iv a t io n s o f h id d e n u n it s ( in c lu d in g in p u t g a t e s , o u t p u t g a t e s , m e m -\no r y c e lls ) . ( 3 ) C o m p u t e o u t p u t u n it a c t iv a t io n s . E x c e p t fo r E x p e r im e n t s 1 , 2 a , a n d 2 b , s e q u e n c e\ne le m e n t s a r e r a n d o m ly g e n e r a t e d o n - lin e , a n d e r r o r s ig n a ls a r e g e n e r a t e d o n ly a t s e q u e n c e e n d s ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "N e t a c t iv a t io n s a r e r e s e t a ft e r e a c h p r o c e s s e d in p u t s e q u e n c e . F o r c o m p a r is o n s w it h r e c u r r e n t n e t s t a u g h t b y g r a d ie n t d e s c e n t , w e g iv e r e s u lt s o n ly fo r R T R L ,\ne x c e p t fo r c o m p a r is o n 2 a , w h ic h a ls o in c lu d e s B P T T . N o t e , h o w e v e r , t h a t u n t r u n c a t e d B P T T ( s e e ,\ne .g ., W illia m s a n d P e n g 1 9 9 0 ) c o m p u t e s e x a c t ly t h e s a m e g r a d ie n t a s o (cid:15) in e R T R L ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "W it h lo n g t im e\nla g p r o b le m s , o (cid:15) in e R T R L ( o r B P T T ) a n d t h e o n lin e v e r s io n o f R T R L ( n o a c t iv a t io n r e s e t s , o n lin e\nw e ig h t c h a n g e s ) le a d t o a lm o s t id e n t ic a l, n e g a t iv e r e s u lt s ( a s c o n (cid:12) r m e d b y a d d it io n a l s im u la t io n s\nin H o c h r e it e r 1 9 9 1 ; s e e a ls o M o z e r 1 9 9 2 ) . T h is is b e c a u s e o (cid:15) in e R T R L , o n lin e R T R L , a n d fu ll\nB P T T a ll s u (cid:11) e r b a d ly fr o m e x p o n e n t ia l e r r o r d e c a y . O u r L S T M a r c h it e c t u r e s a r e s e le c t e d q u it e a r b it r a r ily ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "I f n o t h in g is k n o w n a b o u t t h e c o m p le x -\nit y o f a g iv e n p r o b le m , a m o r e s y s t e m a t ic a p p r o a c h w o u ld b e : s t a r t w it h a v e r y s m a ll n e t c o n s is t in g\no f o n e m e m o r y c e ll. I f t h is d o e s n o t w o r k , t r y t w o c e lls , e t c . A lt e r n a t iv e ly , u s e s e q u e n t ia l n e t w o r k\nc o n s t r u c t io n ( e .g ., F a h lm a n 1 9 9 1 ) . O u t l i n e o f e x p e r i m e n t s . (cid:15) E x p e r im e n t 1 fo c u s e s o n a s t a n d a r d b e n c h m a r k t e s t fo r r e c u r r e n t n e t s : t h e e m b e d d e d R e b e r\ng r a m m a r . S in c e it a llo w s fo r t r a in in g s e q u e n c e s w it h s h o r t t im e la g s , it is n o t a lo n g t im e\nla g p r o b le m ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "W e in c lu d e it b e c a u s e ( 1 ) it p r o v id e s a n ic e e x a m p le w h e r e L S T M 's o u t p u t\ng a t e s a r e t r u ly b e n e (cid:12) c ia l, a n d ( 2 ) it is a p o p u la r b e n c h m a r k fo r r e c u r r e n t n e t s t h a t h a s b e e n\nu s e d b y m a n y a u t h o r s | w e w a n t t o in c lu d e a t le a s t o n e e x p e r im e n t w h e r e c o n v e n t io n a l\nB P T T a n d R T R L d o n o t fa il c o m p le t e ly ( L S T M , h o w e v e r , c le a r ly o u t p e r fo r m s t h e m ) . T h e\ne m b e d d e d R e b e r g r a m m a r 's m in im a l t im e la g s r e p r e s e n t a b o r d e r c a s e in t h e s e n s e t h a t it\nis s t ill p o s s ib le t o le a r n t o b r id g e t h e m w it h c o n v e n t io n a l a lg o r it h m s ."
  },
  {
    "chunk_id": "doc_4_p10_fixed_10",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "O n ly s lig h t ly lo n g e r\n4 It s h o u ld b e m e n t io n e d , h o w e v e r , t h a t d i(cid:11) e r e n t in p u t r e p r e s e n t a t io n s a n d d i(cid:11) e r e n t ty p e s o f n o is e m a y le a d t o\nw o r s e g u e s s in g p e r fo r m a n c e ( Y o s h u a B e n g io , p e r s o n a l c o m m u n ic a t io n , 1 9 9 6 ) . 1 0"
  },
  {
    "chunk_id": "doc_4_p11_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "m in im a l t im e la g s w o u ld m a k e t h is a lm o s t im p o s s ib le . T h e m o r e in t e r e s t in g t a s k s in o u r\np a p e r , h o w e v e r , a r e t h o s e t h a t R T R L , B P T T , e t c . c a n n o t s o lv e a t a ll. (cid:15) E x p e r im e n t 2 fo c u s e s o n n o is e - fr e e a n d n o is y s e q u e n c e s in v o lv in g n u m e r o u s in p u t s y m b o ls\nd is t r a c t in g fr o m t h e fe w im p o r t a n t o n e s . T h e m o s t d i(cid:14) c u lt t a s k ( T a s k 2 c ) in v o lv e s h u n d r e d s\no f d is t r a c t o r s y m b o ls a t r a n d o m p o s it io n s , a n d m in im a l t im e la g s o f 1 0 0 0 s t e p s ."
  },
  {
    "chunk_id": "doc_4_p11_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "L S T M s o lv e s\nit , w h ile B P T T a n d R T R L a lr e a d y fa il in c a s e o f 1 0 - s t e p m in im a l t im e la g s ( s e e a ls o , e .g .,\nH o c h r e it e r 1 9 9 1 a n d M o z e r 1 9 9 2 ) . F o r t h is r e a s o n R T R L a n d B P T T a r e o m it t e d in t h e\nr e m a in in g , m o r e c o m p le x e x p e r im e n t s , a ll o f w h ic h in v o lv e m u c h lo n g e r t im e la g s . (cid:15) E x p e r im e n t 3 a d d r e s s e s lo n g t im e la g p r o b le m s w it h n o is e a n d s ig n a l o n t h e s a m e in p u t\nlin e . E x p e r im e n t s 3 a / 3 b fo c u s o n B e n g io e t a l.'s 1 9 9 4 \\ 2 - s e q u e n c e p r o b le m \" ."
  },
  {
    "chunk_id": "doc_4_p11_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "B e c a u s e\nt h is p r o b le m a c t u a lly c a n b e s o lv e d q u ic k ly b y r a n d o m w e ig h t g u e s s in g , w e a ls o in c lu d e a\nfa r m o r e d i(cid:14) c u lt 2 - s e q u e n c e p r o b le m ( 3 c ) w h ic h r e q u ir e s t o le a r n r e a l- v a lu e d , c o n d it io n a l\ne x p e c t a t io n s o f n o is y t a r g e t s , g iv e n t h e in p u t s . (cid:15) E x p e r im e n t s 4 a n d 5 in v o lv e d is t r ib u t e d , c o n t in u o u s - v a lu e d in p u t r e p r e s e n t a t io n s a n d r e q u ir e\nle a r n in g t o s t o r e p r e c is e , r e a l v a lu e s fo r v e r y lo n g t im e p e r io d s . R e le v a n t in p u t s ig n a ls\nc a n o c c u r a t q u it e d i(cid:11) e r e n t p o s it io n s in in p u t s e q u e n c e s ."
  },
  {
    "chunk_id": "doc_4_p11_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "A g a in m in im a l t im e la g s in v o lv e\nh u n d r e d s o f s t e p s . S im ila r t a s k s n e v e r h a v e b e e n s o lv e d b y o t h e r r e c u r r e n t n e t a lg o r it h m s . (cid:15) E x p e r im e n t 6 in v o lv e s t a s k s o f a d i(cid:11) e r e n t c o m p le x t y p e t h a t a ls o h a s n o t b e e n s o lv e d b y\no t h e r r e c u r r e n t n e t a lg o r it h m s . A g a in , r e le v a n t in p u t s ig n a ls c a n o c c u r a t q u it e d i(cid:11) e r e n t\np o s it io n s in in p u t s e q u e n c e s . T h e e x p e r im e n t s h o w s t h a t L S T M c a n e x t r a c t in fo r m a t io n\nc o n v e y e d b y t h e t e m p o r a l o r d e r o f w id e ly s e p a r a t e d in p u t s ."
  },
  {
    "chunk_id": "doc_4_p11_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "S u b s e c t io n 5 .7 w ill p r o v id e a d e t a ile d s u m m a r y o f e x p e r im e n t a l c o n d it io n s in t w o t a b le s fo r\nr e fe r e n c e . 5 . 1 E X P E R I M E N T 1 : E M B E D D E D R E B E R G R A M M A R\nT a s k . O u r (cid:12) r s t t a s k is t o le a r n t h e \\ e m b e d d e d R e b e r g r a m m a r \" , e .g . S m it h a n d Z ip s e r ( 1 9 8 9 ) ,\nC le e r e m a n s e t a l. ( 1 9 8 9 ) , a n d F a h lm a n ( 1 9 9 1 ) . S in c e it a llo w s fo r t r a in in g s e q u e n c e s w it h s h o r t\nt im e la g s ( o f a s fe w a s 9 s t e p s ) , it is n o t a lo n g t im e la g p r o b le m ."
  },
  {
    "chunk_id": "doc_4_p11_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "W e in c lu d e it fo r t w o r e a s o n s : ( 1 )\nit is a p o p u la r r e c u r r e n t n e t b e n c h m a r k u s e d b y m a n y a u t h o r s | w e w a n t e d t o h a v e a t le a s t o n e\ne x p e r im e n t w h e r e R T R L a n d B P T T d o n o t fa il c o m p le t e ly , a n d ( 2 ) it s h o w s n ic e ly h o w o u t p u t\ng a t e s c a n b e b e n e (cid:12) c ia l.\nS\nX\nT S\nB E\nX P\nP V\nV\nT\nF\ng\nig\nr a\nu\nm\nr e\nm a\n3\nr\n:\n. T r a n s it io n d ia g r a m fo r t h e R e b e r\nREBER\nGRAMMAR\nT T\nB E\nP P\nREBER\nGRAMMAR\ns e q\nS\nu\nt\ne\na\nn\nr\nt\nt in g\nia lly\na\n(\nt\nb\nt h e\ne g in\nle ft m\nn in g w\no s t\nit h\nn\nt\no\nh\nd\ne\ne\ne\no\nm\nf\np\nt h\nt y\ne\ns\nd\nt\nir e\nr in\nc\ng\nt\n)\ne d\nb y\n1\nF ig u r\nR e b e r\nt h e R\ng r a p h\nfo llo w\n1\ne\ne\n4\ng r\nb e\nin\nin\n:\na\nr\ng\nT r a\nm m\ng r a\nF ig u\ne d g\nn s it io n\na r ."
  },
  {
    "chunk_id": "doc_4_p11_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "E a\nm m a r\nr e 4 , s\ne s | a\nc\n(\ny\nn\nd ia\nh b\ns e e\nm b\nd a\ng r a m\no x r e\nF ig u\no l s t\np p e n\nfo r t h e\np r e s e n t s\nr e 3 ) . r in g s a r e\nd in g t h e\ne m b e d d\na c o p y\ng e n e r a\na s s o c ia\ne d\no f\nt e\nt e\nd\nd"
  },
  {
    "chunk_id": "doc_4_p12_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "s y m b o ls t o t h e c u r r e n t s t r in g | u n t il t h e r ig h t m o s t n o d e is r e a c h e d . E d g e s a r e c h o s e n r a n d o m ly\nif t h e r e is a c h o ic e ( p r o b a b ilit y : 0 .5 ) . T h e n e t 's t a s k is t o r e a d s t r in g s , o n e s y m b o l a t a t im e ,\na n d t o p e r m a n e n t ly p r e d ic t t h e n e x t s y m b o l ( e r r o r s ig n a ls o c c u r a t e v e r y t im e s t e p ) . T o c o r r e c t ly\np r e d ic t t h e s y m b o l b e fo r e la s t , t h e n e t h a s t o r e m e m b e r t h e s e c o n d s y m b o l.\nC o m p a r i s o n ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "W e c o m p a r e L S T M t o \\ E lm a n n e t s t r a in e d b y E lm a n 's t r a in in g p r o c e d u r e \"\n( E L M ) ( r e s u lt s t a k e n fr o m C le e r e m a n s e t a l. 1 9 8 9 ) , F a h lm a n 's \\ R e c u r r e n t C a s c a d e - C o r r e la t io n \"\n( R C C ) ( r e s u lt s t a k e n fr o m F a h lm a n 1 9 9 1 ) , a n d R T R L ( r e s u lt s t a k e n fr o m S m it h a n d Z ip s e r ( 1 9 8 9 ) ,\nw h e r e o n ly t h e fe w s u c c e s s fu l t r ia ls a r e lis t e d ) . I t s h o u ld b e m e n t io n e d t h a t S m it h a n d Z ip s e r\na c t u a lly m a k e t h e t a s k e a s ie r b y in c r e a s in g t h e p r o b a b ilit y o f s h o r t t im e la g e x e m p la r s . W e d id n 't\nd o t h is fo r L S T M . T r a i n i n g / T e s t i n g ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "W e u s e a lo c a l in p u t / o u t p u t r e p r e s e n t a t io n ( 7 in p u t u n it s , 7 o u t p u t\nu n it s ) . F o llo w in g F a h lm a n , w e u s e 2 5 6 t r a in in g s t r in g s a n d 2 5 6 s e p a r a t e t e s t s t r in g s . T h e t r a in in g\ns e t is g e n e r a t e d r a n d o m ly ; t r a in in g e x e m p la r s a r e p ic k e d r a n d o m ly fr o m t h e t r a in in g s e t . T e s t\ns e q u e n c e s a r e g e n e r a t e d r a n d o m ly , t o o , b u t s e q u e n c e s a lr e a d y u s e d in t h e t r a in in g s e t a r e n o t\nu s e d fo r t e s t in g . A ft e r s t r in g p r e s e n t a t io n , a ll a c t iv a t io n s a r e r e in it ia liz e d w it h z e r o s ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "A t r ia l is\nc o n s id e r e d s u c c e s s fu l if a ll s t r in g s y m b o ls o f a ll s e q u e n c e s in b o t h t e s t s e t a n d t r a in in g s e t a r e\np r e d ic t e d c o r r e c t ly | t h a t is , if t h e o u t p u t u n it ( s ) c o r r e s p o n d in g t o t h e p o s s ib le n e x t s y m b o l( s )\nis ( a r e ) a lw a y s t h e m o s t a c t iv e o n e s . A r c h i t e c t u r e s . A r c h it e c t u r e s fo r R T R L , E L M , R C C a r e r e p o r t e d in t h e r e fe r e n c e s lis t e d\na b o v e . F o r L S T M , w e u s e 3 ( 4 ) m e m o r y c e ll b lo c k s . E a c h b lo c k h a s 2 ( 1 ) m e m o r y c e lls . T h e\no u t p u t la y e r 's o n ly in c o m in g c o n n e c t io n s o r ig in a t e a t m e m o r y c e lls ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "E a c h m e m o r y c e ll a n d e a c h\ng a t e u n it r e c e iv e s in c o m in g c o n n e c t io n s fr o m a ll m e m o r y c e lls a n d g a t e u n it s ( t h e h id d e n la y e r is\nfu lly c o n n e c t e d | le s s c o n n e c t iv it y m a y w o r k a s w e ll) . T h e in p u t la y e r h a s fo r w a r d c o n n e c t io n s\nt o a ll u n it s in t h e h id d e n la y e r . T h e g a t e u n it s a r e b ia s e d . T h e s e a r c h it e c t u r e p a r a m e t e r s m a k e it\ne a s y t o s t o r e a t le a s t 3 in p u t s ig n a ls ( a r c h it e c t u r e s 3 - 2 a n d 4 - 1 a r e e m p lo y e d t o o b t a in c o m p a r a b le\nn u m b e r s o f w e ig h t s fo r b o t h a r c h it e c t u r e s : 2 6 4 fo r 4 - 1 a n d 2 7 6 fo r 3 - 2 ) ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "O t h e r p a r a m e t e r s m a y b e\na p p r o p r ia t e a s w e ll, h o w e v e r . A ll s ig m o id fu n c t io n s a r e lo g is t ic w it h o u t p u t r a n g e [0 ; 1 ], e x c e p t fo r\nh , w h o s e r a n g e is [(cid:0) 1 ; 1 ], a n d g , w h o s e r a n g e is [(cid:0) 2 ; 2 ]. A ll w e ig h t s a r e in it ia liz e d in [(cid:0) 0 :2 ; 0 :2 ],\ne x c e p t fo r t h e o u t p u t g a t e b ia s e s , w h ic h a r e in it ia liz e d t o - 1 , - 2 , a n d - 3 , r e s p e c t iv e ly ( s e e a b u s e\np r o b le m , s o lu t io n ( 2 ) o f S e c t io n 4 ) . W e t r ie d le a r n in g r a t e s o f 0 .1 , 0 .2 a n d 0 .5 . R e s u l t s . W e u s e 3 d i(cid:11) e r e n t , r a n d o m ly g e n e r a t e d p a ir s o f t r a in in g a n d t e s t s e t s ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "W it h e a c h\ns u c h p a ir w e r u n 1 0 t r ia ls w it h d i(cid:11) e r e n t in it ia l w e ig h t s . S e e T a b le 1 fo r r e s u lt s ( m e a n o f 3 0\nt r ia ls ) . U n lik e t h e o t h e r m e t h o d s , L S T M a lw a y s le a r n s t o s o lv e t h e t a s k . E v e n w h e n w e ig n o r e\nt h e u n s u c c e s s fu l t r ia ls o f t h e o t h e r a p p r o a c h e s , L S T M le a r n s m u c h fa s t e r . I m p o r t a n c e o f o u t p u t g a t e s ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "T h e e x p e r im e n t p r o v id e s a n ic e e x a m p le w h e r e t h e o u t p u t g a t e\nis t r u ly b e n e (cid:12) c ia l. L e a r n in g t o s t o r e t h e (cid:12) r s t T o r P s h o u ld n o t p e r t u r b a c t iv a t io n s r e p r e s e n t in g\nt h e m o r e e a s ily le a r n a b le t r a n s it io n s o f t h e o r ig in a l R e b e r g r a m m a r . T h is is t h e jo b o f t h e o u t p u t\ng a t e s . W it h o u t o u t p u t g a t e s , w e d id n o t a c h ie v e fa s t le a r n in g . 5 . 2 E X P E R I M E N T 2 : N O I S E - F R E E A N D N O I S Y S E Q U E N C E S\nT a s k 2 a : n o i s e - f r e e s e q u e n c e s w i t h l o n g t i m e l a g s . T h e r e a r e p + 1 p o s s ib le in p u t s y m b o ls\nd e n o t e d a ; :::; a ; a = x ; a = y ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "a is \\ lo c a lly \" r e p r e s e n t e d b y t h e p + 1 - d im e n s io n a l v e c t o r 1 p (cid:0) 1 p p + 1 i\nw h o s e i - t h c o m p o n e n t is 1 ( a ll o t h e r c o m p o n e n t s a r e 0 ) . A n e t w it h p + 1 in p u t u n it s a n d p + 1\no u t p u t u n it s s e q u e n t ia lly o b s e r v e s in p u t s y m b o l s e q u e n c e s , o n e a t a t im e , p e r m a n e n t ly t r y in g\nt o p r e d ic t t h e n e x t s y m b o l | e r r o r s ig n a ls o c c u r a t e v e r y s in g le t im e s t e p . T o e m p h a s iz e t h e\n\\ lo n g t im e la g p r o b le m \" , w e u s e a t r a in in g s e t c o n s is t in g o f o n ly t w o v e r y s im ila r s e q u e n c e s :\n( y ; a ; a ; : : : ; a ; y ) a n d ( x ; a ; a ; : : : ; a ; x ) ."
  },
  {
    "chunk_id": "doc_4_p12_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "E a c h is s e le c t e d w it h p r o b a b ilit y 0 .5 . T o p r e d ic t 1 2 p (cid:0) 1 1 2 p (cid:0) 1\nt h e (cid:12) n a l e le m e n t , t h e n e t h a s t o le a r n t o s t o r e a r e p r e s e n t a t io n o f t h e (cid:12) r s t e le m e n t fo r p t im e\ns t e p s . W e c o m p a r e \\ R e a l- T im e R e c u r r e n t L e a r n in g \" fo r fu lly r e c u r r e n t n e t s ( R T R L ) , \\ B a c k - P r o p a -\ng a t io n T h r o u g h T im e \" ( B P T T ) , t h e s o m e t im e s v e r y s u c c e s s fu l 2 - n e t \\ N e u r a l S e q u e n c e C h u n k e r \"\n( C H , S c h m id h u b e r 1 9 9 2 b ) , a n d o u r n e w m e t h o d ( L S T M ) . I n a ll c a s e s , w e ig h t s a r e in it ia liz e d in\n[- 0 .2 ,0 .2 ]."
  },
  {
    "chunk_id": "doc_4_p12_fixed_10",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "D u e t o lim it e d c o m p u t a t io n t im e , t r a in in g is s t o p p e d a ft e r 5 m illio n s e q u e n c e p r e s e n -\n1 2"
  },
  {
    "chunk_id": "doc_4_p13_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_4_p13_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "m e t h o d h id d e n u n it s # w e ig h t s le a r n in g r a t e % o f s u c c e s s s u c c e s s a ft e r\nR T R L 3 (cid:25) 1 7 0 0 .0 5 \\ s o m e fr a c t io n \" 1 7 3 ,0 0 0\nR T R L 1 2 (cid:25) 4 9 4 0 .1 \\ s o m e fr a c t io n \" 2 5 ,0 0 0\nE L M 1 5 (cid:25) 4 3 5 0 > 2 0 0 ,0 0 0\nR C C 7 - 9 (cid:25) 1 1 9 - 1 9 8 5 0 1 8 2 ,0 0 0\nL S T M 4 b lo c k s , s iz e 1 2 6 4 0 .1 1 0 0 3 9 ,7 4 0\nL S T M 3 b lo c k s , s iz e 2 2 7 6 0 .1 1 0 0 2 1 ,7 3 0\nL S T M 3 b lo c k s , s iz e 2 2 7 6 0 .2 9 7 1 4 ,0 6 0\nL S T M 4 b lo c k s , s iz e 1 2 6 4 0 .5 9 7 9 ,5 0 0\nL S T M 3 b lo c k s , s iz e 2 2 7 6 0 .5 1 0 0 8 ,4 4 0\nT a b le 1 : E X P E R I M E N T 1 : E m b e d d e d R e b e r g r a m m a r : p e r c e n t a g e o f s u c c e s s fu l t r ia ls a n d n u m b e r\no f s e q u e n c e p r e s e n t a t io n s u n t il s u c c e s s fo r R T R L ( r e s u lt s t a k e n fr o m S m it h a n d Z ip s e r 1 9 8 9 ) ,\n\\ E lm a n n e t t r a in e d b y E lm a n 's p r o c e d u r e \" ( r e s u lt s t a k e n fr o m C le e r e m a n s e t a l. 1 9 8 9 ) , \\ R e c u r r e n t\nC a s c a d e - C o r r e la t io n \" ( r e s u lt s t a k e n fr o m F a h lm a n 1 9 9 1 ) a n d o u r n e w a p p r o a c h ( L S T M ) ."
  },
  {
    "chunk_id": "doc_4_p13_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "W e ig h t\nn u m b e r s in t h e (cid:12) r s t 4 r o w s a r e e s t im a t e s | t h e c o r r e s p o n d in g p a p e r s d o n o t p r o v id e a ll t h e t e c h n ic a l\nd e t a ils . O n ly L S T M a lm o s t a lw a y s le a r n s t o s o lv e t h e t a s k ( o n ly t w o fa ilu r e s o u t o f 1 5 0 t r ia ls ) . E v e n w h e n w e ig n o r e t h e u n s u c c e s s fu l t r ia ls o f t h e o t h e r a p p r o a c h e s , L S T M le a r n s m u c h fa s t e r\n( t h e n u m b e r o f r e q u ir e d t r a in in g e x a m p le s in t h e b o t t o m r o w v a r ie s b e t w e e n 3 ,8 0 0 a n d 2 4 ,1 0 0 ) . t a t io n s ."
  },
  {
    "chunk_id": "doc_4_p13_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "A s u c c e s s fu l r u n is o n e t h a t fu l(cid:12) lls t h e fo llo w in g c r it e r io n : a ft e r t r a in in g , d u r in g 1 0 ,0 0 0\ns u c c e s s iv e , r a n d o m ly c h o s e n in p u t s e q u e n c e s , t h e m a x im a l a b s o lu t e e r r o r o f a ll o u t p u t u n it s is\na lw a y s b e lo w 0 :2 5 . A r c h i t e c t u r e s . R T R L : o n e s e lf- r e c u r r e n t h id d e n u n it , p + 1 n o n - r e c u r r e n t o u t p u t u n it s . E a c h\nla y e r h a s c o n n e c t io n s fr o m a ll la y e r s b e lo w . A ll u n it s u s e t h e lo g is t ic a c t iv a t io n fu n c t io n s ig m o id\nin [0 ,1 ]. B P T T : s a m e a r c h it e c t u r e a s t h e o n e t r a in e d b y R T R L ."
  },
  {
    "chunk_id": "doc_4_p13_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "C H : b o t h n e t a r c h it e c t u r e s lik e R T R L 's , b u t o n e h a s a n a d d it io n a l o u t p u t fo r p r e d ic t in g t h e\nh id d e n u n it o f t h e o t h e r o n e ( s e e S c h m id h u b e r 1 9 9 2 b fo r d e t a ils ) . L S T M : lik e w it h R T R L , b u t t h e h id d e n u n it is r e p la c e d b y a m e m o r y c e ll a n d a n in p u t g a t e\n( n o o u t p u t g a t e r e q u ir e d ) . g is t h e lo g is t ic s ig m o id , a n d h is t h e id e n t it y fu n c t io n h : h ( x ) = x ; 8 x . M e m o r y c e ll a n d in p u t g a t e a r e a d d e d o n c e t h e e r r o r h a s s t o p p e d d e c r e a s in g ( s e e a b u s e p r o b le m :\ns o lu t io n ( 1 ) in S e c t io n 4 ) . 7\nR e s u l t s ."
  },
  {
    "chunk_id": "doc_4_p13_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "U s in g R T R L a n d a s h o r t 4 t im e s t e p d e la y ( p = 4 ) , o f a ll t r ia ls w e r e s u c c e s s fu l.\n9\nN o t r ia l w a s s u c c e s s fu l w it h p = 1 0 . W it h lo n g t im e la g s , o n ly t h e n e u r a l s e q u e n c e c h u n k e r\na n d L S T M a c h ie v e d s u c c e s s fu l t r ia ls , w h ile B P T T a n d R T R L fa ile d . W it h p = 1 0 0 , t h e 2 - n e t\n1\ns e q u e n c e c h u n k e r s o lv e d t h e t a s k in o n ly o f a ll t r ia ls . L S T M , h o w e v e r , a lw a y s le a r n e d t o s o lv e\n3\nt h e t a s k . C o m p a r in g s u c c e s s fu l t r ia ls o n ly , L S T M le a r n e d m u c h fa s t e r . S e e T a b le 2 fo r d e t a ils ."
  },
  {
    "chunk_id": "doc_4_p13_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "I t\ns h o u ld b e m e n t io n e d , h o w e v e r , t h a t a h ie r a r c h ic a l c h u n k e r c a n a ls o a lw a y s q u ic k ly s o lv e t h is t a s k\n( S c h m id h u b e r 1 9 9 2 c , 1 9 9 3 ) . T a s k 2 b : n o l o c a l r e g u l a r i t i e s . W it h t h e t a s k a b o v e , t h e c h u n k e r s o m e t im e s le a r n s t o\nc o r r e c t ly p r e d ic t t h e (cid:12) n a l e le m e n t , b u t o n ly b e c a u s e o f p r e d ic t a b le lo c a l r e g u la r it ie s in t h e in p u t\ns t r e a m t h a t a llo w fo r c o m p r e s s in g t h e s e q u e n c e ."
  },
  {
    "chunk_id": "doc_4_p13_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "I n a n a d d it io n a l, m o r e d i(cid:14) c u lt t a s k ( in v o lv in g\nm a n y m o r e d i(cid:11) e r e n t p o s s ib le s e q u e n c e s ) , w e r e m o v e c o m p r e s s ib ilit y b y r e p la c in g t h e d e t e r m in -\nis t ic s u b s e q u e n c e ( a ; a ; : : : ; a ) b y a r a n d o m s u b s e q u e n c e ( o f le n g t h p (cid:0) 1 ) o v e r t h e a lp h a - 1 2 p (cid:0) 1\nb e t a ; a ; : : : ; a . W e o b t a in 2 c la s s e s ( t w o s e t s o f s e q u e n c e s ) f ( y ; a ; a ; : : : ; a ; y ) j 1 (cid:20) 1 2 p (cid:0) 1 i i i 1 2 p (cid:0) 1\ni ; i ; : : : ; i (cid:20) p (cid:0) 1 g a n d f ( x ; a ; a ; : : : ; a ; x ) j 1 (cid:20) i ; i ; : : : ; i (cid:20) p (cid:0) 1 g ."
  },
  {
    "chunk_id": "doc_4_p13_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "A g a in , e v e r y 1 2 p (cid:0) 1 i i i 1 2 p (cid:0) 1 1 2 p (cid:0) 1\nn e x t s e q u e n c e e le m e n t h a s t o b e p r e d ic t e d . T h e o n ly t o t a lly p r e d ic t a b le t a r g e t s , h o w e v e r , a r e x\na n d y , w h ic h o c c u r a t s e q u e n c e e n d s . T r a in in g e x e m p la r s a r e c h o s e n r a n d o m ly fr o m t h e 2 c la s s e s . A r c h it e c t u r e s a n d p a r a m e t e r s a r e t h e s a m e a s in E x p e r im e n t 2 a . A s u c c e s s fu l r u n is o n e t h a t\nfu l(cid:12) lls t h e fo llo w in g c r it e r io n : a ft e r t r a in in g , d u r in g 1 0 ,0 0 0 s u c c e s s iv e , r a n d o m ly c h o s e n in p u t\n1 3"
  },
  {
    "chunk_id": "doc_4_p14_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_4_p14_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "M e t h o d D e la y p L e a r n in g r a t e # w e ig h t s % S u c c e s s fu l t r ia ls S u c c e s s a ft e r\nR T R L 4 1 .0 3 6 7 8 1 ,0 4 3 ,0 0 0\nR T R L 4 4 .0 3 6 5 6 8 9 2 ,0 0 0\nR T R L 4 1 0 .0 3 6 2 2 2 5 4 ,0 0 0\nR T R L 1 0 1 .0 - 1 0 .0 1 4 4 0 > 5 ,0 0 0 ,0 0 0\nR T R L 1 0 0 1 .0 - 1 0 .0 1 0 4 0 4 0 > 5 ,0 0 0 ,0 0 0\nB P T T 1 0 0 1 .0 - 1 0 .0 1 0 4 0 4 0 > 5 ,0 0 0 ,0 0 0\nC H 1 0 0 1 .0 1 0 5 0 6 3 3 3 2 ,4 0 0\nL S T M 1 0 0 1 .0 1 0 5 0 4 1 0 0 5 ,0 4 0\nT a b le 2 : T a s k 2 a : P e r c e n t a g e o f s u c c e s s fu l t r ia ls a n d n u m b e r o f t r a in in g s e q u e n c e s u n t il s u c c e s s ,\nfo r \\ R e a l- T im e R e c u r r e n t L e a r n in g \" ( R T R L ) , \\ B a c k - P r o p a g a t io n T h r o u g h T im e \" ( B P T T ) , n e u r a l\ns e q u e n c e c h u n k in g ( C H ) , a n d t h e n e w m e t h o d ( L S T M ) ."
  },
  {
    "chunk_id": "doc_4_p14_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "T a b le e n t r ie s r e fe r t o m e a n s o f 1 8 t r ia ls . W it h 1 0 0 t im e s t e p d e la y s , o n ly C H a n d L S T M a c h ie v e s u c c e s s fu l t r ia ls . E v e n w h e n w e ig n o r e t h e\nu n s u c c e s s fu l t r ia ls o f t h e o t h e r a p p r o a c h e s , L S T M le a r n s m u c h fa s t e r . s e q u e n c e s , t h e m a x im a l a b s o lu t e e r r o r o f a ll o u t p u t u n it s is b e lo w 0 :2 5 a t s e q u e n c e e n d . R e s u l t s . A s e x p e c t e d , t h e c h u n k e r fa ile d t o s o lv e t h is t a s k ( s o d id B P T T a n d R T R L , o f\nc o u r s e ) ."
  },
  {
    "chunk_id": "doc_4_p14_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "L S T M , h o w e v e r , w a s a lw a y s s u c c e s s fu l. O n a v e r a g e ( m e a n o f 1 8 t r ia ls ) , s u c c e s s fo r\np = 1 0 0 w a s a c h ie v e d a ft e r 5 ,6 8 0 s e q u e n c e p r e s e n t a t io n s . T h is d e m o n s t r a t e s t h a t L S T M d o e s n o t\nr e q u ir e s e q u e n c e r e g u la r it ie s t o w o r k w e ll. T a s k 2 c : v e r y l o n g t i m e l a g s | n o l o c a l r e g u l a r i t i e s . T h is is t h e m o s t d i(cid:14) c u lt t a s k in\nt h is s u b s e c t io n . T o o u r k n o w le d g e n o o t h e r r e c u r r e n t n e t a lg o r it h m c a n s o lv e it . N o w t h e r e a r e p + 4\np o s s ib le in p u t s y m b o ls d e n o t e d a ; :::; a ; a ; a = e ; a = b ; a = x ; a = y ."
  },
  {
    "chunk_id": "doc_4_p14_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "a ; :::; a 1 p (cid:0) 1 p p + 1 p + 2 p + 3 p + 4 1 p\na r e a ls o c a lle d \\ d is t r a c t o r s y m b o ls \" . A g a in , a is lo c a lly r e p r e s e n t e d b y t h e p + 4 - d im e n s io n a l v e c t o r i\nw h o s e i t h c o m p o n e n t is 1 ( a ll o t h e r c o m p o n e n t s a r e 0 ) . A n e t w it h p + 4 in p u t u n it s a n d 2 o u t p u t\nu n it s s e q u e n t ia lly o b s e r v e s in p u t s y m b o l s e q u e n c e s , o n e a t a t im e ."
  },
  {
    "chunk_id": "doc_4_p14_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "T r a in in g s e q u e n c e s a r e r a n d o m ly\nc h o s e n fr o m t h e u n io n o f t w o v e r y s im ila r s u b s e t s o f s e q u e n c e s : f ( b ; y ; a ; a ; : : : ; a ; e ; y ) j 1 (cid:20) i i i 1 2 q + k\ni ; i ; : : : ; i (cid:20) q g a n d f ( b ; x ; a ; a ; : : : ; a ; e ; x ) j 1 (cid:20) i ; i ; : : : ; i (cid:20) q g . T o p r o d u c e a 1 2 q + k i i i 1 2 q + k 1 2 q + k\nt r a in in g s e q u e n c e , w e ( 1 ) r a n d o m ly g e n e r a t e a s e q u e n c e p r e (cid:12) x o f le n g t h q + 2 , ( 2 ) r a n d o m ly\n9\ng e n e r a t e a s e q u e n c e s u (cid:14) x o f a d d it io n a l e le m e n t s ( 6= b ; e ; x ; y ) w it h p r o b a b ilit y o r , a lt e r n a t iv e ly ,\n1 0\n1\na n e w it h p r o b a b ilit y ."
  },
  {
    "chunk_id": "doc_4_p14_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "I n t h e la t t e r c a s e , w e ( 3 ) c o n c lu d e t h e s e q u e n c e w it h x o r y , d e p e n d in g\n1 0\no n t h e s e c o n d e le m e n t . F o r a g iv e n k , t h is le a d s t o a u n ifo r m d is t r ib u t io n o n t h e p o s s ib le s e q u e n c e s\nw it h le n g t h q + k + 4 . T h e m in im a l s e q u e n c e le n g t h is q + 4 ; t h e e x p e c t e d le n g t h is\n1 X 1 9 k\n4 + ( ) ( q + k ) = q + 1 4 :\n1 0 1 0\nk = 0\nq + 1 0 q\nT h e e x p e c t e d n u m b e r o f o c c u r r e n c e s o f e le m e n t a ; 1 (cid:20) i (cid:20) p , in a s e q u e n c e is (cid:25) . T h e i p p\ng o a l is t o p r e d ic t t h e la s t s y m b o l, w h ic h a lw a y s o c c u r s a ft e r t h e \\ t r ig g e r s y m b o l\" e ."
  },
  {
    "chunk_id": "doc_4_p14_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "E r r o r s ig n a ls\na r e g e n e r a t e d o n ly a t s e q u e n c e e n d s . T o p r e d ic t t h e (cid:12) n a l e le m e n t , t h e n e t h a s t o le a r n t o s t o r e a\nr e p r e s e n t a t io n o f t h e s e c o n d e le m e n t fo r a t le a s t q + 1 t im e s t e p s ( u n t il it s e e s t h e t r ig g e r s y m b o l\ne ) . S u c c e s s is d e (cid:12) n e d a s \\ p r e d ic t io n e r r o r ( fo r (cid:12) n a l s e q u e n c e e le m e n t ) o f b o t h o u t p u t u n it s a lw a y s\nb e lo w 0 :2 , fo r 1 0 ,0 0 0 s u c c e s s iv e , r a n d o m ly c h o s e n in p u t s e q u e n c e s \" . A r c h i t e c t u r e / L e a r n i n g . T h e n e t h a s p + 4 in p u t u n it s a n d 2 o u t p u t u n it s . W e ig h t s a r e\nin it ia liz e d in [- 0 .2 ,0 .2 ]."
  },
  {
    "chunk_id": "doc_4_p14_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "T o a v o id t o o m u c h le a r n in g t im e v a r ia n c e d u e t o d i(cid:11) e r e n t w e ig h t in it ia l-\niz a t io n s , t h e h id d e n la y e r g e t s t w o m e m o r y c e lls ( t w o c e ll b lo c k s o f s iz e 1 | a lt h o u g h o n e w o u ld\nb e s u (cid:14) c ie n t ) . T h e r e a r e n o o t h e r h id d e n u n it s . T h e o u t p u t la y e r r e c e iv e s c o n n e c t io n s o n ly fr o m\nm e m o r y c e lls . M e m o r y c e lls a n d g a t e u n it s r e c e iv e c o n n e c t io n s fr o m in p u t u n it s , m e m o r y c e lls\na n d g a t e u n it s ( i.e ., t h e h id d e n la y e r is fu lly c o n n e c t e d ) . N o b ia s w e ig h t s a r e u s e d ."
  },
  {
    "chunk_id": "doc_4_p14_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "h a n d g a r e\nlo g is t ic s ig m o id s w it h o u t p u t r a n g e s [(cid:0) 1 ; 1 ] a n d [(cid:0) 2 ; 2 ], r e s p e c t iv e ly . T h e le a r n in g r a t e is 0 .0 1 . 1 4"
  },
  {
    "chunk_id": "doc_4_p15_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "q\nq ( t im e la g (cid:0) 1 ) p ( # r a n d o m in p u t s ) # w e ig h t s S u c c e s s a ft e r\np\n5 0 5 0 1 3 6 4 3 0 ,0 0 0\n1 0 0 1 0 0 1 6 6 4 3 1 ,0 0 0\n2 0 0 2 0 0 1 1 2 6 4 3 3 ,0 0 0\n5 0 0 5 0 0 1 3 0 6 4 3 8 ,0 0 0\n1 ,0 0 0 1 ,0 0 0 1 6 0 6 4 4 9 ,0 0 0\n1 ,0 0 0 5 0 0 2 3 0 6 4 4 9 ,0 0 0\n1 ,0 0 0 2 0 0 5 1 2 6 4 7 5 ,0 0 0\n1 ,0 0 0 1 0 0 1 0 6 6 4 1 3 5 ,0 0 0\n1 ,0 0 0 5 0 2 0 3 6 4 2 0 3 ,0 0 0\nT a b le 3 : T a s k 2 c : L S T M w it h v e r y lo n g m in im a l t im e la g s q + 1 a n d a lo t o f n o is e . p is t h e\nq\nn u m b e r o f a v a ila b le d is t r a c t o r s y m b o ls ( p + 4 is t h e n u m b e r o f in p u t u n it s ) . is t h e e x p e c t e d\np\nn u m b e r o f o c c u r r e n c e s o f a g iv e n d is t r a c t o r s y m b o l in a s e q u e n c e ."
  },
  {
    "chunk_id": "doc_4_p15_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "T h e r ig h t m o s t c o lu m n lis t s t h e\nn u m b e r o f t r a in in g s e q u e n c e s r e q u ir e d b y L S T M ( B P T T , R T R L a n d t h e o t h e r c o m p e t it o r s h a v e\nn o c h a n c e o f s o lv in g t h is t a s k ) . I f w e le t t h e n u m b e r o f d is t r a c t o r s y m b o ls ( a n d w e ig h t s ) in c r e a s e\nin p r o p o r t io n t o t h e t im e la g , le a r n in g t im e in c r e a s e s v e r y s lo w ly . T h e lo w e r b lo c k illu s t r a t e s t h e\ne x p e c t e d s lo w - d o w n d u e t o in c r e a s e d fr e q u e n c y o f d is t r a c t o r s y m b o ls ."
  },
  {
    "chunk_id": "doc_4_p15_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "N o t e t h a t t h e m in im a l t im e la g is q + 1 | t h e n e t n e v e r s e e s s h o r t t r a in in g s e q u e n c e s fa c ilit a t in g\nt h e c la s s i(cid:12) c a t io n o f lo n g t e s t s e q u e n c e s . R e s u l t s . 2 0 t r ia ls w e r e m a d e fo r a ll t e s t e d p a ir s ( p ; q ) . T a b le 3 lis t s t h e m e a n o f t h e n u m b e r\no f t r a in in g s e q u e n c e s r e q u ir e d b y L S T M t o a c h ie v e s u c c e s s ( B P T T a n d R T R L h a v e n o c h a n c e o f\ns o lv in g n o n - t r iv ia l t a s k s w it h m in im a l t im e la g s o f 1 0 0 0 s t e p s ) . S c a l i n g ."
  },
  {
    "chunk_id": "doc_4_p15_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "T a b le 3 s h o w s t h a t if w e le t t h e n u m b e r o f in p u t s y m b o ls ( a n d w e ig h t s ) in c r e a s e\nin p r o p o r t io n t o t h e t im e la g , le a r n in g t im e in c r e a s e s v e r y s lo w ly . T h is is a a n o t h e r r e m a r k a b le\np r o p e r t y o f L S T M n o t s h a r e d b y a n y o t h e r m e t h o d w e a r e a w a r e o f. I n d e e d , R T R L a n d B P T T\na r e fa r fr o m s c a lin g r e a s o n a b ly | in s t e a d , t h e y a p p e a r t o s c a le e x p o n e n t ia lly , a n d a p p e a r q u it e\nu s e le s s w h e n t h e t im e la g s e x c e e d a s fe w a s 1 0 s t e p s . q\nD i s t r a c t o r i n (cid:13) u e n c e ."
  },
  {
    "chunk_id": "doc_4_p15_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "I n T a b le 3 , t h e c o lu m n h e a d e d b y g iv e s t h e e x p e c t e d fr e q u e n c y o f\np\nd is t r a c t o r s y m b o ls . I n c r e a s in g t h is fr e q u e n c y d e c r e a s e s le a r n in g s p e e d , a n e (cid:11) e c t d u e t o w e ig h t\no s c illa t io n s c a u s e d b y fr e q u e n t ly o b s e r v e d in p u t s y m b o ls . 5 . 3 E X P E R I M E N T 3 : N O I S E A N D S I G N A L O N S A M E C H A N N E L\nT h is e x p e r im e n t s e r v e s t o illu s t r a t e t h a t L S T M d o e s n o t e n c o u n t e r fu n d a m e n t a l p r o b le m s if n o is e\na n d s ig n a l a r e m ix e d o n t h e s a m e in p u t lin e ."
  },
  {
    "chunk_id": "doc_4_p15_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "W e in it ia lly fo c u s o n B e n g io e t a l.'s s im p le 1 9 9 4\n\\ 2 - s e q u e n c e p r o b le m \" ; in E x p e r im e n t 3 c w e w ill t h e n p o s e a m o r e c h a lle n g in g 2 - s e q u e n c e p r o b le m . T a s k 3 a ( \\ 2 - s e q u e n c e p r o b le m \" ) . T h e t a s k is t o o b s e r v e a n d t h e n c la s s ify in p u t s e q u e n c e s . T h e r e a r e t w o c la s s e s , e a c h o c c u r r in g w it h p r o b a b ilit y 0 .5 . T h e r e is o n ly o n e in p u t lin e . O n ly\nt h e (cid:12) r s t N r e a l- v a lu e d s e q u e n c e e le m e n t s c o n v e y r e le v a n t in fo r m a t io n a b o u t t h e c la s s . S e q u e n c e\ne le m e n t s a t p o s it io n s t > N a r e g e n e r a t e d b y a G a u s s ia n w it h m e a n z e r o a n d v a r ia n c e 0 .2 ."
  },
  {
    "chunk_id": "doc_4_p15_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "C a s e\nN = 1 : t h e (cid:12) r s t s e q u e n c e e le m e n t is 1 .0 fo r c la s s 1 , a n d - 1 .0 fo r c la s s 2 . C a s e N = 3 : t h e (cid:12) r s t\nt h r e e e le m e n t s a r e 1 .0 fo r c la s s 1 a n d - 1 .0 fo r c la s s 2 . T h e t a r g e t a t t h e s e q u e n c e e n d is 1 .0 fo r\nc la s s 1 a n d 0 .0 fo r c la s s 2 . C o r r e c t c la s s i(cid:12) c a t io n is d e (cid:12) n e d a s \\ a b s o lu t e o u t p u t e r r o r a t s e q u e n c e\ne n d b e lo w 0 .2 \" . G iv e n a c o n s t a n t T , t h e s e q u e n c e le n g t h is r a n d o m ly s e le c t e d b e t w e e n T a n d T +\nT / 1 0 ( a d i(cid:11) e r e n c e t o B e n g io e t a l.'s p r o b le m is t h a t t h e y a ls o p e r m it s h o r t e r s e q u e n c e s o f le n g t h\nT / 2 ) ."
  },
  {
    "chunk_id": "doc_4_p15_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "G u e s s i n g . B e n g io e t a l. ( 1 9 9 4 ) a n d B e n g io a n d F r a s c o n i ( 1 9 9 4 ) t e s t e d 7 d i(cid:11) e r e n t m e t h o d s\no n t h e 2 - s e q u e n c e p r o b le m . W e d is c o v e r e d , h o w e v e r , t h a t r a n d o m w e ig h t g u e s s in g e a s ily o u t p e r -\n1 5"
  },
  {
    "chunk_id": "doc_4_p16_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "T N s t o p : S T 1 s t o p : S T 2 # w e ig h t s S T 2 : fr a c t io n m is c la s s i(cid:12) e d\n1 0 0 3 2 7 ,3 8 0 3 9 ,8 5 0 1 0 2 0 .0 0 0 1 9 5\n1 0 0 1 5 8 ,3 7 0 6 4 ,3 3 0 1 0 2 0 .0 0 0 1 1 7\n1 0 0 0 3 4 4 6 ,8 5 0 4 5 2 ,4 6 0 1 0 2 0 .0 0 0 0 7 8\nT a b le 4 : T a s k 3 a : B e n g io e t a l.'s 2 - s e q u e n c e p r o b le m . T is m in im a l s e q u e n c e le n g t h . N is t h e\nn u m b e r o f in fo r m a t io n - c o n v e y in g e le m e n t s a t s e q u e n c e b e g in . T h e c o lu m n h e a d e d b y S T 1 ( S T 2 )\ng iv e s t h e n u m b e r o f s e q u e n c e p r e s e n t a t io n s r e q u ir e d t o a c h ie v e s t o p p in g c r it e r io n S T 1 ( S T 2 ) ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "T h e\nr ig h t m o s t c o lu m n lis t s t h e fr a c t io n o f m is c la s s i(cid:12) e d p o s t - t r a in in g s e q u e n c e s ( w it h a b s o lu t e e r r o r >\n0 .2 ) fr o m a t e s t s e t c o n s is t in g o f 2 5 6 0 s e q u e n c e s ( t e s t e d a ft e r S T 2 w a s a c h ie v e d ) . A ll v a lu e s a r e\nm e a n s o f 1 0 t r ia ls . W e d is c o v e r e d , h o w e v e r , t h a t t h is p r o b le m is s o s im p le t h a t r a n d o m w e ig h t\ng u e s s in g s o lv e s it fa s t e r t h a n L S T M a n d a n y o t h e r m e t h o d fo r w h ic h t h e r e a r e p u b lis h e d r e s u lt s . 5\nfo r m s t h e m a ll, b e c a u s e t h e p r o b le m is s o s im p le ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "S e e S c h m id h u b e r a n d H o c h r e it e r ( 1 9 9 6 ) a n d\nH o c h r e it e r a n d S c h m id h u b e r ( 1 9 9 6 , 1 9 9 7 ) fo r a d d it io n a l r e s u lt s in t h is v e in . L S T M a r c h i t e c t u r e . W e u s e a 3 - la y e r n e t w it h 1 in p u t u n it , 1 o u t p u t u n it , a n d 3 c e ll b lo c k s\no f s iz e 1 . T h e o u t p u t la y e r r e c e iv e s c o n n e c t io n s o n ly fr o m m e m o r y c e lls . M e m o r y c e lls a n d g a t e\nu n it s r e c e iv e in p u t s fr o m in p u t u n it s , m e m o r y c e lls a n d g a t e u n it s , a n d h a v e b ia s w e ig h t s . G a t e\nu n it s a n d o u t p u t u n it a r e lo g is t ic s ig m o id in [0 ; 1 ], h in [(cid:0) 1 ; 1 ], a n d g in [(cid:0) 2 ; 2 ]. T r a i n i n g / T e s t i n g ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "A ll w e ig h t s ( e x c e p t t h e b ia s w e ig h t s t o g a t e u n it s ) a r e r a n d o m ly in it ia liz e d\nin t h e r a n g e [(cid:0) 0 :1 ; 0 :1 ]. T h e (cid:12) r s t in p u t g a t e b ia s is in it ia liz e d w it h (cid:0) 1 :0 , t h e s e c o n d w it h (cid:0) 3 :0 ,\na n d t h e t h ir d w it h (cid:0) 5 :0 . T h e (cid:12) r s t o u t p u t g a t e b ia s is in it ia liz e d w it h (cid:0) 2 :0 , t h e s e c o n d w it h (cid:0) 4 :0\na n d t h e t h ir d w it h (cid:0) 6 :0 . T h e p r e c is e in it ia liz a t io n v a lu e s h a r d ly m a t t e r t h o u g h , a s c o n (cid:12) r m e d b y\na d d it io n a l e x p e r im e n t s . T h e le a r n in g r a t e is 1 .0 . A ll a c t iv a t io n s a r e r e s e t t o z e r o a t t h e b e g in n in g\no f a n e w s e q u e n c e ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "W e s t o p t r a in in g ( a n d ju d g e t h e t a s k a s b e in g s o lv e d ) a c c o r d in g t o t h e fo llo w in g c r it e r ia : S T 1 :\nn o n e o f 2 5 6 s e q u e n c e s fr o m a r a n d o m ly c h o s e n t e s t s e t is m is c la s s i(cid:12) e d . S T 2 : S T 1 is s a t is (cid:12) e d , a n d\nm e a n a b s o lu t e t e s t s e t e r r o r is b e lo w 0 .0 1 . I n c a s e o f S T 2 , a n a d d it io n a l t e s t s e t c o n s is t in g o f 2 5 6 0\nr a n d o m ly c h o s e n s e q u e n c e s is u s e d t o d e t e r m in e t h e fr a c t io n o f m is c la s s i(cid:12) e d s e q u e n c e s . R e s u l t s . S e e T a b le 4 . T h e r e s u lt s a r e m e a n s o f 1 0 t r ia ls w it h d i(cid:11) e r e n t w e ig h t in it ia liz a t io n s\nin t h e r a n g e [(cid:0) 0 :1 ; 0 :1 ]."
  },
  {
    "chunk_id": "doc_4_p16_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "L S T M is a b le t o s o lv e t h is p r o b le m , t h o u g h b y fa r n o t a s fa s t a s r a n d o m\nw e ig h t g u e s s in g ( s e e p a r a g r a p h \\ G u e s s in g \" a b o v e ) . C le a r ly , t h is t r iv ia l p r o b le m d o e s n o t p r o v id e a\nv e r y g o o d t e s t b e d t o c o m p a r e p e r fo r m a n c e o f v a r io u s n o n - t r iv ia l a lg o r it h m s . S t ill, it d e m o n s t r a t e s\nt h a t L S T M d o e s n o t e n c o u n t e r fu n d a m e n t a l p r o b le m s w h e n fa c e d w it h s ig n a l a n d n o is e o n t h e\ns a m e c h a n n e l.\nT a s k 3 b . A r c h it e c t u r e , p a r a m e t e r s , e t c ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "lik e in T a s k 3 a , b u t n o w w it h G a u s s ia n n o is e ( m e a n\n0 a n d v a r ia n c e 0 .2 ) a d d e d t o t h e in fo r m a t io n - c o n v e y in g e le m e n t s ( t < = N ) . W e s t o p t r a in in g\n( a n d ju d g e t h e t a s k a s b e in g s o lv e d ) a c c o r d in g t o t h e fo llo w in g , s lig h t ly r e d e (cid:12) n e d c r it e r ia : S T 1 :\nle s s t h a n 6 o u t o f 2 5 6 s e q u e n c e s fr o m a r a n d o m ly c h o s e n t e s t s e t a r e m is c la s s i(cid:12) e d . S T 2 : S T 1 is\ns a t is (cid:12) e d , a n d m e a n a b s o lu t e t e s t s e t e r r o r is b e lo w 0 .0 4 ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "I n c a s e o f S T 2 , a n a d d it io n a l t e s t s e t\nc o n s is t in g o f 2 5 6 0 r a n d o m ly c h o s e n s e q u e n c e s is u s e d t o d e t e r m in e t h e fr a c t io n o f m is c la s s i(cid:12) e d\ns e q u e n c e s . R e s u l t s . S e e T a b le 5 . T h e r e s u lt s r e p r e s e n t m e a n s o f 1 0 t r ia ls w it h d i(cid:11) e r e n t w e ig h t in it ia liz a -\nt io n s . L S T M e a s ily s o lv e s t h e p r o b le m . T a s k 3 c . A r c h it e c t u r e , p a r a m e t e r s , e t c ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "lik e in T a s k 3 a , b u t w it h a fe w e s s e n t ia l c h a n g e s t h a t\nm a k e t h e t a s k n o n - t r iv ia l: t h e t a r g e t s a r e 0 .2 a n d 0 .8 fo r c la s s 1 a n d c la s s 2 , r e s p e c t iv e ly , a n d\nt h e r e is G a u s s ia n n o is e o n t h e t a r g e t s ( m e a n 0 a n d v a r ia n c e 0 .1 ; s t .d e v . 0 .3 2 ) . T o m in im iz e m e a n\ns q u a r e d e r r o r , t h e s y s t e m h a s t o le a r n t h e c o n d it io n a l e x p e c t a t io n s o f t h e t a r g e t s g iv e n t h e in p u t s ."
  },
  {
    "chunk_id": "doc_4_p16_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "M is c la s s i(cid:12) c a t io n is d e (cid:12) n e d a s \\ a b s o lu t e d i(cid:11) e r e n c e b e t w e e n o u t p u t a n d n o is e - fr e e t a r g e t ( 0 .2 fo r\n5 It s h o u ld b e m e n t io n e d , h o w e v e r , t h a t d i(cid:11) e r e n t in p u t r e p r e s e n t a t io n s a n d d i(cid:11) e r e n t ty p e s o f n o is e m a y le a d t o\nw o r s e g u e s s in g p e r fo r m a n c e ( Y o s h u a B e n g io , p e r s o n a l c o m m u n ic a t io n , 1 9 9 6 ) . 1 6"
  },
  {
    "chunk_id": "doc_4_p17_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "T N s t o p : S T 1 s t o p : S T 2 # w e ig h t s S T 2 : fr a c t io n m is c la s s i(cid:12) e d\n1 0 0 3 4 1 ,7 4 0 4 3 ,2 5 0 1 0 2 0 .0 0 8 2 8\n1 0 0 1 7 4 ,9 5 0 7 8 ,4 3 0 1 0 2 0 .0 1 5 0 0\n1 0 0 0 1 4 8 1 ,0 6 0 4 8 5 ,0 8 0 1 0 2 0 .0 1 2 0 7\nT a b le 5 : T a s k 3 b : m o d i(cid:12) e d 2 - s e q u e n c e p r o b le m . S a m e a s in T a b le 4 , b u t n o w t h e in fo r m a t io n -\nc o n v e y in g e le m e n t s a r e a ls o p e r t u r b e d b y n o is e . T N s t o p # w e ig h t s fr a c t io n m is c la s s i(cid:12) e d a v ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "d i(cid:11) e r e n c e t o m e a n\n1 0 0 3 2 6 9 ,6 5 0 1 0 2 0 .0 0 5 5 8 0 .0 1 4\n1 0 0 1 5 6 5 ,6 4 0 1 0 2 0 .0 0 4 4 1 0 .0 1 2\nT a b le 6 : T a s k 3 c : m o d i(cid:12) e d , m o r e c h a lle n g in g 2 - s e q u e n c e p r o b le m . S a m e a s in T a b le 4 , b u t w it h\nn o is y r e a l- v a lu e d t a r g e t s . T h e s y s t e m h a s t o le a r n t h e c o n d it io n a l e x p e c t a t io n s o f t h e t a r g e t s g iv e n\nt h e in p u t s . T h e r ig h t m o s t c o lu m n p r o v id e s t h e a v e r a g e d i(cid:11) e r e n c e b e t w e e n n e t w o r k o u t p u t a n d\ne x p e c t e d t a r g e t . U n lik e 3 a a n d 3 b , t h is t a s k c a n n o t b e s o lv e d q u ic k ly b y r a n d o m w e ig h t g u e s s in g ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "c la s s 1 a n d 0 .8 fo r c la s s 2 ) > 0 .1 . \" T h e n e t w o r k o u t p u t is c o n s id e r e d a c c e p t a b le if t h e m e a n\na b s o lu t e d i(cid:11) e r e n c e b e t w e e n n o is e - fr e e t a r g e t a n d o u t p u t is b e lo w 0 .0 1 5 . S in c e t h is r e q u ir e s h ig h\nw e ig h t p r e c is io n , T a s k 3 c ( u n lik e 3 a a n d 3 b ) c a n n o t b e s o lv e d q u ic k ly b y r a n d o m g u e s s in g . T r a i n i n g / T e s t i n g . T h e le a r n in g r a t e is 0 :1 ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "W e s t o p t r a in in g a c c o r d in g t o t h e fo llo w in g\nc r it e r io n : n o n e o f 2 5 6 s e q u e n c e s fr o m a r a n d o m ly c h o s e n t e s t s e t is m is c la s s i(cid:12) e d , a n d m e a n\na b s o lu t e d i(cid:11) e r e n c e b e t w e e n n o is e fr e e t a r g e t a n d o u t p u t is b e lo w 0 .0 1 5 . A n a d d it io n a l t e s t s e t\nc o n s is t in g o f 2 5 6 0 r a n d o m ly c h o s e n s e q u e n c e s is u s e d t o d e t e r m in e t h e fr a c t io n o f m is c la s s i(cid:12) e d\ns e q u e n c e s . R e s u l t s . S e e T a b le 6 . T h e r e s u lt s r e p r e s e n t m e a n s o f 1 0 t r ia ls w it h d i(cid:11) e r e n t w e ig h t in it ia l-\niz a t io n s ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "D e s p it e t h e n o is y t a r g e t s , L S T M s t ill c a n s o lv e t h e p r o b le m b y le a r n in g t h e e x p e c t e d\nt a r g e t v a lu e s . 5 . 4 E X P E R I M E N T 4 : A D D I N G P R O B L E M\nT h e d i(cid:14) c u lt t a s k in t h is s e c t io n is o f a t y p e t h a t h a s n e v e r b e e n s o lv e d b y o t h e r r e c u r r e n t n e t a l-\ng o r it h m s . I t s h o w s t h a t L S T M c a n s o lv e lo n g t im e la g p r o b le m s in v o lv in g d is t r ib u t e d , c o n t in u o u s -\nv a lu e d r e p r e s e n t a t io n s . T a s k . E a c h e le m e n t o f e a c h in p u t s e q u e n c e is a p a ir o f c o m p o n e n t s ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "T h e (cid:12) r s t c o m p o n e n t\nis a r e a l v a lu e r a n d o m ly c h o s e n fr o m t h e in t e r v a l [(cid:0) 1 ; 1 ]; t h e s e c o n d is e it h e r 1 .0 , 0 .0 , o r - 1 .0 ,\na n d is u s e d a s a m a r k e r : a t t h e e n d o f e a c h s e q u e n c e , t h e t a s k is t o o u t p u t t h e s u m o f t h e (cid:12) r s t\nc o m p o n e n t s o f t h o s e p a ir s t h a t a r e m a r k e d b y s e c o n d c o m p o n e n t s e q u a l t o 1 .0 . S e q u e n c e s h a v e\nT\nr a n d o m le n g t h s b e t w e e n t h e m in im a l s e q u e n c e le n g t h T a n d T + ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "I n a g iv e n s e q u e n c e e x a c t ly\n1 0\nt w o p a ir s a r e m a r k e d a s fo llo w s : w e (cid:12) r s t r a n d o m ly s e le c t a n d m a r k o n e o f t h e (cid:12) r s t t e n p a ir s\nT\n( w h o s e (cid:12) r s t c o m p o n e n t w e c a ll X ) . T h e n w e r a n d o m ly s e le c t a n d m a r k o n e o f t h e (cid:12) r s t (cid:0) 1 1 2\ns t ill u n m a r k e d p a ir s ( w h o s e (cid:12) r s t c o m p o n e n t w e c a ll X ) . T h e s e c o n d c o m p o n e n t s o f a ll r e m a in in g 2\np a ir s a r e z e r o e x c e p t fo r t h e (cid:12) r s t a n d (cid:12) n a l p a ir , w h o s e s e c o n d c o m p o n e n t s a r e - 1 ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "( I n t h e r a r e c a s e\nw h e r e t h e (cid:12) r s t p a ir o f t h e s e q u e n c e g e t s m a r k e d , w e s e t X t o z e r o .) A n e r r o r s ig n a l is g e n e r a t e d 1\nX + X 1 2\no n ly a t t h e s e q u e n c e e n d : t h e t a r g e t is 0 :5 + ( t h e s u m X + X s c a le d t o t h e in t e r v a l [0 ; 1 ]) . 1 2 4 :0\nA s e q u e n c e is p r o c e s s e d c o r r e c t ly if t h e a b s o lu t e e r r o r a t t h e s e q u e n c e e n d is b e lo w 0 .0 4 . A r c h i t e c t u r e . W e u s e a 3 - la y e r n e t w it h 2 in p u t u n it s , 1 o u t p u t u n it , a n d 2 c e ll b lo c k s o f s iz e\n2 . T h e o u t p u t la y e r r e c e iv e s c o n n e c t io n s o n ly fr o m m e m o r y c e lls ."
  },
  {
    "chunk_id": "doc_4_p17_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "M e m o r y c e lls a n d g a t e u n it s\nr e c e iv e in p u t s fr o m m e m o r y c e lls a n d g a t e u n it s ( i.e ., t h e h id d e n la y e r is fu lly c o n n e c t e d | le s s\nc o n n e c t iv it y m a y w o r k a s w e ll) . T h e in p u t la y e r h a s fo r w a r d c o n n e c t io n s t o a ll u n it s in t h e h id d e n\n1 7"
  },
  {
    "chunk_id": "doc_4_p18_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "T m in im a l la g # w e ig h t s # w r o n g p r e d ic t io n s S u c c e s s a ft e r\n1 0 0 5 0 9 3 1 o u t o f 2 5 6 0 7 4 ,0 0 0\n5 0 0 2 5 0 9 3 0 o u t o f 2 5 6 0 2 0 9 ,0 0 0\n1 0 0 0 5 0 0 9 3 1 o u t o f 2 5 6 0 8 5 3 ,0 0 0\nT a b le 7 : E X P E R I M E N T 4 : R e s u lt s fo r t h e A d d in g P r o b le m . T is t h e m in im a l s e q u e n c e le n g t h ,\nT = 2 t h e m in im a l t im e la g . \\ # w r o n g p r e d ic t io n s \" is t h e n u m b e r o f in c o r r e c t ly p r o c e s s e d s e q u e n c e s\n( e r r o r > 0 .0 4 ) fr o m a t e s t s e t c o n t a in in g 2 5 6 0 s e q u e n c e s . T h e r ig h t m o s t c o lu m n g iv e s t h e n u m b e r\no f t r a in in g s e q u e n c e s r e q u ir e d t o a c h ie v e t h e s t o p p in g c r it e r io n ."
  },
  {
    "chunk_id": "doc_4_p18_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "A ll v a lu e s a r e m e a n s o f 1 0 t r ia ls . F o r T = 1 0 0 0 t h e n u m b e r o f r e q u ir e d t r a in in g e x a m p le s v a r ie s b e t w e e n 3 7 0 ,0 0 0 a n d 2 ,0 2 0 ,0 0 0 ,\ne x c e e d in g 7 0 0 ,0 0 0 in o n ly 3 c a s e s . la y e r . A ll n o n - in p u t u n it s h a v e b ia s w e ig h t s . T h e s e a r c h it e c t u r e p a r a m e t e r s m a k e it e a s y t o s t o r e\na t le a s t 2 in p u t s ig n a ls ( a c e ll b lo c k s iz e o f 1 w o r k s w e ll, t o o ) . A ll a c t iv a t io n fu n c t io n s a r e lo g is t ic\nw it h o u t p u t r a n g e [0 ; 1 ], e x c e p t fo r h , w h o s e r a n g e is [(cid:0) 1 ; 1 ], a n d g , w h o s e r a n g e is [(cid:0) 2 ; 2 ]. S t a t e d r i f t v e r s u s i n i t i a l b i a s ."
  },
  {
    "chunk_id": "doc_4_p18_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "N o t e t h a t t h e t a s k r e q u ir e s s t o r in g t h e p r e c is e v a lu e s o f\nr e a l n u m b e r s fo r lo n g d u r a t io n s | t h e s y s t e m m u s t le a r n t o p r o t e c t m e m o r y c e ll c o n t e n t s a g a in s t\ne v e n m in o r in t e r n a l s t a t e d r ift ( s e e S e c t io n 4 ) . T o s t u d y t h e s ig n i(cid:12) c a n c e o f t h e d r ift p r o b le m ,\nw e m a k e t h e t a s k e v e n m o r e d i(cid:14) c u lt b y b ia s in g a ll n o n - in p u t u n it s , t h u s a r t i(cid:12) c ia lly in d u c in g\nin t e r n a l s t a t e d r ift . A ll w e ig h t s ( in c lu d in g t h e b ia s w e ig h t s ) a r e r a n d o m ly in it ia liz e d in t h e r a n g e\n[(cid:0) 0 :1 ; 0 :1 ]."
  },
  {
    "chunk_id": "doc_4_p18_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "F o llo w in g S e c t io n 4 's r e m e d y fo r s t a t e d r ift s , t h e (cid:12) r s t in p u t g a t e b ia s is in it ia liz e d w it h\n(cid:0) 3 :0 , t h e s e c o n d w it h (cid:0) 6 :0 ( t h o u g h t h e p r e c is e v a lu e s h a r d ly m a t t e r , a s c o n (cid:12) r m e d b y a d d it io n a l\ne x p e r im e n t s ) . T r a i n i n g / T e s t i n g . T h e le a r n in g r a t e is 0 .5 . T r a in in g is s t o p p e d o n c e t h e a v e r a g e t r a in in g\ne r r o r is b e lo w 0 .0 1 , a n d t h e 2 0 0 0 m o s t r e c e n t s e q u e n c e s w e r e p r o c e s s e d c o r r e c t ly . R e s u l t s ."
  },
  {
    "chunk_id": "doc_4_p18_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "W it h a t e s t s e t c o n s is t in g o f 2 5 6 0 r a n d o m ly c h o s e n s e q u e n c e s , t h e a v e r a g e t e s t s e t\ne r r o r w a s a lw a y s b e lo w 0 .0 1 , a n d t h e r e w e r e n e v e r m o r e t h a n 3 in c o r r e c t ly p r o c e s s e d s e q u e n c e s . T a b le 7 s h o w s d e t a ils . T h e e x p e r im e n t d e m o n s t r a t e s : ( 1 ) L S T M is a b le t o w o r k w e ll w it h d is t r ib u t e d r e p r e s e n t a t io n s . ( 2 ) L S T M is a b le t o le a r n t o p e r fo r m c a lc u la t io n s in v o lv in g c o n t in u o u s v a lu e s ."
  },
  {
    "chunk_id": "doc_4_p18_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "( 3 ) S in c e t h e s y s t e m\nT\nm a n a g e s t o s t o r e c o n t in u o u s v a lu e s w it h o u t d e t e r io r a t io n fo r m in im a l d e la y s o f t im e s t e p s , t h e r e\n2\nis n o s ig n i(cid:12) c a n t , h a r m fu l in t e r n a l s t a t e d r ift . 5 . 5 E X P E R I M E N T 5 : M U L T I P L I C A T I O N P R O B L E M\nO n e m a y a r g u e t h a t L S T M is a b it b ia s e d t o w a r d s t a s k s s u c h a s t h e A d d in g P r o b le m fr o m t h e\np r e v io u s s u b s e c t io n . S o lu t io n s t o t h e A d d in g P r o b le m m a y e x p lo it t h e C E C 's b u ilt - in in t e g r a t io n\nc a p a b ilit ie s ."
  },
  {
    "chunk_id": "doc_4_p18_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "A lt h o u g h t h is C E C p r o p e r t y m a y b e v ie w e d a s a fe a t u r e r a t h e r t h a n a d is a d v a n t a g e\n( in t e g r a t io n s e e m s t o b e a n a t u r a l s u b t a s k o f m a n y t a s k s o c c u r r in g in t h e r e a l w o r ld ) , t h e q u e s t io n\na r is e s w h e t h e r L S T M c a n a ls o s o lv e t a s k s w it h in h e r e n t ly n o n - in t e g r a t iv e s o lu t io n s . T o t e s t t h is ,\nw e c h a n g e t h e p r o b le m b y r e q u ir in g t h e (cid:12) n a l t a r g e t t o e q u a l t h e p r o d u c t ( in s t e a d o f t h e s u m ) o f\ne a r lie r m a r k e d in p u t s . T a s k ."
  },
  {
    "chunk_id": "doc_4_p18_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "L ik e t h e t a s k in S e c t io n 5 .4 , e x c e p t t h a t t h e (cid:12) r s t c o m p o n e n t o f e a c h p a ir is a r e a l v a lu e\nr a n d o m ly c h o s e n fr o m t h e in t e r v a l [0 ; 1 ]. I n t h e r a r e c a s e w h e r e t h e (cid:12) r s t p a ir o f t h e in p u t s e q u e n c e\ng e t s m a r k e d , w e s e t X t o 1 .0 . T h e t a r g e t a t s e q u e n c e e n d is t h e p r o d u c t X (cid:2) X . 1 1 2\nA r c h i t e c t u r e . L ik e in S e c t io n 5 .4 . A ll w e ig h t s ( in c lu d in g t h e b ia s w e ig h t s ) a r e r a n d o m ly\nin it ia liz e d in t h e r a n g e [(cid:0) 0 :1 ; 0 :1 ]. T r a i n i n g / T e s t i n g . T h e le a r n in g r a t e is 0 .1 ."
  },
  {
    "chunk_id": "doc_4_p18_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "W e t e s t p e r fo r m a n c e t w ic e : a s s o o n a s le s s\nt h a n n o f t h e 2 0 0 0 m o s t r e c e n t t r a in in g s e q u e n c e s le a d t o a b s o lu t e e r r o r s e x c e e d in g 0 .0 4 , w h e r e s e q\nn = 1 4 0 , a n d n = 1 3 . W h y t h e s e v a lu e s ? n = 1 4 0 is s u (cid:14) c ie n t t o le a r n s t o r a g e o f t h e s e q s e q s e q\nr e le v a n t in p u t s . I t is n o t e n o u g h t h o u g h t o (cid:12) n e - t u n e t h e p r e c is e (cid:12) n a l o u t p u t s . n = 1 3 , h o w e v e r , s e q\n1 8"
  },
  {
    "chunk_id": "doc_4_p19_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "T m in im a l la g # w e ig h t s n # w r o n g p r e d ic t io n s M S E S u c c e s s a ft e r s e q\n1 0 0 5 0 9 3 1 4 0 1 3 9 o u t o f 2 5 6 0 0 .0 2 2 3 4 8 2 ,0 0 0\n1 0 0 5 0 9 3 1 3 1 4 o u t o f 2 5 6 0 0 .0 1 3 9 1 ,2 7 3 ,0 0 0\nT a b le 8 : E X P E R I M E N T 5 : R e s u lt s fo r t h e M u lt ip lic a t io n P r o b le m . T is t h e m in im a l s e q u e n c e\nle n g t h , T = 2 t h e m in im a l t im e la g . W e t e s t o n a t e s t s e t c o n t a in in g 2 5 6 0 s e q u e n c e s a s s o o n a s le s s\nt h a n n o f t h e 2 0 0 0 m o s t r e c e n t t r a in in g s e q u e n c e s le a d t o e r r o r > 0 .0 4 . \\ # w r o n g p r e d ic t io n s \" s e q\nis t h e n u m b e r o f t e s t s e q u e n c e s w it h e r r o r > 0 .0 4 ."
  },
  {
    "chunk_id": "doc_4_p19_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "M S E is t h e m e a n s q u a r e d e r r o r o n t h e t e s t\ns e t . T h e r ig h t m o s t c o lu m n lis t s n u m b e r s o f t r a in in g s e q u e n c e s r e q u ir e d t o a c h ie v e t h e s t o p p in g\nc r it e r io n . A ll v a lu e s a r e m e a n s o f 1 0 t r ia ls . le a d s t o q u it e s a t is fa c t o r y r e s u lt s . R e s u l t s . F o r n = 1 4 0 ( n = 1 3 ) w it h a t e s t s e t c o n s is t in g o f 2 5 6 0 r a n d o m ly c h o s e n s e q s e q\ns e q u e n c e s , t h e a v e r a g e t e s t s e t e r r o r w a s a lw a y s b e lo w 0 .0 2 6 ( 0 .0 1 3 ) , a n d t h e r e w e r e n e v e r m o r e\nt h a n 1 7 0 ( 1 5 ) in c o r r e c t ly p r o c e s s e d s e q u e n c e s . T a b le 8 s h o w s d e t a ils ."
  },
  {
    "chunk_id": "doc_4_p19_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "( A n e t w it h a d d it io n a l\ns t a n d a r d h id d e n u n it s o r w it h a h id d e n la y e r a b o v e t h e m e m o r y c e lls m a y le a r n t h e (cid:12) n e - t u n in g\np a r t m o r e q u ic k ly .) T h e e x p e r im e n t d e m o n s t r a t e s : L S T M c a n s o lv e t a s k s in v o lv in g b o t h c o n t in u o u s - v a lu e d r e p r e -\ns e n t a t io n s a n d n o n - in t e g r a t iv e in fo r m a t io n p r o c e s s in g . 5 . 6 E X P E R I M E N T 6 : T E M P O R A L O R D E R\nI n t h is s u b s e c t io n , L S T M s o lv e s o t h e r d i(cid:14) c u lt ( b u t a r t i(cid:12) c ia l) t a s k s t h a t h a v e n e v e r b e e n s o lv e d b y\np r e v io u s r e c u r r e n t n e t a lg o r it h m s ."
  },
  {
    "chunk_id": "doc_4_p19_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "T h e e x p e r im e n t s h o w s t h a t L S T M is a b le t o e x t r a c t in fo r m a t io n\nc o n v e y e d b y t h e t e m p o r a l o r d e r o f w id e ly s e p a r a t e d in p u t s . T a s k 6 a : t w o r e l e v a n t , w i d e l y s e p a r a t e d s y m b o l s . T h e g o a l is t o c la s s ify s e q u e n c e s . E le m e n t s a n d t a r g e t s a r e r e p r e s e n t e d lo c a lly ( in p u t v e c t o r s w it h o n ly o n e n o n - z e r o b it ) . T h e\ns e q u e n c e s t a r t s w it h a n E , e n d s w it h a B ( t h e \\ t r ig g e r s y m b o l\" ) a n d o t h e r w is e c o n s is t s o f r a n d o m ly\nc h o s e n s y m b o ls fr o m t h e s e t f a ; b ; c ; d g e x c e p t fo r t w o e le m e n t s a t p o s it io n s t a n d t t h a t a r e e it h e r 1 2\nX o r Y ."
  },
  {
    "chunk_id": "doc_4_p19_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "T h e s e q u e n c e le n g t h is r a n d o m ly c h o s e n b e t w e e n 1 0 0 a n d 1 1 0 , t is r a n d o m ly c h o s e n 1\nb e t w e e n 1 0 a n d 2 0 , a n d t is r a n d o m ly c h o s e n b e t w e e n 5 0 a n d 6 0 . T h e r e a r e 4 s e q u e n c e c la s s e s 2\nQ ; R ; S ; U w h ic h d e p e n d o n t h e t e m p o r a l o r d e r o f X a n d Y . T h e r u le s a r e : X ; X ! Q ; X ; Y ! R ; Y ; X ! S ; Y ; Y ! U . T a s k 6 b : t h r e e r e l e v a n t , w i d e l y s e p a r a t e d s y m b o l s . A g a in , t h e g o a l is t o c la s s ify\ns e q u e n c e s . E le m e n t s / t a r g e t s a r e r e p r e s e n t e d lo c a lly ."
  },
  {
    "chunk_id": "doc_4_p19_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "T h e s e q u e n c e s t a r t s w it h a n E , e n d s w it h\na B ( t h e \\ t r ig g e r s y m b o l\" ) , a n d o t h e r w is e c o n s is t s o f r a n d o m ly c h o s e n s y m b o ls fr o m t h e s e t\nf a ; b ; c ; d g e x c e p t fo r t h r e e e le m e n t s a t p o s it io n s t ; t a n d t t h a t a r e e it h e r X o r Y . T h e s e q u e n c e 1 2 3\nle n g t h is r a n d o m ly c h o s e n b e t w e e n 1 0 0 a n d 1 1 0 , t is r a n d o m ly c h o s e n b e t w e e n 1 0 a n d 2 0 , t is 1 2\nr a n d o m ly c h o s e n b e t w e e n 3 3 a n d 4 3 , a n d t is r a n d o m ly c h o s e n b e t w e e n 6 6 a n d 7 6 . T h e r e a r e 8 3\ns e q u e n c e c la s s e s Q ; R ; S ; U ; V ; A ; B ; C w h ic h d e p e n d o n t h e t e m p o r a l o r d e r o f t h e X s a n d Y s ."
  },
  {
    "chunk_id": "doc_4_p19_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "T h e\nr u le s a r e : X ; X ; X ! Q ; X ; X ; Y ! R ; X ; Y ; X ! S ; X ; Y ; Y ! U ; Y ; X ; X ! V ; Y ; X ; Y ! A ; Y ; Y ; X ! B ; Y ; Y ; Y ! C . T h e r e a r e a s m a n y o u t p u t u n it s a s t h e r e a r e c la s s e s . E a c h c la s s is lo c a lly r e p r e s e n t e d b y a\nb in a r y t a r g e t v e c t o r w it h o n e n o n - z e r o c o m p o n e n t . W it h b o t h t a s k s , e r r o r s ig n a ls o c c u r o n ly a t\nt h e e n d o f a s e q u e n c e . T h e s e q u e n c e is c la s s i(cid:12) e d c o r r e c t ly if t h e (cid:12) n a l a b s o lu t e e r r o r o f a ll o u t p u t\nu n it s is b e lo w 0 .3 . A r c h i t e c t u r e ."
  },
  {
    "chunk_id": "doc_4_p19_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "W e u s e a 3 - la y e r n e t w it h 8 in p u t u n it s , 2 ( 3 ) c e ll b lo c k s o f s iz e 2 a n d 4\n( 8 ) o u t p u t u n it s fo r T a s k 6 a ( 6 b ) . A g a in a ll n o n - in p u t u n it s h a v e b ia s w e ig h t s , a n d t h e o u t p u t\nla y e r r e c e iv e s c o n n e c t io n s fr o m m e m o r y c e lls o n ly . M e m o r y c e lls a n d g a t e u n it s r e c e iv e in p u t s\nfr o m in p u t u n it s , m e m o r y c e lls a n d g a t e u n it s ( i.e ., t h e h id d e n la y e r is fu lly c o n n e c t e d | le s s\nc o n n e c t iv it y m a y w o r k a s w e ll) . T h e a r c h it e c t u r e p a r a m e t e r s fo r T a s k 6 a ( 6 b ) m a k e it e a s y t o\n1 9"
  },
  {
    "chunk_id": "doc_4_p20_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "s t o r e a t le a s t 2 ( 3 ) in p u t s ig n a ls . A ll a c t iv a t io n fu n c t io n s a r e lo g is t ic w it h o u t p u t r a n g e [0 ; 1 ],\ne x c e p t fo r h , w h o s e r a n g e is [(cid:0) 1 ; 1 ], a n d g , w h o s e r a n g e is [(cid:0) 2 ; 2 ]. T r a i n i n g / T e s t i n g . T h e le a r n in g r a t e is 0 .5 ( 0 .1 ) fo r E x p e r im e n t 6 a ( 6 b ) . T r a in in g is s t o p p e d\no n c e t h e a v e r a g e t r a in in g e r r o r fa lls b e lo w 0 .1 a n d t h e 2 0 0 0 m o s t r e c e n t s e q u e n c e s w e r e c la s s i(cid:12) e d\nc o r r e c t ly . A ll w e ig h t s a r e in it ia liz e d in t h e r a n g e [(cid:0) 0 :1 ; 0 :1 ]."
  },
  {
    "chunk_id": "doc_4_p20_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "T h e (cid:12) r s t in p u t g a t e b ia s is in it ia liz e d\nw it h (cid:0) 2 :0 , t h e s e c o n d w it h (cid:0) 4 :0 , a n d ( fo r E x p e r im e n t 6 b ) t h e t h ir d w it h (cid:0) 6 :0 ( a g a in , w e c o n (cid:12) r m e d\nb y a d d it io n a l e x p e r im e n t s t h a t t h e p r e c is e v a lu e s h a r d ly m a t t e r ) . R e s u l t s . W it h a t e s t s e t c o n s is t in g o f 2 5 6 0 r a n d o m ly c h o s e n s e q u e n c e s , t h e a v e r a g e t e s t s e t\ne r r o r w a s a lw a y s b e lo w 0 .1 , a n d t h e r e w e r e n e v e r m o r e t h a n 3 in c o r r e c t ly c la s s i(cid:12) e d s e q u e n c e s . T a b le 9 s h o w s d e t a ils ."
  },
  {
    "chunk_id": "doc_4_p20_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "T h e e x p e r im e n t s h o w s t h a t L S T M is a b le t o e x t r a c t in fo r m a t io n c o n v e y e d b y t h e t e m p o r a l\no r d e r o f w id e ly s e p a r a t e d in p u t s . I n T a s k 6 a , fo r in s t a n c e , t h e d e la y s b e t w e e n (cid:12) r s t a n d s e c o n d\nr e le v a n t in p u t a n d b e t w e e n s e c o n d r e le v a n t in p u t a n d s e q u e n c e e n d a r e a t le a s t 3 0 t im e s t e p s . t a s k # w e ig h t s # w r o n g p r e d ic t io n s S u c c e s s a ft e r\nT a s k 6 a 1 5 6 1 o u t o f 2 5 6 0 3 1 ,3 9 0\nT a s k 6 b 3 0 8 2 o u t o f 2 5 6 0 5 7 1 ,1 0 0\nT a b le 9 : E X P E R I M E N T 6 : R e s u lt s fo r t h e T e m p o r a l O r d e r P r o b le m ."
  },
  {
    "chunk_id": "doc_4_p20_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "\\ # w r o n g p r e d ic t io n s \" is\nt h e n u m b e r o f in c o r r e c t ly c la s s i(cid:12) e d s e q u e n c e s ( e r r o r > 0 .3 fo r a t le a s t o n e o u t p u t u n it ) fr o m a\nt e s t s e t c o n t a in in g 2 5 6 0 s e q u e n c e s . T h e r ig h t m o s t c o lu m n g iv e s t h e n u m b e r o f t r a in in g s e q u e n c e s\nr e q u ir e d t o a c h ie v e t h e s t o p p in g c r it e r io n . T h e r e s u lt s fo r T a s k 6 a a r e m e a n s o f 2 0 t r ia ls ; t h o s e\nfo r T a s k 6 b o f 1 0 t r ia ls . T y p i c a l s o l u t i o n s . I n E x p e r im e n t 6 a , h o w d o e s L S T M d is t in g u is h b e t w e e n t e m p o r a l o r d e r s\n( X ; Y ) a n d ( Y ; X ) ?"
  },
  {
    "chunk_id": "doc_4_p20_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "O n e o f m a n y p o s s ib le s o lu t io n s is t o s t o r e t h e (cid:12) r s t X o r Y in c e ll b lo c k 1 , a n d\nt h e s e c o n d X = Y in c e ll b lo c k 2 . B e fo r e t h e (cid:12) r s t X = Y o c c u r s , b lo c k 1 c a n s e e t h a t it is s t ill e m p t y\nb y m e a n s o f it s r e c u r r e n t c o n n e c t io n s . A ft e r t h e (cid:12) r s t X = Y , b lo c k 1 c a n c lo s e it s in p u t g a t e . O n c e\nb lo c k 1 is (cid:12) lle d a n d c lo s e d , t h is fa c t w ill b e c o m e v is ib le t o b lo c k 2 ( r e c a ll t h a t a ll g a t e u n it s a n d\na ll m e m o r y c e lls r e c e iv e c o n n e c t io n s fr o m a ll n o n - o u t p u t u n it s ) . T y p ic a l s o lu t io n s , h o w e v e r , r e q u ir e o n ly o n e m e m o r y c e ll b lo c k ."
  },
  {
    "chunk_id": "doc_4_p20_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "T h e b lo c k s t o r e s t h e (cid:12) r s t X\no r Y ; o n c e t h e s e c o n d X = Y o c c u r s , it c h a n g e s it s s t a t e d e p e n d in g o n t h e (cid:12) r s t s t o r e d s y m b o l.\nS o lu t io n t y p e 1 e x p lo it s t h e c o n n e c t io n b e t w e e n m e m o r y c e ll o u t p u t a n d in p u t g a t e u n it | t h e\nfo llo w in g e v e n t s c a u s e d i(cid:11) e r e n t in p u t g a t e a c t iv a t io n s : \\ X o c c u r s in c o n ju n c t io n w it h a (cid:12) lle d\nb lo c k \" ; \\ X o c c u r s in c o n ju n c t io n w it h a n e m p t y b lo c k \" . S o lu t io n t y p e 2 is b a s e d o n a s t r o n g\np o s it iv e c o n n e c t io n b e t w e e n m e m o r y c e ll o u t p u t a n d m e m o r y c e ll in p u t ."
  },
  {
    "chunk_id": "doc_4_p20_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "T h e p r e v io u s o c c u r r e n c e\no f X ( Y ) is r e p r e s e n t e d b y a p o s it iv e ( n e g a t iv e ) in t e r n a l s t a t e . O n c e t h e in p u t g a t e o p e n s fo r t h e\ns e c o n d t im e , s o d o e s t h e o u t p u t g a t e , a n d t h e m e m o r y c e ll o u t p u t is fe d b a c k t o it s o w n in p u t . T h is c a u s e s ( X ; Y ) t o b e r e p r e s e n t e d b y a p o s it iv e in t e r n a l s t a t e , b e c a u s e X c o n t r ib u t e s t o t h e\nn e w in t e r n a l s t a t e t w ic e ( v ia c u r r e n t in t e r n a l s t a t e a n d c e ll o u t p u t fe e d b a c k ) . S im ila r ly , ( Y ; X )\ng e t s r e p r e s e n t e d b y a n e g a t iv e in t e r n a l s t a t e . 5 ."
  },
  {
    "chunk_id": "doc_4_p20_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "7 S U M M A R Y O F E X P E R I M E N T A L C O N D I T I O N S\nT h e t w o t a b le s in t h is s u b s e c t io n p r o v id e a n o v e r v ie w o f t h e m o s t im p o r t a n t L S T M p a r a m e t e r s\na n d a r c h it e c t u r a l d e t a ils fo r E x p e r im e n t s 1 { 6 . T h e c o n d it io n s o f t h e s im p le e x p e r im e n t s 2 a a n d\n2 b d i(cid:11) e r s lig h t ly fr o m t h o s e o f t h e o t h e r , m o r e s y s t e m a t ic e x p e r im e n t s , d u e t o h is t o r ic a l r e a s o n s . 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5\nT a s k p la g b s in o u t w c o g b ig b b ia s h g (cid:11)\n1 - 1 9 9 4 1 7 7 2 6 4 F - 1 ,- 2 ,- 3 ,- 4 r g a h 1 g 2 0 .1\n1 - 2 9 9 3 2 7 7 2 7 6 F - 1 ,- 2 ,- 3 r g a h 1 g 2 0 .1\nt o b e c o n t in u e d o n n e x t p a g e\n2 0"
  },
  {
    "chunk_id": "doc_4_p21_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_4_p21_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": "c o n t in u e d fr o m p r e v io u s p a g e\n1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5\nT a s k p la g b s in o u t w c o g b ig b b ia s h g (cid:11)\n1 - 3 9 9 3 2 7 7 2 7 6 F - 1 ,- 2 ,- 3 r g a h 1 g 2 0 .2\n1 - 4 9 9 4 1 7 7 2 6 4 F - 1 ,- 2 ,- 3 ,- 4 r g a h 1 g 2 0 .5\n1 - 5 9 9 3 2 7 7 2 7 6 F - 1 ,- 2 ,- 3 r g a h 1 g 2 0 .5\n2 a 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 5 0 4 B n o o g n o n e n o n e id g 1 1 .0\n2 b 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 5 0 4 B n o o g n o n e n o n e id g 1 1 .0\n2 c - 1 5 0 5 0 2 1 5 4 2 3 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 2 1 0 0 1 0 0 2 1 1 0 4 2 6 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 3 2 0 0 2 0 0 2 1 2 0 4 2 1 2 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 4 5 0 0 5 0 0 2 1 5 0 4 2 3 0 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 5 1 0 0 0 1 0 0 0 2 1 1 0 0 4 2 6 0 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 6 1 0 0 0 1 0 0 0 2 1 5 0 4 2 3 0 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 7 1 0 0 0 1 0 0 0 2 1 2 0 4 2 1 2 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 8 1 0 0 0 1 0 0 0 2 1 1 0 4 2 6 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n2 c - 9 1 0 0 0 1 0 0 0 2 1 5 4 2 3 6 4 F n o n e n o n e n o n e h 1 g 2 0 .0 1\n3 a 1 0 0 1 0 0 3 1 1 1 1 0 2 F - 2 ,- 4 ,- 6 - 1 ,- 3 ,- 5 b 1 h 1 g 2 1 .0\n3 b 1 0 0 1 0 0 3 1 1 1 1 0 2 F - 2 ,- 4 ,- 6 - 1 ,- 3 ,- 5 b 1 h 1 g 2 1 .0\n3 c 1 0 0 1 0 0 3 1 1 1 1 0 2 F - 2 ,- 4 ,- 6 - 1 ,- 3 ,- 5 b 1 h 1 g 2 0 .1\n4 - 1 1 0 0 5 0 2 2 2 1 9 3 F r - 3 ,- 6 a ll h 1 g 2 0 .5\n4 - 2 5 0 0 2 5 0 2 2 2 1 9 3 F r - 3 ,- 6 a ll h 1 g 2 0 .5\n4 - 3 1 0 0 0 5 0 0 2 2 2 1 9 3 F r - 3 ,- 6 a ll h 1 g 2 0 .5\n5 1 0 0 5 0 2 2 2 1 9 3 F r r a ll h 1 g 2 0 .1\n6 a 1 0 0 4 0 2 2 8 4 1 5 6 F r - 2 ,- 4 a ll h 1 g 2 0 .5\n6 b 1 0 0 2 4 3 2 8 8 3 0 8 F r - 2 ,- 4 ,- 6 a ll h 1 g 2 0 .1\nT a b le 1 0 : S u m m a r y o f e x p e r im e n t a l c o n d it io n s fo r L S T M , P a r t I ."
  },
  {
    "chunk_id": "doc_4_p21_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": "1 s t c o lu m n : t a s k n u m b e r . 2 n d\nc o lu m n : m in im a l s e q u e n c e le n g t h p . 3 r d c o lu m n : m in im a l n u m b e r o f s t e p s b e t w e e n m o s t r e c e n t\nr e le v a n t in p u t in fo r m a t io n a n d t e a c h e r s ig n a l. 4 t h c o lu m n : n u m b e r o f c e ll b lo c k s b . 5 t h c o lu m n :\nb lo c k s iz e s . 6 t h c o lu m n : n u m b e r o f in p u t u n it s i n . 7 t h c o lu m n : n u m b e r o f o u t p u t u n it s o u t . 8 t h\nc o lu m n : n u m b e r o f w e ig h t s w ."
  },
  {
    "chunk_id": "doc_4_p21_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": "9 t h c o lu m n : c d e s c r ib e s c o n n e c t iv it y : \\ F \" m e a n s \\ o u t p u t la y e r\nr e c e iv e s c o n n e c t io n s fr o m m e m o r y c e lls ; m e m o r y c e lls a n d g a t e u n it s r e c e iv e c o n n e c t io n s fr o m\nin p u t u n it s , m e m o r y c e lls a n d g a t e u n it s \" ; \\ B \" m e a n s \\ e a c h la y e r r e c e iv e s c o n n e c t io n s fr o m a ll\nla y e r s b e lo w \" . 1 0 t h c o lu m n : in it ia l o u t p u t g a t e b ia s o g b , w h e r e \\ r \" s t a n d s fo r \\ r a n d o m ly c h o s e n\nfr o m t h e in t e r v a l [(cid:0) 0 :1 ; 0 :1 ]\" a n d \\ n o o g \" m e a n s \\ n o o u t p u t g a t e u s e d \" . 1 1 t h c o lu m n : in it ia l in p u t\ng a t e b ia s i g b ( s e e 1 0 t h c o lu m n ) ."
  },
  {
    "chunk_id": "doc_4_p21_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": "1 2 t h c o lu m n : w h ic h u n it s h a v e b ia s w e ig h t s ? \\ b 1 \" s t a n d s fo r \\ a ll\nh id d e n u n it s \" , \\ g a \" fo r \\ o n ly g a t e u n it s \" , a n d \\ a ll\" fo r \\ a ll n o n - in p u t u n it s \" . 1 3 t h c o lu m n : t h e\nfu n c t io n h , w h e r e \\ id \" is id e n t it y fu n c t io n , \\ h 1 \" is lo g is t ic s ig m o id in [(cid:0) 2 ; 2 ]. 1 4 t h c o lu m n : t h e\nlo g is t ic fu n c t io n g , w h e r e \\ g 1 \" is s ig m o id in [0 ; 1 ], \\ g 2 \" in [(cid:0) 1 ; 1 ]. 1 5 t h c o lu m n : le a r n in g r a t e (cid:11) . 1 2 3 4 5 6\nT a s k s e le c t in t e r v a l t e s t s e t s iz e s t o p p in g c r it e r io n s u c c e s s\n1 t 1 [(cid:0) 0 :2 ; 0 :2 ] 2 5 6 t r a in in g & t e s t c o r r e c t ly p r e d ."
  },
  {
    "chunk_id": "doc_4_p21_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": "s e e t e x t\n2 a t 1 [(cid:0) 0 :2 ; 0 :2 ] n o t e s t s e t a ft e r 5 m illio n e x e m p la r s A B S ( 0 .2 5 )\n2 b t 2 [(cid:0) 0 :2 ; 0 :2 ] 1 0 0 0 0 a ft e r 5 m illio n e x e m p la r s A B S ( 0 .2 5 )\n2 c t 2 [(cid:0) 0 :2 ; 0 :2 ] 1 0 0 0 0 a ft e r 5 m illio n e x e m p la r s A B S ( 0 .2 )\n3 a t 3 [(cid:0) 0 :1 ; 0 :1 ] 2 5 6 0 S T 1 a n d S T 2 ( s e e t e x t ) A B S ( 0 .2 )\n3 b t 3 [(cid:0) 0 :1 ; 0 :1 ] 2 5 6 0 S T 1 a n d S T 2 ( s e e t e x t ) A B S ( 0 .2 )\n3 c t 3 [(cid:0) 0 :1 ; 0 :1 ] 2 5 6 0 S T 1 a n d S T 2 ( s e e t e x t ) s e e t e x t\n4 t 3 [(cid:0) 0 :1 ; 0 :1 ] 2 5 6 0 S T 3 ( 0 .0 1 ) A B S ( 0 .0 4 )\n5 t 3 [(cid:0) 0 :1 ; 0 :1 ] 2 5 6 0 s e e t e x t A B S ( 0 .0 4 )\n6 a t 3 [(cid:0) 0 :1 ; 0 :1 ] 2 5 6 0 S T 3 ( 0 .1 ) A B S ( 0 .3 )\n6 b t 3 [(cid:0) 0 :1 ; 0 :1 ] 2 5 6 0 S T 3 ( 0 .1 ) A B S ( 0 .3 )\n2 1"
  },
  {
    "chunk_id": "doc_4_p22_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "T a b le 1 1 : S u m m a r y o f e x p e r im e n t a l c o n d it io n s fo r L S T M , P a r t I I . 1 s t c o lu m n : t a s k n u m b e r . 2 n d c o lu m n : t r a in in g e x e m p la r s e le c t io n , w h e r e \\ t 1 \" s t a n d s fo r \\ r a n d o m ly c h o s e n fr o m t r a in in g\ns e t \" , \\ t 2 \" fo r \\ r a n d o m ly c h o s e n fr o m 2 c la s s e s \" , a n d \\ t 3 \" fo r \\ r a n d o m ly g e n e r a t e d o n - lin e \" . 3 r d\nc o lu m n : w e ig h t in it ia liz a t io n in t e r v a l. 4 t h c o lu m n : t e s t s e t s iz e ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "5 t h c o lu m n : s t o p p in g c r it e r io n\nfo r t r a in in g , w h e r e \\ S T 3 ( (cid:12) ) \" s t a n d s fo r \\ a v e r a g e t r a in in g e r r o r b e lo w (cid:12) a n d t h e 2 0 0 0 m o s t r e c e n t\ns e q u e n c e s w e r e p r o c e s s e d c o r r e c t ly \" . 6 t h c o lu m n : s u c c e s s ( c o r r e c t c la s s i(cid:12) c a t io n ) c r it e r io n , w h e r e\n\\ A B S ( (cid:12) ) \" s t a n d s fo r \\ a b s o lu t e e r r o r o f a ll o u t p u t u n it s a t s e q u e n c e e n d is b e lo w (cid:12) \" . 6 D I S C U S S I O N\nL i m i t a t i o n s o f L S T M ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "(cid:15) T h e p a r t ic u la r ly e (cid:14) c ie n t t r u n c a t e d b a c k p r o p v e r s io n o f t h e L S T M a lg o r it h m w ill n o t e a s ily\ns o lv e p r o b le m s s im ila r t o \\ s t r o n g ly d e la y e d X O R p r o b le m s \" , w h e r e t h e g o a l is t o c o m p u t e t h e\nX O R o f t w o w id e ly s e p a r a t e d in p u t s t h a t p r e v io u s ly o c c u r r e d s o m e w h e r e in a n o is y s e q u e n c e ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "T h e r e a s o n is t h a t s t o r in g o n ly o n e o f t h e in p u t s w ill n o t h e lp t o r e d u c e t h e e x p e c t e d e r r o r\n| t h e t a s k is n o n - d e c o m p o s a b le in t h e s e n s e t h a t it is im p o s s ib le t o in c r e m e n t a lly r e d u c e\nt h e e r r o r b y (cid:12) r s t s o lv in g a n e a s ie r s u b g o a l.\nI n t h e o r y , t h is lim it a t io n c a n b e c ir c u m v e n t e d b y u s in g t h e fu ll g r a d ie n t ( p e r h a p s w it h a d -\nd it io n a l c o n v e n t io n a l h id d e n u n it s r e c e iv in g in p u t fr o m t h e m e m o r y c e lls ) . B u t w e d o n o t\nr e c o m m e n d c o m p u t in g t h e fu ll g r a d ie n t fo r t h e fo llo w in g r e a s o n s : ( 1 ) I t in c r e a s e s c o m p u t a -\nt io n a l c o m p le x it y ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "( 2 ) C o n s t a n t e r r o r (cid:13) o w t h r o u g h C E C s c a n b e s h o w n o n ly fo r t r u n c a t e d\nL S T M . ( 3 ) W e a c t u a lly d id c o n d u c t a fe w e x p e r im e n t s w it h n o n - t r u n c a t e d L S T M . T h e r e\nw a s n o s ig n i(cid:12) c a n t d i(cid:11) e r e n c e t o t r u n c a t e d L S T M , e x a c t ly b e c a u s e o u t s id e t h e C E C s e r r o r\n(cid:13) o w t e n d s t o v a n is h q u ic k ly . F o r t h e s a m e r e a s o n fu ll B P T T d o e s n o t o u t p e r fo r m t r u n c a t e d\nB P T T . (cid:15) E a c h m e m o r y c e ll b lo c k n e e d s t w o a d d it io n a l u n it s ( in p u t a n d o u t p u t g a t e ) ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "I n c o m p a r is o n\nt o s t a n d a r d r e c u r r e n t n e t s , h o w e v e r , t h is d o e s n o t in c r e a s e t h e n u m b e r o f w e ig h t s b y m o r e\nt h a n a fa c t o r o f 9 : e a c h c o n v e n t io n a l h id d e n u n it is r e p la c e d b y a t m o s t 3 u n it s in t h e\n2\nL S T M a r c h it e c t u r e , in c r e a s in g t h e n u m b e r o f w e ig h t s b y a fa c t o r o f 3 in t h e fu lly c o n n e c t e d\nc a s e . N o t e , h o w e v e r , t h a t o u r e x p e r im e n t s u s e q u it e c o m p a r a b le w e ig h t n u m b e r s fo r t h e\na r c h it e c t u r e s o f L S T M a n d c o m p e t in g a p p r o a c h e s ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "(cid:15) G e n e r a lly s p e a k in g , d u e t o it s c o n s t a n t e r r o r (cid:13) o w t h r o u g h C E C s w it h in m e m o r y c e lls , L S T M\nr u n s in t o p r o b le m s s im ila r t o t h o s e o f fe e d fo r w a r d n e t s s e e in g t h e e n t ir e in p u t s t r in g a t o n c e . F o r in s t a n c e , t h e r e a r e t a s k s t h a t c a n b e q u ic k ly s o lv e d b y r a n d o m w e ig h t g u e s s in g b u t n o t b y\nt h e t r u n c a t e d L S T M a lg o r it h m w it h s m a ll w e ig h t in it ia liz a t io n s , s u c h a s t h e 5 0 0 - s t e p p a r it y\np r o b le m ( s e e in t r o d u c t io n t o S e c t io n 5 ) ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "H e r e , L S T M 's p r o b le m s a r e s im ila r t o t h e o n e s o f\na fe e d fo r w a r d n e t w it h 5 0 0 in p u t s , t r y in g t o s o lv e 5 0 0 - b it p a r it y . I n d e e d L S T M t y p ic a lly\nb e h a v e s m u c h lik e a fe e d fo r w a r d n e t t r a in e d b y b a c k p r o p t h a t s e e s t h e e n t ir e in p u t . B u t\nt h a t 's a ls o p r e c is e ly w h y it s o c le a r ly o u t p e r fo r m s p r e v io u s a p p r o a c h e s o n m a n y n o n - t r iv ia l\nt a s k s w it h s ig n i(cid:12) c a n t s e a r c h s p a c e s . (cid:15) L S T M d o e s n o t h a v e a n y p r o b le m s w it h t h e n o t io n o f \\ r e c e n c y \" t h a t g o b e y o n d t h o s e o f\no t h e r a p p r o a c h e s ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_8",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "A ll g r a d ie n t - b a s e d a p p r o a c h e s , h o w e v e r , s u (cid:11) e r fr o m p r a c t ic a l in a b ilit y t o\np r e c is e ly c o u n t d is c r e t e t im e s t e p s . I f it m a k e s a d i(cid:11) e r e n c e w h e t h e r a c e r t a in s ig n a l o c c u r r e d\n9 9 o r 1 0 0 s t e p s a g o , t h e n a n a d d it io n a l c o u n t in g m e c h a n is m s e e m s n e c e s s a r y . E a s ie r t a s k s ,\nh o w e v e r , s u c h a s o n e t h a t o n ly r e q u ir e s t o m a k e a d i(cid:11) e r e n c e b e t w e e n , s a y , 3 a n d 1 1 s t e p s ,\nd o n o t p o s e a n y p r o b le m s t o L S T M ."
  },
  {
    "chunk_id": "doc_4_p22_fixed_9",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "F o r in s t a n c e , b y g e n e r a t in g a n a p p r o p r ia t e n e g a t iv e\nc o n n e c t io n b e t w e e n m e m o r y c e ll o u t p u t a n d in p u t , L S T M c a n g iv e m o r e w e ig h t t o r e c e n t\nin p u t s a n d le a r n d e c a y s w h e r e n e c e s s a r y . 2 2"
  },
  {
    "chunk_id": "doc_4_p23_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "A\n7\nE\nc\nc\nc\np\ng\nw\n(\n1\n8\nT\na\na\nF\nA\nA\nI\nt\na\nd v a n t a g e s o f L S T M . (cid:15) T h e c o n s t a n t e r r o r b a c k p r o p a g a t io n w it h in m e m o r y c e lls r e s u lt s in L S T M 's a b ilit y t o b r id g e\nv e r y lo n g t im e la g s in c a s e o f p r o b le m s s im ila r t o t h o s e d is c u s s e d a b o v e . (cid:15) F o r lo n g t im e la g p r o b le m s s u c h a s t h o s e d is c u s s e d in t h is p a p e r , L S T M c a n h a n d le n o is e ,\nd is t r ib u t e d r e p r e s e n t a t io n s , a n d c o n t in u o u s v a lu e s . I n c o n t r a s t t o (cid:12) n it e s t a t e a u t o m a t a o r\nh id d e n M a r k o v m o d e ls L S T M d o e s n o t r e q u ir e a n a p r io r i c h o ic e o f a (cid:12) n it e n u m b e r o f s t a t e s ."
  },
  {
    "chunk_id": "doc_4_p23_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "I n p r in c ip le it c a n d e a l w it h u n lim it e d s t a t e n u m b e r s . (cid:15) F o r p r o b le m s d is c u s s e d in t h is p a p e r L S T M g e n e r a liz e s w e ll | e v e n if t h e p o s it io n s o f w id e ly\ns e p a r a t e d , r e le v a n t in p u t s in t h e in p u t s e q u e n c e d o n o t m a t t e r . U n lik e p r e v io u s a p p r o a c h e s ,\no u r s q u ic k ly le a r n s t o d is t in g u is h b e t w e e n t w o o r m o r e w id e ly s e p a r a t e d o c c u r r e n c e s o f a\np a r t ic u la r e le m e n t in a n in p u t s e q u e n c e , w it h o u t d e p e n d in g o n a p p r o p r ia t e s h o r t t im e la g\nt r a in in g e x e m p la r s . (cid:15) T h e r e a p p e a r s t o b e n o n e e d fo r p a r a m e t e r (cid:12) n e t u n in g ."
  },
  {
    "chunk_id": "doc_4_p23_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "L S T M w o r k s w e ll o v e r a b r o a d r a n g e\no f p a r a m e t e r s s u c h a s le a r n in g r a t e , in p u t g a t e b ia s a n d o u t p u t g a t e b ia s . F o r in s t a n c e , t o\ns o m e r e a d e r s t h e le a r n in g r a t e s u s e d in o u r e x p e r im e n t s m a y s e e m la r g e . H o w e v e r , a la r g e\nle a r n in g r a t e p u s h e s t h e o u t p u t g a t e s t o w a r d s z e r o , t h u s a u t o m a t ic a lly c o u n t e r m a n d in g it s\no w n n e g a t iv e e (cid:11) e c t s . (cid:15) T h e L S T M a lg o r it h m 's u p d a t e c o m p le x it y p e r w e ig h t a n d t im e s t e p is e s s e n t ia lly t h a t o f\nB P T T , n a m e ly O ( 1 ) . T h is is e x c e lle n t in c o m p a r is o n t o o t h e r a p p r o a c h e s s u c h a s R T R L ."
  },
  {
    "chunk_id": "doc_4_p23_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "U n lik e fu ll B P T T , h o w e v e r , L S T M is lo c a l in b o t h s p a c e a n d t im e . C O N C L U S I O N\na c h m e m o r y c e ll's in t e r n a l a r c h it e c t u r e g u a r a n t e e s c o n s t a n t e r r o r (cid:13) o w w it h in it s c o n s t a n t e r r o r\na r r o u s e l C E C , p r o v id e d t h a t t r u n c a t e d b a c k p r o p c u t s o (cid:11) e r r o r (cid:13) o w t r y in g t o le a k o u t o f m e m o r y\ne lls . T h is r e p r e s e n t s t h e b a s is fo r b r id g in g v e r y lo n g t im e la g s . T w o g a t e u n it s le a r n t o o p e n a n d\nlo s e a c c e s s t o e r r o r (cid:13) o w w it h in e a c h m e m o r y c e ll's C E C ."
  },
  {
    "chunk_id": "doc_4_p23_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "T h e m u lt ip lic a t iv e in p u t g a t e a (cid:11) o r d s\nr o t e c t io n o f t h e C E C fr o m p e r t u r b a t io n b y ir r e le v a n t in p u t s . L ik e w is e , t h e m u lt ip lic a t iv e o u t p u t\na t e p r o t e c t s o t h e r u n it s fr o m p e r t u r b a t io n b y c u r r e n t ly ir r e le v a n t m e m o r y c o n t e n t s . F u t u r e w o r k . T o (cid:12) n d o u t a b o u t L S T M 's p r a c t ic a l lim it a t io n s w e in t e n d t o a p p ly it t o r e a l\no r ld d a t a . A p p lic a t io n a r e a s w ill in c lu d e ( 1 ) t im e s e r ie s p r e d ic t io n , ( 2 ) m u s ic c o m p o s it io n , a n d\n3 ) s p e e c h p r o c e s s in g ."
  },
  {
    "chunk_id": "doc_4_p23_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "I t w ill a ls o b e in t e r e s t in g t o a u g m e n t s e q u e n c e c h u n k e r s ( S c h m id h u b e r\n9 9 2 b , 1 9 9 3 ) b y L S T M t o c o m b in e t h e a d v a n t a g e s o f b o t h . A C K N O W L E D G M E N T S\nh a n k s t o M ik e M o z e r , W ilfr ie d B r a u e r , N ic S c h r a u d o lp h , a n d s e v e r a l a n o n y m o u s r e fe r e e s fo r v a lu -\nb le c o m m e n t s a n d s u g g e s t io n s t h a t h e lp e d t o im p r o v e a p r e v io u s v e r s io n o f t h is p a p e r ( H o c h r e it e r\nn d S c h m id h u b e r 1 9 9 5 ) . T h is w o r k w a s s u p p o r t e d b y D F G g r a n t S C H M 9 4 2 / 3 - 1 fr o m \\ D e u t s c h e\no r s c h u n g s g e m e in s c h a ft \" . P P E N D I X\n."
  },
  {
    "chunk_id": "doc_4_p23_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "1 A L G O R I T H M D E T A I L S\nn w h a t fo llo w s , t h e in d e x k r a n g e s o v e r o u t p u t u n it s , i r a n g e s o v e r h id d e n u n it s , c s t a n d s fo r j\nv\nh e j - t h m e m o r y c e ll b lo c k , c d e n o t e s t h e v - t h u n it o f m e m o r y c e ll b lo c k c , u ; l ; m s t a n d fo r j j\nr b it r a r y u n it s , t r a n g e s o v e r a ll t im e s t e p s o f a g iv e n in p u t s e q u e n c e . 2 3"
  },
  {
    "chunk_id": "doc_4_p24_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 24,
    "chunk_type": "fixed",
    "text": "T\nT\nF\nc\nA\no\nb\nt\ng\n(cid:13)\nT h e g a t e u n it lo g is t ic s ig m o id ( w it h r a n g e [0 ; 1 ]) u s e d in t h e e x p e r im e n t s is\n1\nf ( x ) = . 1 + e x p ( (cid:0) x )\nh e fu n c t io n h ( w it h r a n g e [(cid:0) 1 ; 1 ]) u s e d in t h e e x p e r im e n t s is\n2\nh ( x ) = (cid:0) 1 . 1 + e x p ( (cid:0) x )\nh e fu n c t io n g ( w it h r a n g e [(cid:0) 2 ; 2 ]) u s e d in t h e e x p e r im e n t s is\n4\ng ( x ) = (cid:0) 2 . 1 + e x p ( (cid:0) x )\no r w a r d p a s s . T h e n e t in p u t a n d t h e a c t iv a t io n o f h id d e n u n it i a r e\nX\nu\nn e t ( t ) = w y ( t (cid:0) 1 ) i iu\nu\ni\ny ( t ) = f ( n e t ( t ) ) ."
  },
  {
    "chunk_id": "doc_4_p24_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 24,
    "chunk_type": "fixed",
    "text": "i i\nT h e n e t in p u t a n d t h e a c t iv a t io n o f i n a r e j\nX\nu\nn e t ( t ) = w y ( t (cid:0) 1 ) in in u j j\nu\nin j\ny ( t ) = f ( n e t ( t ) ) . in in j j\nT h e n e t in p u t a n d t h e a c t iv a t io n o f o u t a r e j\nX\nu\nn e t ( t ) = w y ( t (cid:0) 1 ) o u t o u t u j j\nu\no u tj\ny ( t ) = f ( n e t ( t ) ) . o u t o u t j j\nv c j v v T h e n e t in p u t n e t , t h e in t e r n a l s t a t e s , a n d t h e o u t p u t a c t iv a t io n y o f t h e c c j j\ne ll o f m e m o r y c e ll b lo c k c a r e : j\nX\nu\nv v n e t ( t ) = w y ( t (cid:0) 1 ) c c u j j\nu\n(cid:16) (cid:17)\nin j v v v s ( t ) = s ( t (cid:0) 1 ) + y ( t ) g n e t ( t ) c c c j j j\nv c o u tj j v ( t ) h ( s ( t ) ) ."
  },
  {
    "chunk_id": "doc_4_p24_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 24,
    "chunk_type": "fixed",
    "text": "y ( t ) = y c j\nT h e n e t in p u t a n d t h e a c t iv a t io n o f o u t p u t u n it k a r e\nX\nu\nn e t ( t ) = w y ( t (cid:0) 1 ) k k u\nu : u n o t a g a t e\nk\ny ( t ) = f ( n e t ( t ) ) . k k\nT h e b a c k w a r d p a s s t o b e d e s c r ib e d la t e r is b a s e d o n t h e fo llo w in g t r u n c a t e d b a c k p\np p r o x i m a t e d e r i v a t i v e s f o r t r u n c a t e d b a c k p r o p . T h e t r u n c a t e d v e r s io n ( s\nn ly a p p r o x im a t e s t h e p a r t ia l d e r iv a t iv e s , w h ic h is r e (cid:13) e c t e d b y t h e \\ (cid:25) \" s ig n s in tr\ne lo w . I t t r u n c a t e s e r r o r (cid:13) o w o n c e it le a v e s m e m o r y c e lls o r g a t e u n it s ."
  },
  {
    "chunk_id": "doc_4_p24_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 24,
    "chunk_type": "fixed",
    "text": "T r u n c a t io n\nh e r e a r e n o lo o p s a c r o s s w h ic h a n e r r o r t h a t le ft s o m e m e m o r y c e ll t h r o u g h it s in\na t e c a n r e e n t e r t h e c e ll t h r o u g h it s o u t p u t o r o u t p u t g a t e . T h is in t u r n e n s u r e s c\no w t h r o u g h t h e m e m o r y c e ll's C E C . 2 4\n( 3 )\n( 4 )\n( 5 )\n( 6 )\n( 7 )\n( 8 )\nv - t h m e m o r y\n( 9 )\nr o p fo r m u la e . e e S e c t io n 4 )\nt h e n o t a t io n\ne n s u r e s t h a t\np u t o r in p u t\no n s t a n t e r r o r"
  },
  {
    "chunk_id": "doc_4_p25_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 25,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_4_p25_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 25,
    "chunk_type": "fixed",
    "text": "I n t h e t r u n c a t e d b a c k p r o p v e r s io n , t h e fo llo w in g d e r iv a t iv e s a r e r e p la c e d b y z e r o :\n@ n e t ( t ) in j\n(cid:25) 0 8 u ; tr u @ y ( t (cid:0) 1 )\n@ n e t ( t ) o u tj\n(cid:25) 0 8 u ; tr u @ y ( t (cid:0) 1 )\na n d\n@ n e t ( t ) c j\n(cid:25) 0 8 u : tr u @ y ( t (cid:0) 1 )\nT h e r e fo r e w e g e t\nin j @ n e t ( t ) @ y ( t ) in j 0\n= f ( n e t (cid:25) 0 8 u ; ( t ) ) in tr in j j u u @ y ( t (cid:0) 1 ) @ y ( t (cid:0) 1 )\no u tj @ n e t ( t ) @ y ( t ) o u tj 0\n= f ( n e t (cid:25) 0 8 u ; ( t ) ) o u t tr o u t j j u u @ y ( t (cid:0) 1 ) @ y ( t (cid:0) 1 )\na n d\nc c c c j j j j @ n e t ( t ) @ n e t ( t ) @ n e t ( t ) ( t ) @ y ( t ) ( t ) ( t ) @ y @ y @ y o u t in c j j j\n= + + (cid:25) 0 tr u u u u @ y ( t (cid:0) 1 ) @ n e t ( t ) @ y ( t (cid:0) 1 ) @ n e t ( t ) @ y ( t (cid:0) 1 ) @ n e t ( t ) @ y ( t (cid:0) 1 ) o u t in c j j j\nv v\nT h is im p lie s fo r a ll w n o t o n c o n n e c t io n s t o c ; i n ; o u t ( t h a t is , l 62 f c ; i n ; o u t g ) : lm j j j j j j\nv v c c u X j j @ y @ y ( t (cid:0) 1 ) @ y ( t ) ( t )\n= (cid:25) 0 : tr u @ w @ y ( t (cid:0) 1 ) @ w lm lm\nu\nT h e t r u n c a t e d d e r iv a t iv e s o f o u t p u t u n it k a r e :\n!"
  },
  {
    "chunk_id": "doc_4_p25_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 25,
    "chunk_type": "fixed",
    "text": "k u X @ y ( t ) @ y ( t (cid:0) 1 ) 0 m\n= f + (cid:14) y ( t (cid:0) 1 ) (cid:25) ( n e t ( t ) ) w k l tr k k u k\n@ w @ w lm lm\nu : u n o t a g a t e\n0\nv v S S j j c c X X X X j j (cid:0) (cid:1) @ y ( t (cid:0) @ y ( t (cid:0) 1 ) 0 @ v v v f + (cid:14) + (cid:14) w ( n e t ( t ) ) (cid:14) w in l o u t l k c k c l k c j j k j j j @ w @ w lm lm\nv = 1 v = 1 j j\n!"
  },
  {
    "chunk_id": "doc_4_p25_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 25,
    "chunk_type": "fixed",
    "text": "i X @ y ( t (cid:0) 1 ) m\nw + (cid:14) y ( t (cid:0) 1 ) = k i k l\n@ w lm\ni: i h id d e n u n it\n8\nm\ny ( t (cid:0) 1 ) l = k >>\nv > c > j > @ y ( t(cid:0) 1 ) v < v l = c w k c j @ w j lm 0 v c , f ( n e t ( t ) ) k P k j S @ y ( t(cid:0) 1 ) j > v w l = i n O R l = o u t > k c j j v = 1 > @ w j lm > i P > @ y ( t(cid:0) 1 ) :\nw l o t h e r w is e k i i: i h id d e n u n it @ w lm\nw h e r e (cid:14) is t h e K r o n e c k e r d e lt a ( (cid:14) = 1 if a = b a n d 0 o t h e r w is e ) , a n d S is t h e s iz e o f m e a b j\nc e ll b lo c k c . T h e t r u n c a t e d d e r iv a t iv e s o f a h id d e n u n it i t h a t is n o t p a r t o f a m e m o r y c e ll j\ni\n@ n e t ( t ) @ y ( t ) i m 0 0\n( n e t ( t ) ) ( n e t ( t ) ) y ( t (cid:0) 1 ) ."
  },
  {
    "chunk_id": "doc_4_p25_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 25,
    "chunk_type": "fixed",
    "text": "= f (cid:25) (cid:14) f i i tr li i i\n@ w @ w lm lm\n( N o t e : h e r e it w o u ld b e p o s s ib le t o u s e t h e fu ll g r a d ie n t w it h o u t a (cid:11) e c t in g c o n s t a n t e r r o r\nt h r o u g h in t e r n a l s t a t e s o f m e m o r y c e lls .) 2 5\n8 u :\n( 1 0 )\n1 ) +\nm o r y\na r e :\n( 1 1 )\n(cid:13) o w"
  },
  {
    "chunk_id": "doc_4_p26_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 26,
    "chunk_type": "fixed",
    "text": "C e ll b lo c k c 's t r u n c a t e d d e r iv a t iv e s a r e : j\nin j @ n e t ( t ) @ y ( t ) in j 0 0 m\n= f ( n e t (cid:25) (cid:14) f ( n e t ( t ) ) ( t ) ) y ( t (cid:0) 1 ) . in tr in l in j j j in in j j\n@ w @ w lm lm\no u tj @ n e t ( t ) @ y ( t ) o u tj 0 0 m\n= f ( n e t (cid:25) (cid:14) f ( n e t ( t ) ) ( t ) ) y ( t (cid:0) 1 ) ."
  },
  {
    "chunk_id": "doc_4_p26_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 26,
    "chunk_type": "fixed",
    "text": "o u t tr o u t l o u t j j j o u t o u t j j\n@ w @ w lm lm\n(cid:16) (cid:17) (cid:16) (cid:17) in v v v j @ n e t ( @ s ( t ) @ s ( t (cid:0) 1 ) c c c @ y ( t ) j j j in 0 j v v n e t ( t ) = + g n e t ( t ) + y ( t ) g c c j j @ w @ w @ w @ w lm lm lm lm\n(cid:16) (cid:17) (cid:16) (cid:17) in v j @ s ( t (cid:0) 1 ) c @ y ( t ) j\nv v (cid:14) + (cid:14) + (cid:14) g n e t ( t ) + in l c l in l c j j j j @ w @ w lm lm\n(cid:16) (cid:17) v @ n e t ( t ) c j in 0 j v v n e t ( t ) (cid:14) y ( t ) g = c c l j j @ w lm\n(cid:16) (cid:17) (cid:16) (cid:17) v @ s ( t (cid:0) 1 ) c j 0 m\nv v (cid:14) + (cid:14) + (cid:14) f ( n e t ( t ) ) g n e t ( t ) y ( t in l c l in l in c j in j j j j j @ w lm\n(cid:16) (cid:17)\nm in 0 j v v n e t ( t ) y ( t (cid:0) 1 ) ."
  },
  {
    "chunk_id": "doc_4_p26_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 26,
    "chunk_type": "fixed",
    "text": "(cid:14) y ( t ) g c c l j j\nv\nc o u t v j @ s ( t ) j c @ y ( t ) @ y ( t ) j o u t 0 j v v = h ( s y ( t ) (cid:25) ( t ) ) + h ( s ( t ) ) c tr c j j @ w @ w @ w lm lm lm\n(cid:16) (cid:17) o u t v j @ s ( t ) c ( t ) @ y j o u t 0 j v v v (cid:14) h ( s y ( t ) ( t ) ) + (cid:14) + (cid:14) h ( s ( t ) ) o u t l c in l c l c j j j j j @ w @ w lm lm\nT o e (cid:14) c ie n t ly u p d a t e t h e s y s t e m a t t im e t , t h e o n ly ( t r u n c a t e d ) d e r iv a t iv e s t h a t n e e d\nv @ s ( t(cid:0) 1 ) c\nj v\na t t im e t (cid:0) 1 a r e , w h e r e l = c o r l = i n . j j @ w lm\nB a c k w a r d p a s s ."
  },
  {
    "chunk_id": "doc_4_p26_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 26,
    "chunk_type": "fixed",
    "text": "W e w ill d e s c r ib e t h e b a c k w a r d p a s s o n ly fo r t h e p a r t ic u la r ly e (cid:14) c ie n\ng r a d ie n t v e r s io n \" o f t h e L S T M a lg o r it h m . F o r s im p lic it y w e w ill u s e e q u a l s ig n s\na p p r o x im a t io n s a r e m a d e a c c o r d in g t o t h e t r u n c a t e d b a c k p r o p e q u a t io n s a b o v e . T h e s q u a r e d e r r o r a t t im e t is g iv e n b y\nX (cid:0) (cid:1) 2 k k\nE ( t ) = t ( t ) (cid:0) y ( t ) ,\nk : k o u t p u t u n it\nk\nw h e r e t ( t ) is o u t p u t u n it k 's t a r g e t a t t im e t . T im e t 's c o n t r ib u t io n t o w 's g r a d ie n t - b a s e d u p d a t e w it h le a r n in g r a t e (cid:11) is lm\n@ E ( t )\n(cid:1) w ( t ) = (cid:0) (cid:11) ."
  },
  {
    "chunk_id": "doc_4_p26_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 26,
    "chunk_type": "fixed",
    "text": "lm\n@ w lm\nW e d e (cid:12) n e s o m e u n it l 's e r r o r a t t im e s t e p t b y\n@ E ( t )\n. e ( t ) := (cid:0) l\n@ n e t ( t ) l\nU s in g ( a lm o s t ) s t a n d a r d b a c k p r o p , w e (cid:12) r s t c o m p u t e u p d a t e s fo r w e ig h t s t o o u t p u t u\nw e ig h t s t o h id d e n u n it s ( l = i ) a n d w e ig h t s t o o u t p u t g a t e s ( l = o u t ) . W e o b t a j\nfo r m u la e ( 1 0 ) , ( 1 1 ) , ( 1 3 ) ) :\n(cid:0) (cid:1)\n0 k k\n( n e t ( t ) ) t ( t ) (cid:0) y ( t ) , l = k ( o u t p u t ) : e ( t ) = f k k k\nX\n0\n( n e t ( t ) ) w e ( t ) , l = i ( h id d e n ) : e ( t ) = f i i k i k i\nk : k o u t p u t u n it\n2 6\nt ) (cid:25) tr\n(cid:0) 1 ) +\n."
  },
  {
    "chunk_id": "doc_4_p26_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 26,
    "chunk_type": "fixed",
    "text": "t o b e s t\nt \\ t r u n c\ne v e n w\nn it s ( l =\nin ( c o m\n(\n(\n(\n(\no\na\nh\n(\n(\n(\np\n(\n(\n1 2 )\n1 3 )\n1 4 )\n1 5 )\nr e d\nt e d\ne r e\n1 6 )\n1 7 )\n1 8 )\nk ) ,\na r e\n1 9 )\n2 0 )"
  },
  {
    "chunk_id": "doc_4_p27_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 27,
    "chunk_type": "fixed",
    "text": "l = o u t ( o u t p u t g a t e s ) : j\n0 1\nS j X X\n0 @ A v v e ( t ) = f ( n e t ( t ) ) h ( s ( t ) ) w e ( t ) . o u t o u t c k c k j j o u tj j j\nv = 1 k : k o u t p u t u n it\nF o r a ll p o s s ib le l t im e t 's c o n t r ib u t io n t o w 's u p d a t e is lm\nm\n(cid:1) w ( t ) = (cid:11) e ( t ) y ( t (cid:0) 1 ) . lm l\nv\nT h e r e m a in in g u p d a t e s fo r w e ig h t s t o in p u t g a t e s ( l = i n ) a n d t o c e ll u n it s ( l = c ) a r j j\nv c o n v e n t io n a l. W e d e (cid:12) n e s o m e in t e r n a l s t a t e s 's e r r o r : c j\n@ E ( t )\n= e := (cid:0) v s c v j @ s ( t ) c j\nX\n0\nv v f ( n e t ( t ) ) h ( s ( t ) ) w e ( t ) ."
  },
  {
    "chunk_id": "doc_4_p27_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 27,
    "chunk_type": "fixed",
    "text": "o u t o u t c k c k j j j j\nk : k o u t p u t u n it\nv\nW e o b t a in fo r l = i n o r l = c ; v = 1 ; : : : ; S j j j\nS j v X @ s ( t ) c @ E ( t ) j\n= e ( t ) . (cid:0) v s c\nj @ w @ w lm lm\nv = 1\nT h e d e r iv a t iv e s o f t h e in t e r n a l s t a t e s w it h r e s p e c t t o w e ig h t s a n d t h e c o r r e s p o n d in g w\nu p d a t e s a r e a s fo llo w s ( c o m p a r e e x p r e s s io n ( 1 4 ) ) :\nl = i n ( in p u t g a t e s ) : j\nv v @ s @ s ( t ) ( t (cid:0) 1 ) c c j j 0 m\nv = + g ( n e t ( t ) ) f ( n e t ( t ) ) y ( t (cid:0) 1 ) ; c in j in j j @ w @ w in m in m j j\nt h e r e fo r e t im e t 's c o n t r ib u t io n t o w 's u p d a t e is ( c o m p a r e e x p r e s s io n ( 1 0 ) ) : in m j\nS j v X @ s ( t ) c j\n."
  },
  {
    "chunk_id": "doc_4_p27_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 27,
    "chunk_type": "fixed",
    "text": "(cid:1) w ( t ) = (cid:11) e ( t ) v in m s j c\nj @ w in m j v = 1\nS im ila r ly w e g e t ( c o m p a r e e x p r e s s io n ( 1 4 ) ) :\nv\nl = c ( m e m o r y c e lls ) : j\nv v @ s @ s ( t ) ( t (cid:0) 1 ) c c j j 0 m\nv = + g ( n e t ( t ) ) f ( n e t ( t ) ) y ( t (cid:0) 1 ) ; c in in j j j v v @ w @ w c m c m j j\nv t h e r e fo r e t im e t 's c o n t r ib u t io n t o w 's u p d a t e is ( c o m p a r e e x p r e s s io n ( 1 0 ) ) : c m j\nv @ s ( t ) c j\nv . ( t ) (cid:1) w ( t ) = (cid:11) e v c m s c j v j @ w c m j\nA ll w e n e e d t o im p le m e n t fo r t h e b a c k w a r d p a s s a r e e q u a t io n s ( 1 9 ) , ( 2 0 ) , ( 2 1 ) , ( 2 2 ) , ( 2 3 ) ,\n( 2 6 ) , ( 2 7 ) , ( 2 8 ) ."
  },
  {
    "chunk_id": "doc_4_p27_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 27,
    "chunk_type": "fixed",
    "text": "E a c h w e ig h t 's t o t a l u p d a t e is t h e s u m o f t h e c o n t r ib u t io n s o f a ll t im e s t e p s\nC o m p u t a t i o n a l c o m p l e x i t y ."
  },
  {
    "chunk_id": "doc_4_p27_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 27,
    "chunk_type": "fixed",
    "text": "L S T M 's u p d a t e c o m p le x it y p e r t im e s t e p is\nO ( K H + K C S + H I + C S I ) = O ( W ) ;\nw h e r e K is t h e n u m b e r o f o u t p u t u n it s , C is t h e n u m b e r o f m e m o r y c e ll b lo c k s , S > 0 is t h\no f t h e m e m o r y c e ll b lo c k s , H is t h e n u m b e r o f h id d e n u n it s , I is t h e ( m a x im a l) n u m b e r o f\nfo r w a r d - c o n n e c t e d t o m e m o r y c e lls , g a t e u n it s a n d h id d e n u n it s , a n d\nW = K H + K C S + C S I + 2 C I + H I = O ( K H + K C S + C S I + H I )\n2 7\n( 2 1 )\n( 2 2 )\ne le s s\n( 2 3 )\n( 2 4 )\ne ig h t\n( 2 5 )\n( 2 6 )\n( 2 7 )\n( 2 8 )\n( 2 5 ) ,\n. ( 2 9 )\ne s iz e\nu n it s"
  },
  {
    "chunk_id": "doc_4_p28_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 28,
    "chunk_type": "fixed",
    "text": "is t h e n u m b e r o f w e ig h t s . E x p r e s s io n ( 2 9 ) is o b t a in e d b y c o n s id e r in g a ll c o m p u t a t io n s o f t h e\nb a c k w a r d p a s s : e q u a t io n ( 1 9 ) n e e d s K s t e p s ; ( 2 0 ) n e e d s K H s t e p s ; ( 2 1 ) n e e d s K S C s t e p s ; ( 2 2 )\nn e e d s K ( H + C ) s t e p s fo r o u t p u t u n it s , H I s t e p s fo r h id d e n u n it s , C I s t e p s fo r o u t p u t g a t e s ;\n( 2 3 ) n e e d s K C S s t e p s ; ( 2 5 ) n e e d s C S I s t e p s ; ( 2 6 ) n e e d s C S I s t e p s ; ( 2 7 ) n e e d s C S I s t e p s ;\n( 2 8 ) n e e d s C S I s t e p s . T h e t o t a l is K + 2 K H + K C + 2 K S C + H I + C I + 4 C S I s t e p s , o r\nO ( K H + K S C + H I + C S I ) s t e p s ."
  },
  {
    "chunk_id": "doc_4_p28_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 28,
    "chunk_type": "fixed",
    "text": "W e c o n c lu d e : L S T M a lg o r it h m 's u p d a t e c o m p le x it y p e r t im e\ns t e p is ju s t lik e B P T T 's fo r a fu lly r e c u r r e n t n e t . v @ s c\nj\nA t a g iv e n t im e s t e p , o n ly t h e 2 C S I m o s t r e c e n t v a lu e s fr o m e q u a t io n s ( 2 5 ) a n d ( 2 7 )\n@ w lm\nn e e d t o b e s t o r e d . H e n c e L S T M 's s t o r a g e c o m p le x it y a ls o is O ( W ) | it d o e s n o t d e p e n d o n t h e\nin p u t s e q u e n c e le n g t h . A . 2 E R R O R F L O W\nW e c o m p u t e h o w m u c h a n e r r o r s ig n a l is s c a le d w h ile (cid:13) o w in g b a c k t h r o u g h a m e m o r y c e ll fo r q t im e\ns t e p s ."
  },
  {
    "chunk_id": "doc_4_p28_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 28,
    "chunk_type": "fixed",
    "text": "A s a b y - p r o d u c t , t h is a n a ly s is r e c o n (cid:12) r m s t h a t t h e e r r o r (cid:13) o w w it h in a m e m o r y c e ll's C E C is\nin d e e d c o n s t a n t , p r o v id e d t h a t t r u n c a t e d b a c k p r o p c u t s o (cid:11) e r r o r (cid:13) o w t r y in g t o le a v e m e m o r y c e lls\n( s e e a ls o S e c t io n 3 .2 ) . T h e a n a ly s is a ls o h ig h lig h t s a p o t e n t ia l fo r u n d e s ir a b le lo n g - t e r m d r ift s o f\ns ( s e e ( 2 ) b e lo w ) , a s w e ll a s t h e b e n e (cid:12) c ia l, c o u n t e r m a n d in g in (cid:13) u e n c e o f n e g a t iv e ly b ia s e d in p u t c j\ng a t e s ( s e e ( 3 ) b e lo w ) ."
  },
  {
    "chunk_id": "doc_4_p28_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 28,
    "chunk_type": "fixed",
    "text": "U s in g t h e t r u n c a t e d b a c k p r o p le a r n in g r u le , w e o b t a in\n@ s ( t (cid:0) k ) c j\n= ( 3 0 )\n@ s ( t (cid:0) k (cid:0) 1 ) c j\nin j (cid:0) (cid:1) (cid:0) (cid:1) @ n e t ( t (cid:0) k ) ( t (cid:0) k ) @ y c j in 0 j\n1 + g n e t ( t (cid:0) k ) + y ( t (cid:0) k ) g = n e t ( t (cid:0) k ) c c j j\n@ s ( t (cid:0) k (cid:0) 1 ) @ s ( t (cid:0) k (cid:0) 1 ) c c j j\n(cid:21) (cid:20)\nu in j X (cid:0) (cid:1) ( t (cid:0) k ) @ y ( t (cid:0) k (cid:0) 1 ) @ y\ng n e t ( t (cid:0) k ) + 1 + c j u @ y ( t (cid:0) k (cid:0) 1 ) @ s ( t (cid:0) k (cid:0) 1 ) c j u\n(cid:21) (cid:20)\nu X (cid:0) (cid:1) @ n e t ( t (cid:0) k ) @ y ( t (cid:0) k (cid:0) 1 ) c j in 0 j\ny ( t (cid:0) k ) g (cid:25) 1 : n e t ( t (cid:0) k ) tr c j u @ y ( t (cid:0) k (cid:0) 1 ) @ s ( t (cid:0) k (cid:0) 1 ) c j u\nT h e (cid:25) s ig n in d ic a t e s e q u a lit y d u e t o t h e fa c t t h a t t r u n c a t e d b a c k p r o p r e p la c e s b y z e r o t h e tr\nin @ n e t ( t(cid:0) k ) j c @ y ( t(cid:0) k ) j\n8 u a n d 8 u ."
  },
  {
    "chunk_id": "doc_4_p28_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 28,
    "chunk_type": "fixed",
    "text": "fo llo w in g d e r iv a t iv e s : u u @ y ( t(cid:0) k (cid:0) 1 ) @ y ( t(cid:0) k (cid:0) 1 )\nI n w h a t fo llo w s , a n e r r o r # ( t ) s t a r t s (cid:13) o w in g b a c k a t c 's o u t p u t . W e r e d e (cid:12) n e j j\nX\n# ( t ) := w # ( t + 1 ) . ( 3 1 ) j ic i j\ni\nF o llo w in g t h e d e (cid:12) n it io n s / c o n v e n t io n s o f S e c t io n 3 .1 , w e c o m p u t e e r r o r (cid:13) o w fo r t h e t r u n c a t e d\nb a c k p r o p le a r n in g r u le . T h e e r r o r o c c u r r in g a t t h e o u t p u t g a t e is\no u t c j j @ y ( t ) @ y ( t )\n# ( t ) (cid:25) # ( t ) ."
  },
  {
    "chunk_id": "doc_4_p28_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 28,
    "chunk_type": "fixed",
    "text": "( 3 2 ) o u t tr j j o u tj @ n e t ( t ) @ y ( t ) o u tj\nT h e e r r o r o c c u r r in g a t t h e in t e r n a l s t a t e is\nc j @ s ( t + 1 ) @ y ( t ) c j\n# ( t ) = # ( t + 1 ) + # ( t ) . ( 3 3 ) s s j c c j j @ s ( t ) @ s ( t ) c c j j\nP\nS in c e w e u s e t r u n c a t e d b a c k p r o p w e h a v e # ( t ) = w # ( t + 1 ) ; j ic i j i, i n o g a t e a n d n o m e m o r y c e ll\nt h e r e fo r e w e g e t\nX @ # ( t + 1 ) @ # ( t ) j i\nw = (cid:25) 0 . ( 3 4 ) ic tr j\n@ # ( t + 1 ) @ # ( t + 1 ) s s c c j j i\n2 8"
  },
  {
    "chunk_id": "doc_4_p29_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 29,
    "chunk_type": "fixed",
    "text": "m\nT\nT\nc\nn\nW\nW\ne\nc\ne\no\nc\no\n(\nT h e p r e v io u s e q u a t io n s ( 3 3 ) a n d ( 3 4 ) im p ly c o n s t a n t e r r o r (cid:13) o w t h r o u g h in t e r n a l s t a t e s o f\ne m o r y c e lls :\n@ # ( t ) s @ s ( t + 1 ) c c j j\n= (cid:25) 1 . ( 3 5 ) tr\n@ # ( t + 1 ) @ s ( t ) s c c j j\nh e e r r o r o c c u r r in g a t t h e m e m o r y c e ll in p u t is\n@ g ( n e t @ s ( t ) ) ( t ) c c j j\n# # ( t ) . ( 3 6 ) ( t ) = c s j c j @ n e t @ g ( n e t ( t ) ( t ) ) c c j j\nh e e r r o r o c c u r r in g a t t h e in p u t g a t e is\nin j @ s ( t ) @ y ( t ) c j\n# ( t ) . ( 3 7 ) # ( t ) (cid:25) s in tr c j in j j @ n e t @ y ( t ) ) ( t ) in j\nN o e x t e r n a l e r r o r (cid:13) o w ."
  },
  {
    "chunk_id": "doc_4_p29_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 29,
    "chunk_type": "fixed",
    "text": "E r r o r s a r e p r o p a g a t e d b a c k fr o m u n it s l t o u n it v a lo n g o u t g o in g\no n n e c t io n s w it h w e ig h t s w . T h is \\ e x t e r n a l e r r o r \" ( n o t e t h a t fo r c o n v e n t io n a l u n it s t h e r e is lv\no t h in g b u t e x t e r n a l e r r o r ) a t t im e t is\nv X @ n e t ( t + 1 ) @ y ( t ) l e\n# ( t + 1 ) . ( 3 8 ) # ( t ) = l v v @ n e t ( t ) @ y ( t ) v\nl\ne o b t a in\ne\n@ # ( t (cid:0) 1 ) v\n= ( 3 9 )\n@ # ( t ) j\n(cid:18) (cid:19)\nv @ n e t ( t ) @ n e t ( t ) @ n e t ( t ) @ # ( t ) @ # ( t ) @ # ( t ) @ y ( t (cid:0) 1 ) o u t in c o u t in c j j j j j j\n+ + (cid:25) 0 ."
  },
  {
    "chunk_id": "doc_4_p29_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 29,
    "chunk_type": "fixed",
    "text": "tr v v v @ n e t ( t (cid:0) 1 ) @ # ( t ) @ y ( t (cid:0) 1 ) @ # ( t ) @ y ( t (cid:0) 1 ) @ # ( t ) @ y ( t (cid:0) 1 ) v j j j\ne o b s e r v e : t h e e r r o r # a r r iv in g a t t h e m e m o r y c e ll o u t p u t is n o t b a c k p r o p a g a t e d t o u n it s v v ia j\nx t e r n a l c o n n e c t io n s t o i n ; o u t ; c . j j j\nE r r o r (cid:13) o w w i t h i n m e m o r y c e l l s . W e n o w fo c u s o n t h e e r r o r b a c k (cid:13) o w w it h in a m e m o r y\ne ll's C E C . T h is is a c t u a lly t h e o n ly t y p e o f e r r o r (cid:13) o w t h a t c a n b r id g e s e v e r a l t im e s t e p s ."
  },
  {
    "chunk_id": "doc_4_p29_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 29,
    "chunk_type": "fixed",
    "text": "S u p p o s e\nr r o r # ( t ) a r r iv e s a t c 's o u t p u t a t t im e t a n d is p r o p a g a t e d b a c k fo r q s t e p s u n t il it r e a c h e s i n j j j\n@ # ( t(cid:0) q ) v\nr t h e m e m o r y c e ll in p u t g ( n e t ) . I t is s c a le d b y a fa c t o r o f , w h e r e v = i n ; c . W e (cid:12) r s t c j j j @ # ( t) j\no m p u t e\n8 c j @ y ( t)\n< q = 0 @ # ( t (cid:0) q ) s @ s ( t) c c j j\n(cid:25) . ( 4 0 ) ( t(cid:0) q + 1 ) tr @ # @ s ( t(cid:0) q + 1 ) s c c j j : @ # ( t ) j q > 0\n@ s ( t(cid:0) q ) @ # ( t) c j j\nE x p a n d in g e q u a t io n ( 4 0 ) , w e o b t a in\n@ # ( t (cid:0) q ) s @ # ( t (cid:0) q ) @ # ( t (cid:0) q ) c v v j\n(cid:25) (cid:25) ( 4 1 ) tr tr\n@ # ( t ) @ # ( t (cid:0) q ) @ # ( t ) j s j c j\n!"
  },
  {
    "chunk_id": "doc_4_p29_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 29,
    "chunk_type": "fixed",
    "text": "1 c Y j @ s ( t (cid:0) m + 1 ) @ # ( t (cid:0) q ) @ y ( t ) c v j\n(cid:25) tr\n@ # ( t (cid:0) q ) @ s ( t (cid:0) m ) @ s ( t ) s c c c j j j m = q\n(cid:26)\n0 in j g ( n e t ( t (cid:0) q ) y ( t (cid:0) q ) v = c c j o u t 0 j j\ny ( t ) h ( s ( t ) ) . c 0 j g ( n e t ( t (cid:0) q ) f ( n e t ( t (cid:0) q ) ) v = i n c in j j j in j\nC o n s id e r t h e fa c t o r s in t h e p r e v io u s e q u a t io n 's la s t e x p r e s s io n . O b v io u s ly , e r r o r (cid:13) o w is s c a le d\nn ly a t t im e s t ( w h e n it e n t e r s t h e c e ll) a n d t (cid:0) q ( w h e n it le a v e s t h e c e ll) , b u t n o t in b e t w e e n\nc o n s t a n t e r r o r (cid:13) o w t h r o u g h t h e C E C ) . W e o b s e r v e :\n2 9"
  },
  {
    "chunk_id": "doc_4_p30_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "o u tj\n( 1 ) T h e o u t p u t g a t e 's e (cid:11) e c t is : y ( t ) s c a le s d o w n t h o s e e r r o r s t h a t c a n b e r e d u c e d e a r ly\nd u r in g t r a in in g w it h o u t u s in g t h e m e m o r y c e ll. L ik e w is e , it s c a le s d o w n t h o s e e r r o r s r e s u lt in g\nfr o m u s in g ( a c t iv a t in g / d e a c t iv a t in g ) t h e m e m o r y c e ll a t la t e r t r a in in g s t a g e s | w it h o u t t h e o u t p u t\ng a t e , t h e m e m o r y c e ll m ig h t fo r in s t a n c e s u d d e n ly s t a r t c a u s in g a v o id a b le e r r o r s in s it u a t io n s t h a t\na lr e a d y s e e m e d u n d e r c o n t r o l ( b e c a u s e it w a s e a s y t o r e d u c e t h e c o r r e s p o n d in g e r r o r s w it h o u t\nm e m o r y c e lls ) ."
  },
  {
    "chunk_id": "doc_4_p30_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "S e e \\ o u t p u t w e ig h t c o n (cid:13) ic t \" a n d \\ a b u s e p r o b le m \" in S e c t io n s 3 / 4 . ( 2 ) I f t h e r e a r e la r g e p o s it iv e o r n e g a t iv e s ( t ) v a lu e s ( b e c a u s e s h a s d r ift e d s in c e t im e s t e p c c j j\n0\nt (cid:0) q ) , t h e n h ( s ( t ) ) m a y b e s m a ll ( a s s u m in g t h a t h is a lo g is t ic s ig m o id ) . S e e S e c t io n 4 . D r ift s c j\no f t h e m e m o r y c e ll's in t e r n a l s t a t e s c a n b e c o u n t e r m a n d e d b y n e g a t iv e ly b ia s in g t h e in p u t g a t e c j\ni n ( s e e S e c t io n 4 a n d n e x t p o in t ) . R e c a ll fr o m S e c t io n 4 t h a t t h e p r e c is e b ia s v a lu e d o e s n o t j\nm a t t e r m u c h ."
  },
  {
    "chunk_id": "doc_4_p30_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "in 0 j ( 3 ) y ( t (cid:0) q ) a n d f ( n e t ( t (cid:0) q ) ) a r e s m a ll if t h e in p u t g a t e is n e g a t iv e ly b ia s e d ( a s s u m e in j in j\nf is a lo g is t ic s ig m o id ) . H o w e v e r , t h e p o t e n t ia l s ig n i(cid:12) c a n c e o f t h is is n e g lig ib le c o m p a r e d t o t h e in j\np o t e n t ia l s ig n i(cid:12) c a n c e o f d r ift s o f t h e in t e r n a l s t a t e s . c j\nS o m e o f t h e fa c t o r s a b o v e m a y s c a le d o w n L S T M 's o v e r a ll e r r o r (cid:13) o w , b u t n o t in a m a n n e r\nt h a t d e p e n d s o n t h e le n g t h o f t h e t im e la g ."
  },
  {
    "chunk_id": "doc_4_p30_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "T h e (cid:13) o w w ill s t ill b e m u c h m o r e e (cid:11) e c t iv e t h a n a n\ne x p o n e n t ia lly ( o f o r d e r q ) d e c a y in g (cid:13) o w w it h o u t m e m o r y c e lls . R e f e r e n c e s\nA lm e id a , L . B . ( 1 9 8 7 ) . A le a r n in g r u le fo r a s y n c h r o n o u s p e r c e p t r o n s w it h fe e d b a c k in a c o m b in a -\nt o r ia l e n v ir o n m e n t . I n I E E E 1 s t I n t e r n a t io n a l C o n fe r e n c e o n N e u r a l N e t w o r k s , S a n D ie g o ,\nv o lu m e 2 , p a g e s 6 0 9 { 6 1 8 . B a ld i, P . a n d P in e d a , F . ( 1 9 9 1 ) . C o n t r a s t iv e le a r n in g a n d n e u r a l o s c illa t o r . N e u r a l C o m p u t a t io n ,\n3 :5 2 6 { 5 4 5 . B e n g io , Y . a n d F r a s c o n i, P . ( 1 9 9 4 ) ."
  },
  {
    "chunk_id": "doc_4_p30_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "C r e d it a s s ig n m e n t t h r o u g h t im e : A lt e r n a t iv e s t o b a c k p r o p a g a -\nt io n . I n C o w a n , J . D ., T e s a u r o , G ., a n d A ls p e c t o r , J ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n\nP r o c e s s in g S y s t e m s 6 , p a g e s 7 5 { 8 2 . S a n M a t e o , C A : M o r g a n K a u fm a n n . B e n g io , Y ., S im a r d , P ., a n d F r a s c o n i, P . ( 1 9 9 4 ) . L e a r n in g lo n g - t e r m d e p e n d e n c ie s w it h g r a d ie n t\nd e s c e n t is d i(cid:14) c u lt . I E E E T r a n s a c t io n s o n N e u r a l N e t w o r k s , 5 ( 2 ) :1 5 7 { 1 6 6 . C le e r e m a n s , A ., S e r v a n - S c h r e ib e r , D ., a n d M c C le lla n d , J . L . ( 1 9 8 9 ) ."
  },
  {
    "chunk_id": "doc_4_p30_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "F in it e - s t a t e a u t o m a t a a n d\ns im p le r e c u r r e n t n e t w o r k s . N e u r a l C o m p u t a t io n , 1 :3 7 2 { 3 8 1 . d e V r ie s , B . a n d P r in c ip e , J . C . ( 1 9 9 1 ) . A t h e o r y fo r n e u r a l n e t w o r k s w it h t im e d e la y s . I n\nL ip p m a n n , R . P ., M o o d y , J . E ., a n d T o u r e t z k y , D . S ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n\nP r o c e s s in g S y s t e m s 3 , p a g e s 1 6 2 { 1 6 8 . S a n M a t e o , C A : M o r g a n K a u fm a n n . D o y a , K . ( 1 9 9 2 ) . B ifu r c a t io n s in t h e le a r n in g o f r e c u r r e n t n e u r a l n e t w o r k s ."
  },
  {
    "chunk_id": "doc_4_p30_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "I n P r o c e e d in g s o f 1 9 9 2\nI E E E I n t e r n a t io n a l S y m p o s iu m o n C ir c u it s a n d S y s t e m s , p a g e s 2 7 7 7 { 2 7 8 0 . D o y a , K . a n d Y o s h iz a w a , S . ( 1 9 8 9 ) . A d a p t iv e n e u r a l o s c illa t o r u s in g c o n t in u o u s - t im e b a c k -\np r o p a g a t io n le a r n in g . N e u r a l N e t w o r k s , 2 :3 7 5 { 3 8 5 . E lm a n , J . L . ( 1 9 8 8 ) . F in d in g s t r u c t u r e in t im e . T e c h n ic a l R e p o r t C R L T e c h n ic a l R e p o r t 8 8 0 1 ,\nC e n t e r fo r R e s e a r c h in L a n g u a g e , U n iv e r s it y o f C a lifo r n ia , S a n D ie g o . F a h lm a n , S . E . ( 1 9 9 1 ) . T h e r e c u r r e n t c a s c a d e - c o r r e la t io n le a r n in g a lg o r it h m . I n L ip p m a n n , R ."
  },
  {
    "chunk_id": "doc_4_p30_fixed_7",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "P .,\nM o o d y , J . E ., a n d T o u r e t z k y , D . S ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n P r o c e s s in g\nS y s t e m s 3 , p a g e s 1 9 0 { 1 9 6 . S a n M a t e o , C A : M o r g a n K a u fm a n n . H o c h r e it e r , J . ( 1 9 9 1 ) . U n t e r s u c h u n g e n z u d y n a m is c h e n n e u r o n a le n N e t z e n . D ip lo m a t h e -\ns is , I n s t it u t f (cid:127)u r I n fo r m a t ik , L e h r s t u h l P r o f. B r a u e r , T e c h n is c h e U n iv e r s it (cid:127)a t M (cid:127)u n c h e n . S e e\nw w w 7 .in fo r m a t ik .t u - m u e n c h e n .d e / ~ h o c h r e it . 3 0"
  },
  {
    "chunk_id": "doc_4_p31_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "H\nH\nH\nL\nM\nM\nM\nP\nP\nP\nP\nP\nP\nP\nR\nR\nS\no c h r e it e r , S . a n d S c h m id h u b e r , J . ( 1 9 9 5 ) . L o n g s h o r t - t e r m m e m o r y . T e c h n ic a l R e p o r t F K I - 2 0 7 -\n9 5 , F a k u lt (cid:127)a t f (cid:127)u r I n fo r m a t ik , T e c h n is c h e U n iv e r s it (cid:127)a t M (cid:127)u n c h e n . o c h r e it e r , S . a n d S c h m id h u b e r , J . ( 1 9 9 6 ) . B r id g in g lo n g t im e la g s b y w e ig h t g u e s s in g a n d \\ L o n g\nS h o r t - T e r m M e m o r y \" . I n S ilv a , F . L ., P r in c ip e , J . C ., a n d A lm e id a , L . B ., e d it o r s , S p a -\nt io t e m p o r a l m o d e ls in b io lo g ic a l a n d a r t i(cid:12) c ia l s y s t e m s , p a g e s 6 5 { 7 2 . I O S P r e s s , A m s t e r d a m ,\nN e t h e r la n d s ."
  },
  {
    "chunk_id": "doc_4_p31_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "S e r ie : F r o n t ie r s in A r t i(cid:12) c ia l I n t e llig e n c e a n d A p p lic a t io n s , V o lu m e 3 7 . o c h r e it e r , S . a n d S c h m id h u b e r , J . ( 1 9 9 7 ) . L S T M c a n s o lv e h a r d lo n g t im e la g p r o b le m s . I n\nA d v a n c e s in N e u r a l I n fo r m a t io n P r o c e s s in g S y s t e m s 9 . M I T P r e s s , C a m b r id g e M A . P r e s e n t e d\na t N I P S 9 6 . a n g , K ., W a ib e l, A ., a n d H in t o n , G . E . ( 1 9 9 0 ) . A t im e - d e la y n e u r a l n e t w o r k a r c h it e c t u r e fo r\nis o la t e d w o r d r e c o g n it io n . N e u r a l N e t w o r k s , 3 :2 3 { 4 3 . ille r , C . B . a n d G ile s , C . L . ( 1 9 9 3 ) ."
  },
  {
    "chunk_id": "doc_4_p31_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "E x p e r im e n t a l c o m p a r is o n o f t h e e (cid:11) e c t o f o r d e r in r e c u r r e n t\nn e u r a l n e t w o r k s . I n t e r n a t io n a l J o u r n a l o f P a t t e r n R e c o g n it io n a n d A r t i(cid:12) c ia l I n t e llig e n c e ,\n7 ( 4 ) :8 4 9 { 8 7 2 . o z e r , M . C . ( 1 9 8 9 ) . A fo c u s e d b a c k - p r o p a g a t io n a lg o r it h m fo r t e m p o r a l s e q u e n c e r e c o g n it io n . C o m p le x S y s t e m s , 3 :3 4 9 { 3 8 1 . o z e r , M . C . ( 1 9 9 2 ) . I n d u c t io n o f m u lt is c a le t e m p o r a l s t r u c t u r e . I n L ip p m a n , D . S ., M o o d y ,\nJ . E ., a n d T o u r e t z k y , D . S ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n P r o c e s s in g S y s t e m s 4 ,\np a g e s 2 7 5 { 2 8 2 ."
  },
  {
    "chunk_id": "doc_4_p31_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "S a n M a t e o , C A : M o r g a n K a u fm a n n . e a r lm u t t e r , B . A . ( 1 9 8 9 ) . L e a r n in g s t a t e s p a c e t r a je c t o r ie s in r e c u r r e n t n e u r a l n e t w o r k s . N e u r a l\nC o m p u t a t io n , 1 ( 2 ) :2 6 3 { 2 6 9 . e a r lm u t t e r , B . A . ( 1 9 9 5 ) . G r a d ie n t c a lc u la t io n s fo r d y n a m ic r e c u r r e n t n e u r a l n e t w o r k s : A s u r v e y . I E E E T r a n s a c t io n s o n N e u r a l N e t w o r k s , 6 ( 5 ) :1 2 1 2 { 1 2 2 8 . in e d a , F . J . ( 1 9 8 7 ) . G e n e r a liz a t io n o f b a c k - p r o p a g a t io n t o r e c u r r e n t n e u r a l n e t w o r k s . P h y s ic a l\nR e v ie w L e t t e r s , 1 9 ( 5 9 ) :2 2 2 9 { 2 2 3 2 . in e d a , F . J . ( 1 9 8 8 ) ."
  },
  {
    "chunk_id": "doc_4_p31_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "D y n a m ic s a n d a r c h it e c t u r e fo r n e u r a l c o m p u t a t io n . J o u r n a l o f C o m p le x it y ,\n4 :2 1 6 { 2 4 5 . la t e , T . A . ( 1 9 9 3 ) . H o lo g r a p h ic r e c u r r e n t n e t w o r k s . I n S . J . H a n s o n , J . D . C . a n d G ile s , C . L .,\ne d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n P r o c e s s in g S y s t e m s 5 , p a g e s 3 4 { 4 1 . S a n M a t e o , C A :\nM o r g a n K a u fm a n n . o lla c k , J . B . ( 1 9 9 1 ) . L a n g u a g e in d u c t io n b y p h a s e t r a n s it io n in d y n a m ic a l r e c o g n iz e r s . I n\nL ip p m a n n , R . P ., M o o d y , J . E ., a n d T o u r e t z k y , D ."
  },
  {
    "chunk_id": "doc_4_p31_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "S ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n\nP r o c e s s in g S y s t e m s 3 , p a g e s 6 1 9 { 6 2 6 . S a n M a t e o , C A : M o r g a n K a u fm a n n . u s k o r iu s , G . V . a n d F e ld k a m p , L . A . ( 1 9 9 4 ) . N e u r o c o n t r o l o f n o n lin e a r d y n a m ic a l s y s t e m s w it h\nK a lm a n (cid:12) lt e r t r a in e d r e c u r r e n t n e t w o r k s . I E E E T r a n s a c t io n s o n N e u r a l N e t w o r k s , 5 ( 2 ) :2 7 9 {\n2 9 7 . in g , M . B . ( 1 9 9 3 ) . L e a r n in g s e q u e n t ia l t a s k s b y in c r e m e n t a lly a d d in g h ig h e r o r d e r s . I n S . J . H a n -\ns o n , J . D . C . a n d G ile s , C ."
  },
  {
    "chunk_id": "doc_4_p31_fixed_6",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "L ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n P r o c e s s in g S y s t e m s\n5 , p a g e s 1 1 5 { 1 2 2 . M o r g a n K a u fm a n n . o b in s o n , A . J . a n d F a lls id e , F . ( 1 9 8 7 ) . T h e u t ilit y d r iv e n d y n a m ic e r r o r p r o p a g a t io n n e t w o r k . T e c h n ic a l R e p o r t C U E D / F - I N F E N G / T R .1 , C a m b r id g e U n iv e r s it y E n g in e e r in g D e p a r t m e n t . c h m id h u b e r , J . ( 1 9 8 9 ) . T h e N e u r a l B u c k e t B r ig a d e : A lo c a l le a r n in g a lg o r it h m fo r d y n a m ic\nfe e d fo r w a r d a n d r e c u r r e n t n e t w o r k s . C o n n e c t io n S c ie n c e , 1 ( 4 ) :4 0 3 { 4 1 2 . 3 1"
  },
  {
    "chunk_id": "doc_4_p32_fixed_0",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 32,
    "chunk_type": "fixed",
    "text": "S\nS\nS\nS\nS\nS\nS\nS\nW\nW\nW\nW\nW\n3\nc h m id h u b e r , J . ( 1 9 9 2 a ) . A (cid:12) x e d s iz e s t o r a g e O ( n ) t im e c o m p le x it y le a r n in g a lg o r it h m fo r fu lly\nr e c u r r e n t c o n t in u a lly r u n n in g n e t w o r k s . N e u r a l C o m p u t a t io n , 4 ( 2 ) :2 4 3 { 2 4 8 . c h m id h u b e r , J . ( 1 9 9 2 b ) . L e a r n in g c o m p le x , e x t e n d e d s e q u e n c e s u s in g t h e p r in c ip le o f h is t o r y\nc o m p r e s s io n . N e u r a l C o m p u t a t io n , 4 ( 2 ) :2 3 4 { 2 4 2 . c h m id h u b e r , J . ( 1 9 9 2 c ) . L e a r n in g u n a m b ig u o u s r e d u c e d s e q u e n c e d e s c r ip t io n s . I n M o o d y , J . E .,\nH a n s o n , S . J ., a n d L ip p m a n , R ."
  },
  {
    "chunk_id": "doc_4_p32_fixed_1",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 32,
    "chunk_type": "fixed",
    "text": "P ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n P r o c e s s in g S y s -\nt e m s 4 , p a g e s 2 9 1 { 2 9 8 . S a n M a t e o , C A : M o r g a n K a u fm a n n . c h m id h u b e r , J . ( 1 9 9 3 ) . N e t z w e r k a r c h it e k t u r e n , Z ie lfu n k t io n e n u n d K e t t e n r e g e l. H a b ilit a t io n s -\ns c h r ift , I n s t it u t f (cid:127)u r I n fo r m a t ik , T e c h n is c h e U n iv e r s it (cid:127)a t M (cid:127)u n c h e n . c h m id h u b e r , J . a n d H o c h r e it e r , S . ( 1 9 9 6 ) . G u e s s in g c a n o u t p e r fo r m m a n y lo n g t im e la g a lg o -\nr it h m s . T e c h n ic a l R e p o r t I D S I A - 1 9 - 9 6 , I D S I A . ilv a , G . X ., A m a r a l, J . D ., L a n g lo is , T ., a n d A lm e id a , L . B . ( 1 9 9 6 ) ."
  },
  {
    "chunk_id": "doc_4_p32_fixed_2",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 32,
    "chunk_type": "fixed",
    "text": "F a s t e r t r a in in g o f r e c u r r e n t\nn e t w o r k s . I n S ilv a , F . L ., P r in c ip e , J . C ., a n d A lm e id a , L . B ., e d it o r s , S p a t io t e m p o r a l m o d e ls\nin b io lo g ic a l a n d a r t i(cid:12) c ia l s y s t e m s , p a g e s 1 6 8 { 1 7 5 . I O S P r e s s , A m s t e r d a m , N e t h e r la n d s . S e r ie :\nF r o n t ie r s in A r t i(cid:12) c ia l I n t e llig e n c e a n d A p p lic a t io n s , V o lu m e 3 7 . m it h , A . W . a n d Z ip s e r , D . ( 1 9 8 9 ) . L e a r n in g s e q u e n t ia l s t r u c t u r e s w it h t h e r e a l- t im e r e c u r r e n t\nle a r n in g a lg o r it h m . I n t e r n a t io n a l J o u r n a l o f N e u r a l S y s t e m s , 1 ( 2 ) :1 2 5 { 1 3 1 . u n , G ., C h e n , H ., a n d L e e , Y . ( 1 9 9 3 ) ."
  },
  {
    "chunk_id": "doc_4_p32_fixed_3",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 32,
    "chunk_type": "fixed",
    "text": "T im e w a r p in g in v a r ia n t n e u r a l n e t w o r k s . I n S . J . H a n s o n ,\nJ . D . C . a n d G ile s , C . L ., e d it o r s , A d v a n c e s in N e u r a l I n fo r m a t io n P r o c e s s in g S y s t e m s 5 ,\np a g e s 1 8 0 { 1 8 7 . S a n M a t e o , C A : M o r g a n K a u fm a n n . a t r o u s , R . L . a n d K u h n , G . M . ( 1 9 9 2 ) . I n d u c t io n o f (cid:12) n it e - s t a t e la n g u a g e s u s in g s e c o n d - o r d e r\nr e c u r r e n t n e t w o r k s . N e u r a l C o m p u t a t io n , 4 :4 0 6 { 4 1 4 . e r b o s , P . J . ( 1 9 8 8 ) . G e n e r a liz a t io n o f b a c k p r o p a g a t io n w it h a p p lic a t io n t o a r e c u r r e n t g a s m a r k e t\nm o d e l. N e u r a l N e t w o r k s , 1 . illia m s , R . J . ( 1 9 8 9 ) ."
  },
  {
    "chunk_id": "doc_4_p32_fixed_4",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 32,
    "chunk_type": "fixed",
    "text": "C o m p le x it y o f e x a c t g r a d ie n t c o m p u t a t io n a lg o r it h m s fo r r e c u r r e n t n e u r a l\nn e t w o r k s . T e c h n ic a l R e p o r t T e c h n ic a l R e p o r t N U - C C S - 8 9 - 2 7 , B o s t o n : N o r t h e a s t e r n U n iv e r -\ns it y , C o lle g e o f C o m p u t e r S c ie n c e . illia m s , R . J . a n d P e n g , J . ( 1 9 9 0 ) . A n e (cid:14) c ie n t g r a d ie n t - b a s e d a lg o r it h m fo r o n - lin e t r a in in g o f\nr e c u r r e n t n e t w o r k t r a je c t o r ie s . N e u r a l C o m p u t a t io n , 4 :4 9 1 { 5 0 1 . illia m s , R . J . a n d Z ip s e r , D . ( 1 9 9 2 ) . G r a d ie n t - b a s e d le a r n in g a lg o r it h m s fo r r e c u r r e n t n e t w o r k s\na n d t h e ir c o m p u t a t io n a l c o m p le x it y ."
  },
  {
    "chunk_id": "doc_4_p32_fixed_5",
    "doc_id": "doc_4",
    "pdf_name": "4.pdf",
    "page": 32,
    "chunk_type": "fixed",
    "text": "I n B a c k - p r o p a g a t io n : T h e o r y , A r c h it e c t u r e s a n d A p p li-\nc a t io n s . H ills d a le , N J : E r lb a u m . 3 2"
  },
  {
    "chunk_id": "doc_5_p1_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Generative Adversarial Networks (GAN)\nA Gentle Introduction\nSuWang\nDepartmentofStatisticsandDataScience\nUniversityofTexasatAustin\nAbstract\nInthistutorial,IpresentanintuitiveintroductiontotheGenerativeAdversarial\nNetwork(GAN)[1],inventedbyIanGoodfellowofGoogleBrain,overviewthe\ngeneral idea of the model, and describe the algorithm for training it as per the\noriginal work. I further briefly introduce the application of GAN in Natural\nLanguageProcessingtoshowitsflexibilityandstrongpotentialasaneuralnetwork\narchitecture. Inlieuofthediscussion,IalsopresentsimpleTensorflowcode1\nfortheoriginalGAN[1]andanimportantvariant—WassersteinGAN [26,27],to\nhelpthereadergettingaquickstartinpracticalapplications. 1 Overview\nGenerativeAdversarialNetworks(GAN)[1]isadeeplearningframeworkinwhichtwomodels,a\ngenerativemodelGandadiscriminativemodelD,aretrainedsimultaneously. TheobjectiveofG\nistocapturethedistributionofsometargetdata(e.g. distributionsofpixelintensityinimages). D\naidsthetrainingofGbyexaminingthedatageneratedbyGinreferenceto“real”data,andthereby\nhelpingGlearnthedistributionthatunderpinstherealdata. GANisfleshedoutinGoodfellowetal. (2014)[1]asapairofsimpleneuralnetworks."
  },
  {
    "chunk_id": "doc_5_p1_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Howeverinpractice,themodelscaninprinciplebe\nanygenerative-discriminativepairs2\nIntheoriginalwork[1,2]andelsewhere[3],GANhasbeengiventheanalogyasaprocessofmaking\ncounterfeitmoney: Gplaystheroleofacounterfeiterin-training,whileDthebankstrivestoidentify\nfakebillsandintheprocess(hopefullyunintentionally)helpsGhoningitsbill-makingskills. More\nconcretely,letx ∼ p becharacterizingfeaturesforrealbills,andG(z)befeaturesGcreates\ndata\nfromsomenoisedistributionz ∼p . FurtherletJ besomequantitativemetricwhichmeasuresthe\nz\nextenttowhichabillisreal. Then,D’sjobistolowerJ(G(z))(thescoreoffakebill)whileincrease\nthescoreJ(x)(thescoreofrealbill)formoresuccessfulidentification. G,ontheotherhand,aims\natincreasingJ(G(z))(i.e. improvingthequalityoffakebill)bylearningfrom“observing”howD\nmakedifferentiations. Asthegameof“bustingfakebill”and“makingbetterfakebill”proceeds,the\nmodeldistributionp drawsclosertop ,andeventuallyreachesanequilibrium[2]whereDcan\nG data\nnolongerclassfiybetterthanchance(i.e. D(x)=D(G(z))= 1). NowwesayGhasarrivedatan\n2\noptimalpoint3incounterfeiting."
  },
  {
    "chunk_id": "doc_5_p1_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "GANhasgainedmassiveattractionincomputervision[4,5,6],featurerepresentation[7],andmore\nrecentlyinNaturalLanguageProcessing(NLP)tasks:DocumentModeling[8],DialogueGeneration\n[9], Sentiment Analysis [10], and Domain Adaptation [11]. In Section 5 I briefly exemplify the\nsuccessfulapplicationofGANinNLP[8,9]. 1https://github.com/suwangcompling/GAN-tutorials. 2Thesecanbeanygenerativeanddiscriminativemodels,inamuchwidersensethantheterm“G-Dpair”is\nusedintheliterature[12]. 3Iwillshowthattheoptimalpointisreachediffp =p (Section2)\nG data\nTutorialonGANinLIN395C:ResearchinComputationalLinguistics,UniversityofTexasatAustin."
  },
  {
    "chunk_id": "doc_5_p2_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "2 Model\nFormaldescription. Tobegin,IdescribeabarebonevariantofGANinitsoriginalformulation,in\nwhichbothDandGaresimpleMulti-LayerPerceptrons(MLP).Letp beourtargetdistribution\ndata\nwhichthegeneratorGwilllearnbyapproximiatingitwiththemodeldistributionp . Gisassociated\nG\nwithanoisepriorp ,fromwhichGdrawssamplez,andcreatefakedatum4 G(z;θ ),whereθ\nz G G\naremodelparameters. ThediscriminatorD(x;θ )takesxorG(z)asinput,andreturnsabinary\nD\njudgmentastowhethertheinputisfromp orp . data G\nDevaluatesthequalityofinputxwiththefollowingmetric:\nJ(D,G)=E [logD(x)]+E [log(1−D(G(z)))] (1)\nx∼pdata z∼pz\nThefirsttermof(1)evaluatestheexpectationofthelogprobabilitythatxisdrawnfromrealdata;the\nsecondterm,thatxisdrawnfromfakedata(i.e. noise). D’sobjectiveistomaximizeJ,i.e. pushing\nthebothtermstowards0(i.e. D(x)→1,D(G(z))→0). G,ontheotherhand,wantstominimize\nJ bylowerthesecondterm5 (i.e. D(G(z))→1)."
  },
  {
    "chunk_id": "doc_5_p2_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "The“conflictingobjectives”ofthetwomodels\nresultsinaMinmaxGame[1]whichisdescribedasfollows:\nminmaxJ(D,G) (2)\nG D\nAlgorithmically, the training takes the form of an alternation process between minimization and\nmaximization,whichisdescribedinAlgorithm1. Inpractice,Eq. 1oftendoesnotbringthemodelto\nequilibrium,thisisbecauselog(1−D(G(z)))rapidlysaturatesintheearlystageoftraining,where\nDeasilyrejectsG(z)becauseGgeneratesfakedataofpoorquality,suchthattheyconspicuously\ndifferfromtherealdata. Therefore,ratherthanevaluatinghowbadfakedataare(G’sobjectivein\nthesecondtermofEq. 1),weinsteadevaluatehowgoodtheyarebysettingG’sgoaltomaximizing\nJ (G)=logD(G(z)). Insodoingweendupwithtwoobjectivefunctions:\nG\nmaxJ (D,G)\nD\nD\n(3)\nmaxJ (G)\nG\nG\nToillustratethetrainingprocessgraphically,weobserve(a)howp changesovertime,andconse-\nG\nquently(b)howthediscriminationboundaryofDchangesaccordingly(Figure1,Figure1in[1])."
  },
  {
    "chunk_id": "doc_5_p2_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Figure1: TrainingProcessofGAN(greensolid: p ;blackdotted: p ;bluedash: D’sdiscrimina-\nG data\ntionboundary;arrows: generationoffakedata)\nIn(a)through(d),zaresampleduniformlyfromthenoisepriorp ,andp drawsclosertop . In\nz G data\ntheprocess,thedescriminationboundarychangesaccordingly,andfinallymorphsintoaflatline(i.e. D(x)=D(G(z))= 1)whichindicatesDisnowunabletotellfakeandrealdataapart. Specifically,\n2\nFigure 1 shows a scenario where the model is near convergence: (a) p is similar to p , and\nG data\nDisnowpartiallyaccurate6;(b)Disupdated: basedontherelativedistributionofp andp ,\nG data\nD convergesintheinnerloopofAlgorithm1toD∗(x) = pdata(x) ; (c)Gisupdated: p\npdata(x)+pG(x) G\n4Thegeneratorisessentiallyafunctionthatmapsanoisedatumintothespaceofarealdatum,i.e.G:z(cid:55)→x. Notethatzandxthereforedonothavetobeequalindimensionality. 5ThefirsttermisindependentofG."
  },
  {
    "chunk_id": "doc_5_p2_fixed_3",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "6Inthelatestageoftraining,Gstartstogeneratehighqualityfakes,totheeffectthatD’sclassification\nperformancesuffers(butnotentirelydowntothelevelofrandomness,i.e.D(x)=D(G(z))= 1). 2\n2"
  },
  {
    "chunk_id": "doc_5_p3_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Algorithm1MinmaxGame\n1: forspecified#oftrainingiterationsdo\n(cid:46)TRAININGDISCRIMINATOR\n2: forspecifiedksteps*do\n3: Drawminibatchofmnoisesamples{z(1),...,z(m)}∼p z . 4: Drawminibatchofmdatasamples{x(1),...,x(m)}∼p data . 5: UpdateD’sparametersbygradientascent:\nm\n1 (cid:88)(cid:104) (cid:16) (cid:17)(cid:105)\n∇ logD(x(i))+log 1−D(G(z(i)))\nθdm\ni=1\n6: endfor\n(cid:46)TRAININGGENERATOR\n7: Drawminibatchofmnoisesamples{z(1),...,z(m)}∼p z . 8: UpdateG’sparametersbygradientdescent:\nm\n1 (cid:88) (cid:16) (cid:17)\n∇ log 1−D(G(z(i)))\nθgm\ni=1\n9: endfor\n*kisatunablehyperparameterwhichisusuallysetto1tolowerthetrainingcostofeachiteration. isdrawnclosertop undertheguidanceofthegradientofD;(d)finalconvergence: assuming\ndata\nsufficientmodelcapacity,theadversarialpairreachtheequilibriump =p ,whereD(x)= 1. G data 2\nAnalysis. Wenowlookat(b)-(d)inFigure1analyticallytounderstandwhythetrainingscheme\nworks."
  },
  {
    "chunk_id": "doc_5_p3_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Webeginbyaddressing(b): howdoesDconvergetoD∗(x)intheinnerloop? Statement1. ForfixedG,theoptimaldiscriminatorDis\np (x)\nD∗(x)= data (4)\nG p (x)+p (x)\ndata G\nProof. GivenafixedG,thetrainingobjectiveforDistomaximizeJ(D,G),where\nJ(G,D)=E [logD(x)]+E [log(1−D(G(z)))]\nx∼pdata z∼pz\n(cid:90) (cid:90)\n= p (x)log(D(x))dx+ p (z)log(1−D(G(z)))dz\ndata z\nx z\n(cid:90)\n= p (x)log(D(x))+p (x)log(1−D(x))dx (5)\ndata G\nx\nThelast equalityemployschangeof variable forthe secondterm ofthe penultimateformula: as\nG : z (cid:55)→ x,(i)integratingoverp (z)isequivalenttointegratingoverp (x),and(ii)integrating\nz G\noverG(z)isequivalenttointegratingoverx. Further,wehave7\nD∗ =argmaxJ(G,D)\nD\n(cid:90)\n=argmax p (x)log(D(x))+p (x)log(1−D(x))dx (6)\ndata G\nD x\nwhere the integrand can be abstracted in the form f(D) = alog(D)+blog(1−D)."
  },
  {
    "chunk_id": "doc_5_p3_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "By setting\n∂f (cid:44)0andsolvingforD,wehaveD∗ = a ,satisfyingEq. 4,concludingtheproof. ∂D a+b\nNextwelookat(c,d)inthefigureandask: howdoesupdatingGgetustop =p ? G data\nStatement2. LetC(G) = maxJ(G,D),i.e. C(G)isG’sminimizationobjective(cf. Eq. 2). The\nD\nglobalminimumofC(G)isreachediffp =p ,atwhichpointC(G)=−log4. G data\n7ThelastequalityinEq.6iscopiedfromtheresultsofEq.5. 3"
  },
  {
    "chunk_id": "doc_5_p4_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Proof. Firstweprovep =p ⇒C(G)=−log4. Weknowthat\nG data\nC(G)=maxJ(G,D)\nD\n=E [logD∗(x)]+E [log(1−D∗(G(z)))]\nx∼pdata G z∼pz G\n=E [logD∗(x)]+E [log(1−D∗(x))] (bychangeofvariable(cf. Eq. 5))\nx∼pdata G x∼pG G\n(cid:20) (cid:21) (cid:20) (cid:21)\np (x) p (x)\n=E log data +E log G (7)\nx∼pdata p (x)+p (x) x∼pG p (x)+p (x)\ndata G data G\nWealsoknowthatifp = p ,thenD∗(x) = 1 (byEq. 4),whichwepluginEq. 7toobtain\ndata G G 2\nC(G) = log1 +log1 = −log4. That is, p = p ⇒ C(G) = −log4. We now show that\n2 2 G data\nC(G)=−log4⇒p =p ."
  },
  {
    "chunk_id": "doc_5_p4_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "G data\n(cid:20) (cid:21) (cid:20) (cid:21)\np (x) p (x)\nC(G)=E log data +E log G\nx∼pdata p (x)+p (x) x∼pG p (x)+p (x)\ndata G data G\n(cid:20) (cid:21) (cid:20) (cid:21)\np (x) p (x)\n=−log4+log4+E log data +E log G\nx∼pdata p (x)+p (x) x∼pG p (x)+p (x)\ndata G data G\n(cid:20) (cid:18) (cid:19)(cid:21) (cid:20) (cid:18) (cid:19)(cid:21)\np (x) p (x)\n=−log4+E log 2· data +E log 2· G\nx∼pdata p (x)+p (x) x∼pG p (x)+p (x)\ndata G data G\n(cid:34) (cid:35) (cid:34) (cid:35)\np (x) p (x)\n=−log4+E log data +E log G\nx∼pdata pdata(x)+pG(x) x∼pG pdata(x)+pG(x)\n2 2\n(cid:18) (cid:13) (cid:19) (cid:18) (cid:13) (cid:19)\n=−log4+KL p data (cid:13) (cid:13) (cid:13) p data (x) 2 +p G (x) +KL p G (cid:13) (cid:13) (cid:13) p data (x) 2 +p G (x)\n=−log4+2·JS(p (cid:107)p ) (bythedef."
  },
  {
    "chunk_id": "doc_5_p4_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "ofJensen-ShannonDivergence) (8)\ndata G\nNowgivenC(G)=log4,wemusthaveJS(p (cid:107)p )=0,whichisonlytruewhenp =p . data G G data\nThuswehaveshownC(G)=−log4⇒p =p . G data\nStatement1and2showtheoptimainthealternatingupdatesinAlgorithm1leadustotheequilibrium\noftheMinmaxGame. Theoriginalwork[1]alsoprovestheconvergenceofAlgorithm1. However\nthederivationinvolvesadvancedknowledgeofoptimization,Ithusrefertheinterestedreadertothe\nproofthere. 3 BasicImplementation\nThissectionpresentsademoimplementationwithMNISTimagereconstruction8. Specifically,we\nhavethediscriminatorDasaconvolutionalnet,andthegeneratorGadeconvolutionalnet[24]. A\nsimpleGANgraphisasfollows(theprecisesetupofthenetworksisomittedtodefertothefullcode\nJupyternotebook):\nimport tensorflow as tf\n# input images to the discriminator\nx_placeholder = tf.placeholder(tf.float32, shape = [None,28,28,1])\n# input noise vectors to the generator\nz_placeholder = tf.placeholder(tf.float32, [None, z_dimensions])\n# D(x): predicted logits/probabilities for real image. Dx = D(x_placeholder)\n# G(z): fake image generated by the generator."
  },
  {
    "chunk_id": "doc_5_p4_fixed_3",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Gz = G(z_placeholder, BATCH_SIZE, z_dimensions)\n# D(G(z)): predicted logits/probabilities for generated image. Dg = D(Gz, reuse=True)\n# Discriminator loss\n8For full code see https://github.com/suwangcompling/GAN-tutorials/blob/master/Basic\nGAN (MNIST demo).ipynb\n4"
  },
  {
    "chunk_id": "doc_5_p5_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "Figure2: Left: realimage;Center: generatedimagebeforetraining;Right: generatedimageafter\ntraining. Theconvolutionalnethastwoconv-avgpoollayersandtwofully-connectedlayers; the\ndeconvolutionalnethasfourlayersanddoesexponentialupscaling. # J(D,G) = E[log(Dx)] + E[log(1-D(Gz))]\n# NB: only update D’s params in training. d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dx,\nlabels = tf.ones_like(Dx)))\n# the above maximizes E[log(Dx)] by pushing Dx to label=1 (recognizing real\nimage). d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg,\nlabels = tf.zeros_like(Dg)))\n# the above maximizes E[log(1-D(Gz))] by pushing D(Gz) to label=0 (recognizing\nfake image). d_loss = d_loss_real + d_loss_fake\n# Generator loss\n# J(D,G) = E[log(1-D(Gz))]\n# NB: only update G’s params in training."
  },
  {
    "chunk_id": "doc_5_p5_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg,\nlabels = tf.ones_like(Dg)))\n# the above minimizes E[log(1-D(Gz))] by pushing D(G(z)) to label=1 (fool\ndiscriminator). # Separate training params\n# Ensure the correct params are updated in training. tvars = tf.trainable_variables()\nd_vars = [var for var in tvars if ’d_’ in var.name]\ng_vars = [var for var in tvars if ’g_’ in var.name]\n# Create optimizers for D & G\nadam = tf.train.AdamOptimizer(2e-4)\ntrainer_D = adam.minimize(d_loss, var_list=d_vars)\ntrainer_G = adam.minimize(g_loss, var_list=g_vars)\nItshouldbenotedthatGANinitsvanillaformisnotoriouslydifficulttotrainandsensitivetohyper-\nparameterorarchitecturalsetup. InthenextsectionwebrieflydiscusstheWassersteinGAN [26,27],\namassiveimprovementontheoriginalGANwhichismoreamenabletopracticalapplications. 4 Improvement: WasserteinGAN\nThissectionismyattempttogiveasimpleandintuitivedescriptionofWassersteinGAN (WGAN)as\nastrongandtheory-backedimprovement. Ideferthegorydetailsandmathematicalargumentstothe\noriginalpaper[26]andstayfocusedonthemainstorylineoftheargument."
  },
  {
    "chunk_id": "doc_5_p5_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "Despitethesexinessoftheideaofadversarialtraining,theoriginalGANexhibitsficklebehavior\nintrainingandisnotoriouslydifficulttotunecorrectly. Arjovskyetal. [26]arguetheobservation\nto a large extent stems from the KL-based distance metric it uses. Specifically, strong deviation\nbetweenthedistributionoftherealdataandthegeneratorleadstonon-convergenceofthemodel\nunder KL-based distance (one among many other distance metrics such as total variation). The\nWGAN,whichoperateswiththeEarthMover(EM)distance,ontheotherhand,doesnotsufferfrom\n5"
  },
  {
    "chunk_id": "doc_5_p6_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "theseissues. Itis“... continuouseverywhereanddifferentiable(almost)everywhere.” (Theorem\n1.2,[26]). Letp ,p bethedistributionsoftherealdataandthegeneratorrespectively,theEM\ndata g\ndistancecanbeunderstandastheminimaleffortrequiredtomoveprobabilitymassfromp to\ndata\ntransformitintop . Itisdefinedasfollows:\ng\nW(p ,p )= inf E [(cid:107)x−y (cid:107)] (9)\ndata g (x,y)∼γ\nγ∈Π(pdata,pg)\nwhereΠ(p ,p )isthesetofalljointdistributionsγ whosemarginalsarep andp . Because\ndata g data g\ntheEMdistanceisintractableinexact,Arjovskyetal. proposetoapproximateitbyapplyingthe\nKantorovich-Rubinsteinduality,whichsaysW isequivalenttothefollowing:\nW(p ,p )= sup E [f(x)]−E [f(x)] (10)\ndata g x∼pdata x∼pg\n(cid:107)f(cid:107)L≤1\nwhere the supremum is taken over all 1-Lipschitz functions, and f is a general notation for a\nfunction,which,inthecontextofGAN,denotesthediscriminatorD(itisalsoknownasthecritic). Sidesteppingthesophisticatedmathematicalideasinvolved,oneonlyneedstounderstandthatthe\nK-Lipschitz condition provides guarantee that the EM distance is continuous and differentiable\neverywhere with only minor assumptions (cf. Theorem 1. [26])."
  },
  {
    "chunk_id": "doc_5_p6_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "With this approximation, the\ngradientisnoweasilycalculated:\n∇ W(p ,p )=∇ {E [D (x)]−E D (G (z))}\nθ data g θ x∼pdata w z∼pz w θ\n=−E [∇ D (G (z))]\nz∼pz θ w θ\nwhereθaretheparametersofthegeneratorG,z ∼p therandomnoise. Finally,Arjovskyetal. also\nz\nproposeaweightclippingtechniquewhichguaranteesthefamilyofdistributionsthediscriminator\ncantakeon, i.e. D ,w ∈ W isK-Lipschitz—theweightsareconstrainedtoliewithin[−c,c],\nw\nwherecissome(usallysmall)parameter(e.g. 0.01isapopulardefault). Theweight-clippingtakes\nplaceaftertheweightupdate."
  },
  {
    "chunk_id": "doc_5_p6_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "DespiteofthecomplexityinthemathematicalargumentofWGAN,implementation-wise9itisquite\nsimple: wesimplyreplacethelosscalculationinthecodeoftheprevioussectionwiththefollowing:\n# Approximating the Earth Mover (EM) distance\nd_loss = tf.reduce_mean(Dx) - tf.reduce_mean(Dg)\ng_loss = tf.reduce_mean(Dg)\nandthenaddtheweight-clippingtothediscriminator:\nd_clip = [v.assign(tf.clip_by_value(v,-0.01,0.01)) for v in d_vars]\nWhiletheoreticallysoundandgeneralbeingsuperiortothevanillaGAN,WGANaspresentedabove\nstillexhibitsundesiredbehaviorinpractice. Inparticularitisproneto(i)experienceexplodingor\nvanishinggradients; (ii)failstomatchhigher-ordermomentsinpullingp andp close. Asa\ndata g\nremedywhichisveryrobustinpractice,Gulrajanietal. [27]suggestagradientpenaltymethodto\nreplacetheweightclippingmethod(towhichtheyattributethebehaviorsabove). Thefollowing\npenaltytermisaddedinthelossfunction:\nλE (cid:2) ((cid:107)∇ D(xˆ)(cid:107) −1)2(cid:3) (11)\nxˆ∼pxˆ xˆ 2\nwherexˆ =(cid:15)·x+(1−(cid:15))·G(z),aninterpolationofthetruedataandthegenerateddata."
  },
  {
    "chunk_id": "doc_5_p6_fixed_3",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Wewill\nnotexplorethedetailsofthismethod. Interestedreadersareencouragedtoreadtheoriginalpaper\nwhichisverywellandclearlywritten[27]. Thegradientpenaltyisimplementedasfollows(added\naftertheWGANimplementationandgetridofweight-clipping)10:\n# Gradient penalty\n# Gulrajani et al. (2017), Algorithm 1. 9https://github.com/suwangcompling/GAN-tutorials/blob/master/Wasserstein GAN\n(original weight-clipping, MNIST demo).ipynb. 10https://github.com/suwangcompling/GAN-tutorials/blob/master/Wasserstein GAN\n(gradient-penalty, MNISTdemo).ipynb. 6"
  },
  {
    "chunk_id": "doc_5_p7_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "Figure3: AdversarialDocumentModel: avariantofEnergy-basedGAN\n# \\lambda * (||grad(D(x_hat))||_2 - 1.0)**2\nepsilon = tf.random_uniform([], 0.0, 1.0) # get random number. x_hat = epsilon*x_placeholder + (1-epsilon)*Gz # compute x_hat by interpolation. Dx_hat = d_net(x_hat, reuse=True) # compute D_w(x_hat). scale = 10.0 # set scale=\\lambda. dDx = tf.gradients(Dx_hat, x_hat)[0] # compute grad(D(x_hat)). dDx = tf.sqrt(tf.reduce_sum(tf.square(dDx), axis=1)) # compute ||grad(D(x_hat))||_2. dDx = tf.reduce_mean(tf.square(dDx - 1.0) * scale) # compute final penalty term. d_loss = d_loss + dDx # apply to original loss. Finallytwotrainingtips: (i)thescalingfactorλworkswellempiricallyacrossawiderangeoftasks\nwhensetto10.0,aspertherecommendationinthepaper;(ii)theresultsaregenerallybetterifwe\nreplacebatchnormwithlayernormforthediscriminator/critic."
  },
  {
    "chunk_id": "doc_5_p7_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "5 ApplicationsinNLP\nAsbrieflymentionedinSection1,recentyearshaveseenapplicationsofGANinthespaceofNLP.In\nthissection,ItakeDocumentModeling[8]andDialogueGeneration[9]asexamplestodemonstrate\nthepotentialofGANasaflexibleandversatilelearningframework. Documentmodeling. Glover(2016)[8]proposesanEnergy-basedGAN[13](ADM)11wherethe\ngeneratorisaregularMLP,whilethediscrimatorisaDenoisingAutoencoder(DAE)[14](asthe\nenergyfunction[15])instead. Themodelispitchedagainstahighlywell-tunedRestrictedBoltzmann\nMachine(RBM)[17]basedsystemDocNADE[17],andabaseline(astand-alonediscriminative\nDAEclassifier). ThearchitectureisillustratedgraphicallyinFigure2. InADM,adocumentismodeledasabinarybag-of-wordsvectorx∈{0,1}V,whereV isthesize\nofvocabulary. D(theDAEdiscriminator)takestwoinputs: (i)a“real”documentvectorx,and(ii)a\nfakedocumentvectorG(z)generatedbyG(anMLP).Ininputvectorisfirstprocessedthrougha\ncorruptionprocess12C toobtainvectorxcandfeedxctoaregularencode-decodecomponent[14]\n(Enc,Dec)."
  },
  {
    "chunk_id": "doc_5_p7_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "h=f(Wexc+b ) (12)\ne\ny =Wdh+b (13)\nd\nwhere(We,b ),(Wd,b )aretheparametersoftheencoder/decoder,respectively. Thequalityof\ne d\ntheinputisevaluatedbyMeanSquareError(MSE)asametricforreconstructionerror. V\n1 (cid:88)\nMSE = (x −y )2 (14)\nV i i\ni=1\n11Themodelislistedin[8]withthenameAdversarialDocumentModel. 12Vincentetal. (2010)[14]showthatformulatingaautoencoderreconstructiontaskasadenoisingtask(with\ncorruptedinput)helpstheautoencodertogeneralizebetter. 7"
  },
  {
    "chunk_id": "doc_5_p8_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Figure4: ADMresults\nIntraining,GandDstrivetominimizetheirrespectiveenergyfunctions13:\nf (x,z)=D(x)+max(0,m−D(G(z))) (15)\nD\nf (z)=D(G(z)) (16)\nG\nTheprocessamountstothesameeffectasMinmaxGame(Eq. 3),inthatDincreasesthescoreit\nassignstherealdata(i.e. x)whileloweringthescoreforthefakedata(i.e. G(z))generatedbyG,\nandGincreasesthescoreofthefakedata. Inhisexperiments,[8]formulatesadocumentclassificationtaskwiththe20NewsgroupDataset\n[18],wherethemodeltakesaquerydocumentdasinputandproducesasetofoutputdocumentsthat\nareclosest14 tod. TheresultsareshowninFigure3asaprecision-recallcurve. Whileproducing\noverallweakerperformancethanthestate-of-the-artDocNADE,ADMdemonstratesitspowerby\ndefeatingthestrongbaselineDAEmodelsbyarespectablemargin,showingitspromise15inmore\nsophisticatedformulation. Dialoguegeneration. Lietal.,(2017)[9]presentsaninterestingGAN-baseddialoguegeneration\nsystem. Theyformulateareinforcementlearning[20]basedTurningTestwherethegoalisforthe\ngenerator G, under the guidance of a discriminator D, to learn to generate realistic responses to\ninputsentencesthatareindistinguishablefromresponsesgivenbyhumans."
  },
  {
    "chunk_id": "doc_5_p8_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "DguidesGbygivingit\nfeedbacksintheformofreinforcementrewards: Positiverewardforrealisticresponses,andnegative\nrewardfornon-realisticones. Concretely,Gtakestheformofasequence-to-sequenceRecurrentNeuralNetwork(RNN)[21]which\ngeneratesresponseybasedonadialoguehistoryx. Disanautoencoder-basedbinaryclassifier[22]\nthattakesa(dialoguehistory,response)pair{x,y},andproducesalabelindicatingwhethertheinput\nisgeneratedbyahuman(i.e. realdata)orG(i.e. fakedata). Itfurtherreturnsarewardscoretoguide\nG: Q ({x,y}),Q ({x,y})forpositiveandnegativerewards,respectively. + −\nDeferringthedetailsofthemodeltotheoriginalwork[9],Ionlyshowsomesampleoutputsfrom\nitstwovariants16 evaluatedthereintodemonstratethequalityofthegeneratedresponses(Figure4). Here,theinputisadialogueprobe,andthefollowinglinesaretherespectiveresponsesofthemodels\ngivetotheprobe. ThetwoGAN’sproduceapparentlysuperiorresponsesbyhumanjudgment. In\nadditiontogeneratinggoodresponses,[9]alsoshowsthereliabilityoftheirmodel(seeTable3inthe\noriginalwork). 13Onemayconsideranenergyfunctionasafamilyoflossfunction[15]. 14Bycosinesimilarity."
  },
  {
    "chunk_id": "doc_5_p8_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "15Inimagegenerationtasks,whilesometimesfallshortbehindVAE[19]inclassificationtasks[2],GAN\noftentimesgeneratesmorerealisticimagesbyhumanjudgment.Thisthusleadsustoreasonablybelieveinits\npotentialindocument-basedtasks. 16REGSimprovesonREINFORCEbyamelioratingitstendencyformodecollapose—forGtogeneratethe\nsamefakedataoverandoveragain[2]. 8"
  },
  {
    "chunk_id": "doc_5_p9_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Figure5: DialogueGeneration(REINFORCEandREGSareGANs)\n6 Conclusion\nIn this tutorial, I started by giving an in-detail description of GAN (Section 1) and step-by-step\nderivationinrelatedproofs(Section2). InSection3and4Ipresentedsimpleimplementationsofthe\noriginalGANandtheWassersteinGAN.FinallyIdemonstratedtheflexibilityofGANasanovel\nneuralnetarchitecturewithexamplesofitsapplicationinNLP(Section5). References\n[1]Goodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.&Bengio,\nY.(2014)GenerativeAdversarialNets.NIPS2014. [2] Goodfellow, I.J. (2016) NIPS 2016 Tutorial: Generative Adversarial Networks. NIPS 2016. CoRR,\narXiv:1701.00160. [3]Jang,E. (2015)GenerativeAdversarialNetsinTensorflow.EricJang’sBlog:blog.evjang.com/2016/\n06/generative-adversarial-nets-in.html. [4]Lotter,W.,Kreiman,G.&Cox,D. (2015)UnsupervisedLearningofVisualStructureUsingPredictive\nGenerativeNetworks.CoRR,arXiv:1151.06380."
  },
  {
    "chunk_id": "doc_5_p9_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "[5] Ledig, C., Theis, L., Huszar, F., Caballero, J., Aitken, A. P., Tejani, A., Totz, J., Wang, Z., & Shi,\nW.(2016)Photo-realisticsingleImageSuper-resolutionUsingaGenerativeAdversarialNetwork. CoRR,\narXiv:1609.04802. [6] Isola, P., Zhu, J.-Y., Zhou, T., & Efros, A. A. (2016). Image-to-image Translation with Conditional\nAdversarialNetworks.CoRR,arXiv:1611.07004. [7]Donahue,J.,Krähenbühl,P.&Darrell,T.(2017)AdversarialFeatureLearning.CoRR,arXiv:1605.09782. [8]Glover,J.(2016)ModelingDocumentswithGenerativeAdversarialNetworks.CoRR,arXiv:1612.09122. [9]Li, J., Monroe, W., Shi, T., Jean, S., Ritter, A.&Jurafsky, D.(2017)AdversarialLearningforNeural\nDialogueGeneration.CoRR,arXiv:1701.06547. [10]ChenX.,Athiwaratkun,B.,Sun,Y.,Weinberger,K.&Cardie,C. (2016)AdversarialDeepAveraging\nNetworksforCross-lingualSentimentClassification.CoRR,arXiv:1606.01614."
  },
  {
    "chunk_id": "doc_5_p9_fixed_2",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "[11] Zhang, Y., Barzilay, R. & Jaakkola, T. (2017) Aspect-augmented Adversarial Networks for Domain\nAdaptation.CoRR,arXiv:1701.00188. [12]Ng,A.&Jordan,M.I.(2002)OnDiscriminativevs. GenerativeClassifiers: AComparisonofLogistic\nRegressionandNaïveBayes.NIPS2002. [13] Zhao, J., Mathieu, M. & LeCun Y. (2016) Energy-based Generative Adversarial Network. CoRR,\narXiv:1609.03126. 9"
  },
  {
    "chunk_id": "doc_5_p10_fixed_0",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "[14]Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.&Manzagol,P-A. (2010)StackedDenoisingAutoencoders:\nLearningUsefulRepresentationsinaDeepNetworkwithaLocalDenoisingCriterion.JMLR2010. [15]LeCun,Y.,Chopra,S.,Hadsell,R.,Ranzato,M-A.&Huang,F.J. (2006)ATutorialonEnergy-based\nLearning.InBakir,G.,Hofman,T.,Schölkopf,B.,Smola,A.&Taskar,B.(eds.)PredictingStructuredData. MITPress. [16]Hinton,G.E. (2002)TrainingProductsofExpertsbyMinimizingContrastiveDivergence.NeuralComputa-\ntion,vol.14,no.8,pp.1607–1614. [17]Larochelle,H.&Lauly,S(2012)ANeuralAutoregressiveDistributionEstimator.NIPS2012. [18]Lang,K.(1995)Newsweeder:LearningtoFilterNews.ICML1995. [19]Kingma,D.P.&Welling,M.(2014)Auto-encodingVariationalBayes.ICLR2014. [20]Williams,R.J. (1992)SimpleStatisticalGradient-followingAlgorithmsforConnectionistReinforcement\nLearning.MachineLearning,vol.8(3-4):229–256."
  },
  {
    "chunk_id": "doc_5_p10_fixed_1",
    "doc_id": "doc_5",
    "pdf_name": "5.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "[21]Sutskever,I.,Vinyals,O.&Quoc,V.L. (2014)SequencetoSequenceLearningwithNeuralNetworks.NIPS\n2014. [22] Li, J., Luong, M-T. & Jurafsky, D. (2015) A Hierarchical Neural Autoencoder for Paragraphs and\nDocuments.CoRR,arXiv:1506.01057. [23] Gutmann, M. & Hyvärinen A. (2010) Noise-contrastive estimation: A new estimation principle for\nunnormalizedstatisticalmodels.InProceedingsofAISTATS.Sardina,Italy. [24]Zeiler,M.D.,Krishnan,D.,Taylor,G.W.,Fergus,R. (2010)DeconvolutionalNetworks.InProceedingsof\nCVPR. [25]Radford,A.,Metz,L.,Chintala,S. (2016)UnsupervisedRepresentationLearningwithDeepConvolutional\nGenerativeAdversarialNetworks.InProceedingsofICLR. [26]Arjovsky,M.,Chintala,S.,Bottou,L.(2017)WassersteinGAN.CoRR. [27]Gulrajani,I.,Ahmed,F.,Arjovsky,M.,Dumoulin,V.,Courville,A. (2017)ImprovedTrainingofWasserstein\nGANs.InProceedingsofNIPS. 10"
  },
  {
    "chunk_id": "doc_6_p1_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "ImageNet: A Large-Scale Hierarchical Image Database\nJiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLiandLiFei-Fei\nDept. ofComputerScience,PrincetonUniversity,USA\n{jiadeng, wdong, rsocher, jial, li, feifeili}@cs.princeton.edu\nAbstract content-basedimagesearchandimageunderstandingalgo-\nrithms,aswellasforprovidingcriticaltrainingandbench-\nTheexplosionofimagedataontheInternethasthepo- markingdataforsuchalgorithms. tential to foster more sophisticated and robust models and ImageNetusesthehierarchicalstructureofWordNet[9]. algorithmstoindex,retrieve,organizeandinteractwithim- Each meaningful concept in WordNet, possibly described\nages and multimedia data. But exactly how such data can by multiple words or word phrases, is called a “synonym\nbeharnessedandorganizedremainsacriticalproblem. We set” or “synset”. There are around 80,000 noun synsets\nintroducehereanewdatabasecalled“ImageNet”,alarge- in WordNet. In ImageNet, we aim to provide on aver-\nscale ontology of images built upon the backbone of the age 500-1000 images to illustrate each synset. Images of\nWordNetstructure. ImageNetaimstopopulatethemajority each concept are quality-controlled and human-annotated\nof the 80,000 synsets of WordNet with an average of 500- as described in Sec. 3.2. ImageNet, therefore, will offer\n1000 clean and full resolution images. This will result in tens of millions of cleanly sorted images."
  },
  {
    "chunk_id": "doc_6_p1_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "In this paper,\ntens of millions of annotated images organized by the se- wereportthecurrentversionofImageNet,consistingof12\nmantic hierarchy of WordNet. This paper offers a detailed “subtrees”: mammal,bird,fish,reptile,amphibian,vehicle,\nanalysis of ImageNet in its current state: 12 subtrees with furniture, musical instrument, geological formation, tool,\n5247synsetsand3.2millionimagesintotal. Weshowthat flower, fruit. These subtrees contain 5247 synsets and 3.2\nImageNet is much larger in scale and diversity and much millionimages. Fig.1showsasnapshotoftwobranchesof\nmoreaccuratethanthecurrentimagedatasets. Construct- themammalandvehiclesubtrees. Thedatabaseispublicly\ning such a large-scale database is a challenging task. We availableathttp://www.image-net.org. describethedatacollectionschemewithAmazonMechan- The rest of the paper is organized as follows: We first\nical Turk. Lastly, we illustrate the usefulness of ImageNet show that ImageNet is a large-scale, accurate and diverse\nthroughthreesimpleapplicationsinobjectrecognition,im- imagedatabase(Section2). InSection4,wepresentafew\nageclassificationandautomaticobjectclustering. Wehope simpleapplicationexamplesbyexploitingthecurrentIma-\nthat the scale, accuracy, diversity and hierarchical struc- geNet, mostlythemammalandvehiclesubtrees."
  },
  {
    "chunk_id": "doc_6_p1_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Ourgoal\ntureofImageNetcanofferunparalleledopportunitiestore- istoshowthatImageNetcanserveasausefulresourcefor\nsearchersinthecomputervisioncommunityandbeyond. visual recognition applications such as object recognition,\nimageclassificationandobjectlocalization.Inaddition,the\nconstructionofsuchalarge-scaleandhigh-qualitydatabase\ncan no longer rely on traditional data collection methods. 1.Introduction\nSec. 3 describes how ImageNet is constructed by leverag-\nThe digital era has brought with it an enormous explo- ingAmazonMechanicalTurk. sion of data. The latest estimations put a number of more\n2.PropertiesofImageNet\nthan 3 billion photos on Flickr, a similar number of video\nclipsonYouTubeandanevenlargernumberforimagesin ImageNet is built upon the hierarchical structure pro-\ntheGoogleImageSearchdatabase. Moresophisticatedand vided by WordNet. In its completion, ImageNet aims to\nrobust models and algorithms can be proposed by exploit- containintheorderof50millioncleanlylabeledfullreso-\ning these images, resulting in better applications for users lutionimages(500-1000persynset). Atthetimethispaper\ntoindex,retrieve,organizeandinteractwiththesedata. But iswritten,ImageNetconsistsof12subtrees. Mostanalysis\nexactly how such data can be utilized and organized is a willbebasedonthemammalandvehiclesubtrees. problemyettobesolved. Inthispaper,weintroduceanew\nimage database called “ImageNet”, a large-scale ontology Scale ImageNetaimstoprovidethemostcomprehensive\nofimages."
  },
  {
    "chunk_id": "doc_6_p1_fixed_3",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Webelievethatalarge-scaleontologyofimages and diverse coverage of the image world. The current 12\nis a critical resource for developing advanced, large-scale subtrees consist of a total of 3.2 million cleanly annotated\n1"
  },
  {
    "chunk_id": "doc_6_p2_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "mammal placental carnivore canine dog working dog husky\nvehicle craft watercraft sailing vessel sailboat trimaran\nFigure 1: Asnapshotoftworoot-to-leafbranchesofImageNet: thetoprowisfromthemammalsubtree; thebottomrowisfromthe\nvehiclesubtree.Foreachsynset,9randomlysampledimagesarepresented. 0.2\n0.15\n0.1\n0.05\n0 0 500 1000 1500 2000 2500\n# images per synset\negatnecrep\nESP Cat Subtree Imagenet Cat Subtree\nSummary of selected subtrees\nAvg. synset Total # 376\nSubtree # Synsets\nsize image\nMammal 1170 737 862K\nVehicle 520 610 317K\nGeoForm 176 436 77K 1830\nFurniture 197 797 157K\nBird 872 809 705K\nMusicInstr 164 672 110K\nESP Cattle Subtree Imagenet Cattle Subtree\n176\n1377\nFigure2: ScaleofImageNet. Redcurve: Histogramofnumber\nof images per synset. About 20% of the synsets have very few\nimages. Over 50% synsets have more than 500 images. Table: Figure3: Comparisonofthe“cat”and“cattle”subtreesbetween\nSummaryofselectedsubtrees.Forcompleteandup-to-datestatis- ESP [25] and ImageNet. Within each tree, the size of a node is\nticsvisithttp://www.image-net.org/about-stats. proportionaltothenumberofimagesitcontains. Thenumberof\nimagesforthelargestnodeisshownforeachtree. Sharednodes\nbetweenanESPtreeandanImageNettreearecoloredinred. images spread over 5247 categories (Fig. 2)."
  },
  {
    "chunk_id": "doc_6_p2_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "On average\nover600imagesarecollectedforeachsynset. Fig.2shows\nthedistributionsofthenumberofimagespersynsetforthe gorylabelsintoasemantichierarchybyusingWordNet,the\ncurrent ImageNet 1. To our knowledge this is already the densityofImageNetisunmatchedbyothers. Forexample,\nlargestcleanimagedatasetavailabletothevisionresearch toourknowledgenoexistingvisiondatasetoffersimagesof\ncommunity,intermsofthetotalnumberofimages,number 147dogcategories. Fig.3comparesthe“cat”and“cattle”\nofimagespercategoryaswellasthenumberofcategories2. subtreesofImageNetandtheESPdataset[25]. Weobserve\nthatImageNetoffersmuchdenserandlargertrees. Hierarchy ImageNet organizes the different classes of\nimages in a densely populated semantic hierarchy. The Accuracy We would like to offer a clean dataset at all\nmainassetofWordNet[9]liesinitssemanticstructure,i.e. levels of the WordNet hierarchy. Fig. 4 demonstrates the\nitsontologyofconcepts. SimilarlytoWordNet, synsetsof labeling precision on a total of 80 synsets randomly sam-\nimages in ImageNet are interlinked by several types of re- pled at different tree depths. An average of 99.7% preci-\nlations, the “IS-A” relation being the most comprehensive sionisachievedonaverage. Achievingahighprecisionfor\nand useful."
  },
  {
    "chunk_id": "doc_6_p2_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Although one can map any dataset with cate- all depths of the ImageNet tree is challenging because the\nlowerinthehierarchyasynsetis,theharderitistoclassify,\n1About20%ofthesynsetshaveveryfewimages,becauseeitherthere\ne.g. SiamesecatversusBurmesecat. areveryfewwebimagesavailable,e.g.“vespertilianbat”,orthesynsetby\ndefinitionisdifficulttobeillustratedbyimages,e.g.“two-year-oldhorse”. 2ItisclaimedthattheESPgame[25]haslabeledaverylargenumber Diversity ImageNetisconstructedwiththegoalthatob-\nofimages,butonlyasubsetof60Kimagesarepubliclyavailable. jectsinimagesshouldhavevariableappearances,positions,"
  },
  {
    "chunk_id": "doc_6_p3_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "1\n0.95\n0.9\n1 2 3 4 5 6 7 8 9\nnoisicerp datasets are needed for the next generation of algorithms. ThecurrentImageNetoffers20×thenumberofcategories,\nand100×thenumberoftotalimagesthanthesedatasets. TinyImage TinyImage [24] is a dataset of 80 million\ntree depth 32 × 32 low resolution images, collected from the Inter-\nnet by sending all words in WordNet as queries to image\nFigure4: Percentofcleanimagesatdifferenttreedepthlevelsin\nsearchengines. EachsynsetintheTinyImagedatasetcon-\nImageNet. A total of 80 synsets are randomly sampled at every\ntainsanaverageof1000images,amongwhich10-25%are\ntreedepthofthemammalandvehiclesubtrees. Anindependent\npossiblycleanimages. AlthoughtheTinyImagedatasethas\ngroup of subjects verified the correctness of each of the images. Anaverageof99.7%precisionisachievedforeachsynset. hadsuccesswithcertainapplications,thehighlevelofnoise\nand low resolution images make it less suitable for gen-\neral purpose algorithm development, training, and evalua-\nImageNet TinyImage LabelMe ESP LHill\ntion. Compared to the TinyImage dataset, ImageNet con-\nLabelDisam Y Y N N Y\nClean Y N Y Y Y tainshighqualitysynsets(∼ 99%precision)andfullreso-\nDenseHie Y Y N N N lutionimageswithanaveragesizeofaround400×350."
  },
  {
    "chunk_id": "doc_6_p3_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "FullRes Y N Y Y Y\nPublicAvail Y Y Y N N\nESPdataset TheESPdatasetisacquiredthroughanon-\nSegmented N N Y N Y\nline game [25]. Two players independently propose labels\nTable1: ComparisonofsomeofthepropertiesofImageNetver- to one image with the goal of matching as many words as\nsus other existing datasets. ImageNet offers disambiguated la- possible in a certain time limit. Millions of images are la-\nbels (LabelDisam), clean annotations (Clean), a dense hierarchy\nbeledthroughthisgame,butitsspeedednaturealsoposesa\n(DenseHie),fullresolutionimages(FullRes)andispubliclyavail-\nmajordrawback. RoschandLloyd[20]havedemonstrated\nable(PublicAvail). ImageNetcurrentlydoesnotprovidesegmen-\nthathumanstendtolabelvisualobjectsataneasilyacces-\ntationannotations. sible semantic level termed as “basic level” (e.g. bird), as\nopposed to more specific level (“sub-ordinate level”, e.g. viewpoints,posesaswellasbackgroundclutterandocclu-\nsparrow),ormoregenerallevel(“super-ordinatelevel”,e.g. sions. Inanattempttotacklethedifficultproblemofquan-\nvertebrate). Labels collected from the ESP game largely\ntifying image diversity, we compute the average image of\nconcentrate at the “basic level” of the semantic hierarchy\neachsynsetandmeasurelosslessJPGfilesizewhichreflects\nas illustrated by the color bars in Fig. 6."
  },
  {
    "chunk_id": "doc_6_p3_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "ImageNet, how-\nthe amount of information in an image. Our idea is that a\never, demonstrates a much more balanced distribution of\nsynsetcontainingdiverseimageswillresultinablurrierav-\nimagesacrossthesemantichierarchy. Anothercriticaldif-\nerage image, the extreme being a gray image, whereas a\nference between ESP and ImageNet is sense disambigua-\nsynset with little diversity will result in a more structured,\ntion. Whenhumanplayersinputtheword“bank”,itisun-\nsharperaverageimage.Wethereforeexpecttoseeasmaller\nclear whether it means “a river bank” or a “financial insti-\nJPGfilesizeoftheaverageimageofamorediversesynset. tution”. Atthislargescale,disambiguationbecomesanon-\nFig.5comparestheimagediversityinfourrandomlysam-\ntrivial task. Without it, the accuracy and usefulness of the\npledsynsetsinCaltech101[8]3andthemammalsubtreeof\nESP data could be affected. ImageNet, on the other hand,\nImageNet. doesnothavethisproblembyconstruction. Seesection3.2\nformoredetails. Lastly,mostoftheESPdatasetisnotpub-\n2.1.ImageNetandRelatedDatasets\nlicly available. Only 60K images and their labels can be\nWe compare ImageNet with other datasets and summa- accessed[1]. rizethedifferencesinTable14."
  },
  {
    "chunk_id": "doc_6_p3_fixed_3",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "LabelMeandLotusHilldatasets LabelMe[21]andthe\nSmall image datasets A number of well labeled small LotusHilldataset[27]provide30kand50klabeledandseg-\ndatasets(Caltech101/256[8,12],MSRC[22],PASCAL[7] mentedimages,respectively5. Thesetwodatasetsprovide\netc.) have served as training and evaluation benchmarks complementary resources for the vision community com-\nfor most of today’s computer vision algorithms. As com- paredtoImageNet. Bothonlyhavearound200categories,\nputervisionresearchadvances,largerandmorechallenging but the outlines and locations of objects are provided. Im-\nageNetinitscurrentformdoesnotprovidedetailedobject\n3WealsocomparewithCaltech256[12].Theresultindicatesthediver-\noutlines(seepotentialextensionsinSec.5.1),butthenum-\nsityofImageNetiscomparable,whichisreassuringsinceCaltech256was\nspecificallydesignedtobemorediverse. ber of categories and the number of images per category\n4Wefocusourcomparisonsondatasetsofgenericobjects.Specialpur-\nposedatasets,suchasFERETfaces[19],LabeledfacesintheWild[13] 5Allstatisticsarefrom[21,27]. Inadditiontothe50kimages, the\nandtheMammalBenchmarkbyFinkandUllman[11]arenotincluded. LotusHilldatasetalsoincludes587kvideoframes."
  },
  {
    "chunk_id": "doc_6_p4_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Lossless JPG size in byte\nplatypus\npanda\nokapi\nelephant ImageNet\nCaltech101\n90 0 1000 1100\n(a) (b) (c)\nFigure5: ImageNetprovidesdiversifiedimages. (a)ComparisonofthelosslessJPGfilesizesofaverageimagesforfourdifferentsynsets\ninImageNet(themammalsubtree)andCaltech101.Averageimagesaredownsampledto32×32andsizesaremeasuredinbyte.Amore\ndiversesetofimagesresultsinasmallerlosslessJPGfilesize. (b)ExampleimagesfromImageNetandaverageimagesforeachsynset\nindicatedby(a). (c)ExamplesimagesfromCaltech101andaverageimages. Foreachcategoryshown,theaverageimageiscomputed\nusingallimagesfromCaltech101andanequalnumberofrandomlysampledimagesfromImageNet. 0.5\n0.4\n0.3\n0.2\n0.1\n0\n1 2 3 4 5 6 7 8 9\negatnecrep\naccuracyofimagesearchresultsfromtheInternetisaround\n10028 Imagenet 10% [24]. ImageNet aims to eventually offer 500-1000\nESP\nclean images per synset. We therefore collect a large set\nof candidate images. After intra-synset duplicate removal, 197850\neachsynsethasover10K imagesonaverage. WecollectcandidateimagesfromtheInternetbyquery-\ning several image search engines. For each synset, the\nqueries are the set of WordNet synonyms."
  },
  {
    "chunk_id": "doc_6_p4_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Search engines\ntypicallylimitthenumberofimagesretrievable(intheor-\ndepth derofafewhundredtoathousand). Toobtainasmanyim-\nagesaspossible,weexpandthequerysetbyappendingthe\nFigure 6: Comparison of the distribution of “mammal” labels\nquerieswiththewordfromparentsynsets,ifthesameword\novertreedepthlevelsbetweenImageNetandESPgame. They-\nappearsintheglossofthetargetsynset. Forexample,when\naxis indicates the percentage of the labels of the corresponding\ndataset. ImageNetdemonstratesamuchmorebalanceddistribu- querying“whippet”,accordingtoWordNet’sglossa“small\ntion,offeringsubstantiallymorelabelsatdeepertreedepthlevels. slender dog of greyhound type developed in England”, we\nTheactualnumberofimagescorrespondingtothehighestbaris alsouse“whippetdog”and“whippetgreyhound”. alsogivenforeachdataset. To further enlarge and diversify the candidate pool, we\ntranslate the queries into other languages [10], including\nChinese, Spanish, Dutch and Italian. We obtain accurate\nalreadyfarexceedsthesetwodatasets. Inaddition,images\ntranslationsbyWordNetsinthoselanguages[3,2,4,26]. in these two datasets are largely uploaded or provided by\nusersorresearchersofthedataset,whereasImageNetcon-\n3.2.CleaningCandidateImages\ntains images crawled from the entire Internet. The Lotus\nHilldatasetisonlyavailablethroughpurchase."
  },
  {
    "chunk_id": "doc_6_p4_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Tocollectahighlyaccuratedataset,werelyonhumans\ntoverifyeachcandidateimagecollectedinthepreviousstep\n3.ConstructingImageNet foragivensynset. Thisisachievedbyusingtheserviceof\nAmazon Mechanical Turk (AMT), an online platform on\nImageNet is an ambitious project. Thus far, we have\nwhich one can put up tasks for users to complete and to\nconstructed12subtreescontaining3.2millionimages. Our\ngetpaid. AMThasbeenusedforlabelingvisiondata[23]. goal is to complete the construction of around 50 million\nWith a global user base, AMT is particularly suitable for\nimagesinthenexttwoyears. Wedescribeherethemethod\nlargescalelabeling. weusetoconstructImageNet,sheddinglightonhowprop-\nIn each of our labeling tasks, we present the users with\nertiesofSec.2canbeensuredinthisprocess. a set of candidate images and the definition of the target\nsynset (including a link to Wikipedia). We then ask the\n3.1.CollectingCandidateImages\nusers to verify whether each image contains objects of the\nThefirststageoftheconstructionofImageNetinvolves synset. We encourage users to select images regardless of\ncollecting candidate images for each synset. The average occlusions, number of objects and clutter in the scene to"
  },
  {
    "chunk_id": "doc_6_p5_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "4.ImageNetApplications\n#Y # N Conf Conf\nCat BCat\n0 1 0.07 0.23 Inthissection,weshowthreeapplicationsofImageNet. User1 Y Y Y 1 0 0.85 0.69 Thefirstsetofexperimentsunderlinetheadvantagesofhav-\nUser 2 N Y Y\n1 1 0.46 0.49 ing clean, full resolution images. The second experiment\nUser 3 N Y Y 2 0 0.97 0.83 exploitsthetreestructureofImageNet,whereasthelastex-\nUser 4 Y N Y 0 2 0.02 0.12 periment outlines a possible extension and gives more in-\nUser5 Y Y Y 3 0 0.99 0.90\nsightsintothedata. User 6 N N Y 2 1 0.85 0.68\n4.1.Non-parametricObjectRecognition\nFigure 7: Left: IsthereaBurmesecatintheimages? Sixran-\ndomly sampled users have different answers. Right: The confi-\nGiven an image containing an unknown object, we\ndence score table for “Cat” and “Burmese cat”. More votes are\nwouldliketorecognizeitsobjectclassbyqueryingsimilar\nneededtoreachthesamedegreeofconfidencefor“Burmesecat”\nimagesinImageNet. Torralbaetal. [24]hasdemonstrated\nimages. that,givenalargenumberofimages,simplenearestneigh-\nbormethodscanachievereasonableperformancesdespitea\nensurediversity. high level of noise. We show that with a clean set of full\nresolutionimages,objectrecognitioncanbemoreaccurate,\nWhile users are instructed to make accurate judgment,\nespeciallybyexploitingmorefeaturelevelinformation."
  },
  {
    "chunk_id": "doc_6_p5_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "we need to set up a quality control system to ensure this\nWerunfourdifferentobjectrecognitionexperiments. In\naccuracy. There are two issues to consider. First, human\nall experiments, we test on images from the 16 common\nusers make mistakes and not all users follow the instruc-\ncategories7 betweenCaltech256andthemammalsubtree. tions. Second, users do not always agree with each other,\nWemeasureclassificationperformanceoneachcategoryin\nespeciallyformoresubtleorconfusingsynsets,typicallyat\ntheformofanROCcurve. Foreachcategory,thenegative\nthedeeperlevelsofthetree. Fig.7(left)showsanexample\nsetconsistsofallimagesfromtheother15categories. We\nofhowusers’judgmentsdifferfor“Burmesecat”. nowdescribeindetailourexperimentsandresults(Fig.8). Thesolutiontotheseissuesistohavemultipleusersin-\n1. NN-voting + noisy ImageNet First we replicate one\ndependentlylabelthesameimage. Animageisconsidered\nof the experiments described in [24], which we refer\npositive only if it gets a convincing majority of the votes. to as “NN-voting” hereafter. To imitate the TinyIm-\nWe observe, however, that different categories require dif-\nagedataset(i.e. imagescollectedfromsearchengines\nferentlevelsofconsensusamongusers."
  },
  {
    "chunk_id": "doc_6_p5_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "Forexample,while\nwithout human cleaning), we use the original candi-\nfiveusersmightbenecessaryforobtainingagoodconsen-\ndate images for each synset (Section 3.1) and down-\nsus on “Burmese cat” images, a much smaller number is\nsamplethemto32×32. Givenaqueryimage,were-\nneededfor“cat”images. Wedevelopasimplealgorithmto\ntrieve100ofthenearestneighborimagesbySSDpixel\ndynamically determine the number of agreements needed\ndistancefromthemammalsubtree. Thenweperform\nfordifferentcategoriesofimages. Foreachsynset,wefirst\nclassificationbyaggregatingvotes(numberofnearest\nrandomly sample an initial subset of images. At least 10\nneighbors)insidethetreeofthetargetcategory. usersareaskedtovoteoneachoftheseimages.Wethenob-\ntainaconfidencescoretable,indicatingtheprobabilityofan 2. NN-voting+cleanImageNet Nextwerunthesame\nimagebeingagoodimagegiventheuservotes(Fig.7(right) NN-voting experiment described above on the clean\nshowsexamplesfor“Burmesecat”and“cat”). Foreachof ImageNetdataset. Thisresultshowsthathavingmore\nremainingcandidateimagesinthissynset,weproceedwith accuratedataimprovesclassificationperformance. the AMT user labeling until a pre-determined confidence\nscore threshold is reached. It is worth noting that the con- 3."
  },
  {
    "chunk_id": "doc_6_p5_fixed_3",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "NBNN We also implement the Naive Bayesian\nfidencetablegivesanaturalmeasureofthe“semanticdiffi- Nearest Neighbor (NBNN) method proposed in [5]\nculty”ofthesynset. Forsomesynsets,usersfailtoreacha to underline the usefulness of full resolution im-\nmajorityvoteforanyimage,indicatingthatthesynsetcan- ages. NBNN employs a bag-of-features representa-\nnotbeeasilyillustratedbyimages6. Fig.4showsthatour tion of images. SIFT [15] descriptors are used in\nalgorithm successfully filters the candidate images, result- this experiment. Given a query image Q with de-\ninginahighpercentageofcleanimagespersynset. scriptors {d i },i = 1,...,M, for each object class\nC, we compute the query-class distance D =\nC\n7The categories are bat, bear, camel, chimp, dog, elk, giraffe, goat,\n6Analternativeexplanationisthatwedidnotobtainenoughsuitable gorilla,greyhound,horse,killer-whale,porcupine,raccoon,skunk,zebra. candidateimages.Giventheextensivenessofourcrawlingscheme,thisis Duplicates(∼20percategory)withImageNetareremovedfromthetest\nararescenario. set."
  },
  {
    "chunk_id": "doc_6_p6_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "1\n0.8\n0.6\n0.4\n0.2\n0\n0 0.2 0.4 0.6 0.8 1\netar\nevitisop\neurt\nNBNN\nNBNN−100\nNN−voting + clean ImageNet\nNN−voting + noisy ImageNet\nfalse positive rate\n(a)averageROC\n1\n0.8\n0.6\n0.4\n0.2\n00 0.2 0.4 0.6 0.8 1\nfalse positive rate\netar\nevitisop\neurt\n1\n0.8\n0.6\n0.4\nN N B B N N N N−100 0.2\nNN−voting + clean ImageNet\nNN−voting + noisy ImageNet\n00 0.2 0.4 0.6 0.8 1\nfalse positive rate\n(b)elk\netar\nevitisop\neurt\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n1 2 3 4 5 6 7 8 9\ntree height\nNBNN NBNN−100\nNN−voting + clean ImageNet\nNN−voting + noisy ImageNet\n(c)killer-whale\nFigure 8: (a) Object recognition experiment results plotted in\nROC curves. Each curve is the result of one of the four experi-\nmentsdescribedinSection4.1.ItisanaverageofallROCresults\nof16objectcategoriescommonlysharedbetweenCaltech256and\nthemammalsubtree. Caltech256imagesserveastestingimages. (b)(c)TheROCcurvefor“elk”and“killer-whale”. (cid:80)M (cid:107)d −dC(cid:107)2,wheredC isthenearestneighborof\ni=1 i i i\nd fromalltheimagedescriptorsinclassC."
  },
  {
    "chunk_id": "doc_6_p6_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Weorder\ni\nall classes by D and define the classification score\nC\nas the minimum rank of the target class and its sub-\nclasses. The result shows that NBNN gives substan-\ntiallybetterperformance,demonstratingtheadvantage\nof using a more sophisticated feature representation\navailablethroughfullresolutionimages. 4. NBNN-100 Finally, we run the same NBNN experi-\nment, but limit the number of images per category to\n100. The result confirms the findings of [24]. Per-\nformance can be significantly improved by enlarging\nthe dataset. It is worth noting that NBNN-100 out-\nperforms NN-voting with access to the entire dataset,\nagaindemonstratingthebenefitofhavingdetailedfea-\nturelevelinformationbyusingfullresolutionimages. 4.2.TreeBasedImageClassification\nComparedtootheravailabledatasets,ImageNetprovides\nimage data in a densely populated hierarchical structure. Manypossiblealgorithmscouldbeappliedtoexploitahi-\nerarchicaldatastructure(e.g. [16,17,28,18]). Inthisexperiment,wechoosetoillustratetheusefulness\noftheImageNethierarchybyasimpleobjectclassification\nCUA\negareva\nindependent classifier\ntree−max classifier\nFigure9: AverageAUCateachtreeheightlevel. Performance\ncomparisonatdifferenttreeheightlevelsbetweenindependently\ntrained classifiers and tree-max classifiers. The tree height of a\nnodeisdefinedasthelengthofthelongestpathtoitsleafnodes. Allleafnodes’heightis1. method which we call the “tree-max classifier”."
  },
  {
    "chunk_id": "doc_6_p6_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Imagine\nyouhaveaclassifierateachsynsetnodeofthetreeandyou\nwanttodecidewhetheranimagecontainsanobjectofthat\nsynset or not. The idea is to not only consider the classi-\nficationscoreatanodesuchas“dog”, butalsoofitschild\nsynsets,suchas“Germanshepherd”,“Englishterrier”,etc. Themaximumofalltheclassifierresponsesinthissubtree\nbecomestheclassificationscoreofthequeryimage. Fig. 9 illustrates the result of our experiment on the\nmammalsubtree. Notethatouralgorithmisagnostictoany\nmethod used to learn image classifiers for each synset. In\nthiscase,weuseanAdaBoost-basedclassifierproposedby\n[6]. For each synset, we randomly sample 90% of the im-\nagestoformthepositivetrainingimageset,leavingtherest\nof the 10% as testing images. We form a common neg-\native image set by aggregating 10 images randomly sam-\npled from each synset. When training an image classifier\nfor a particular synset, we use the positive set from this\nsynsetaswellasthecommonnegativeimagesetexcluding\ntheimagesdrawnfromthissynset,anditschildandparent\nsynsets. We evaluate the classification results by AUC (the area\nunder ROC curve). Fig. 9 shows the results of AUC for\nsynsets at different levels of the hierarchy, compared with\nanindependentclassifierthatdoesnotexploitthetreestruc-\ntureofImageNet. Theplotindicatesthatimagesareeasier\nto classify at the bottom of the tree (e.g."
  },
  {
    "chunk_id": "doc_6_p6_fixed_3",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "star-nosed mole,\nminivan, polar bear) as opposed to the top of the tree (e.g. vehicles,mammal,artifact,etc.). Thisismostlikelydueto\nstrongervisualcoherenceneartheleafnodesofthetree. At nearly all levels, the performance of the tree-max\nclassifier is consistently higher than the independent clas-\nsifier. This result shows that a simple way of exploiting\ntheImageNethierarchycanalreadyprovidesubstantialim-\nprovement for the image classification task without addi-\ntionaltrainingormodellearning."
  },
  {
    "chunk_id": "doc_6_p7_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "1 P R r e e c c a is ll ion\n0.8\n0.6\n0.4\n0.2\n0 Texaslo h n u g m ho a r n n Minivan tiger Golden R Ly e n tr x ieve w r olf helicopte ta r nk jet baby ca p rr a ia c g e e car moped greyhou b n o d vine tusker yacht tricycle Armadill p o uppy stealtha c ir a c m ra e f l t dobbin spaceshuttle\nFigure 10: Precision and recall of 22 categories from different\nlevelsofthehierarchy.Precisioniscalculatedbydividingthearea\nofcorrectlysegmentedpixelsbytheareaofdetectedpixels.Recall\nisthefractionofrelevantpixelareathatissuccessfullydetected. 4.3.AutomaticObjectLocalization\nImageNet can be extended to provide additional infor-\nFigure11: Samplesofdetectedboundingboxesarounddifferent\nmationabouteachimage. Onesuchinformationisthespa-\nobjects. tial extent of the objects in each image. Two application\nareas come to mind. First, for training a robust object de-\ntection algorithm one often needs localized objects in dif-\nferent poses and under different viewpoints. Second, hav-\ninglocalizedobjectsinclutteredscenesenablesuserstouse\nImageNetasabenchmarkdatasetforobjectlocalizational-\ngorithms. In this section we present results of localization\non22categoriesfromdifferentdepthsoftheWordNethier-\narchy.Theresultsalsothrowlightonthediversityofimages\nineachofthesecategories."
  },
  {
    "chunk_id": "doc_6_p7_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "Weusethenon-parametricgraphicalmodeldescribedin\n[14] to learn the visual representation of objects against a\nglobal background class. In this model, every input im-\nage is represented as a “bag of words”. The output is\na probability for each image patch to belong to the top-\nics z of a given category (see [14] for details). In or-\ni\nder to annotate images with a bounding box we calcu-\nlatethelikelihoodofeachimagepatchgivenacategoryc:\n(cid:80)\np(x|c) = p(x|z ,c)p(z |c). Finally, one bounding box\ni i i\nisputaroundtheregionwhichaccumulatesthehighestlike- Figure 12: Left: Averageimagesandimagesamplesofthede-\nlihood. tectedboundingboxesfromthe‘tusker’and‘stealthaircraft’cate-\nWe annotated 100 images in 22 different categories of gories.Right:Averageimagesandexamplesofthreebigclusters\nthe mammal and vehicle subtrees with bounding boxes afterk-meansclustering(seeSec.4.3fordetail). Differentview-\npoints and poses emerge in the “tusker” category. The first row\naroundtheobjectsofthatcategory. Fig.10showsprecision\nshowstuskersinsideview,frontviewandinprofile. Onecluster\nandrecallvalues. Notethatprecisionislowduetoextreme\nofaircraftimagesdisplaysmostlyplanesontheground. variabilityoftheobjectsandbecauseofsmallobjectswhich\nhavehardlyanysalientregions. 5.DiscussionandFutureWork\nFig."
  },
  {
    "chunk_id": "doc_6_p7_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "11 shows sampled bounding boxes on different\nclasses. The colored region is the detected bounding box, Ourfutureworkhastwogoals:\nwhiletheoriginalimageisinlightgray. 5.1.CompletingImageNet\nIn order to illustrate the diversity of ImageNet inside\neach category, Fig. 12 shows results on running k-means The current ImageNet constitutes ∼ 10% of the Word-\nclusteringonthedetectedboundingboxesafterconverting Net synsets. To further speed up the construction process,\nthemtograyscaleandrescalingthemto32×32.Allaverage wewillcontinuetoexploremoreeffectivemethodstoeval-\nimages, including those for the entire cluster, are created uatetheAMTuserlabelsandoptimizethenumberofrepe-\nwith approximately 40 images. While it is hard to iden- titionsneededtoaccuratelyverifyeachimage. Atthecom-\ntify the object in the average image of all bounding boxes pletionofImageNet,weaimto(i)haveroughly50million\n(showninthecenter)duetothediversityofImageNet,the clean, diverse and full resolution images spread over ap-\naverage images of the single clusters consistently discover proximately50Ksynsets;(ii)deliverImageNettoresearch\nviewpointsorcommonposes. communitiesbymakingitpubliclyavailableandreadilyac-"
  },
  {
    "chunk_id": "doc_6_p8_fixed_0",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "cessibleonline. Weplantousecloudstoragetoenableeffi- References\ncientdistributionofImageNetdata;(iii)extendImageNetto\n[1] http://www.hunch.net/˜jl/. includemoreinformationsuchaslocalizationasdescribed\n[2] TheChineseWordNet.http://bow.sinica.edu.tw. in Sec. 4.3, segmentation, cross-synset referencing of im- [3] TheSpanishWordNet.http://www.lsi.upc.edu/˜nlp. ages, as well as expert annotation for difficult synsets and [4] A.Artale,B.Magnini,andS.C. Wordnetforitaliananditsusefor\nlexicaldiscrimination.InAI*IA97,pages16–19,1997. (iv) foster an ImageNet community and develop an online\n[5] O. Boiman, E. Shechtman, and M. Irani. In defense of nearest-\nplatformwhereeveryonecancontributetoandbenefitfrom\nneighborbasedimageclassification.InCVPR08,pages1–8,2008. ImageNetresources. [6] B.Collins,J.Deng,K.Li,andL.Fei-Fei. Towardsscalabledataset\nconstruction:Anactivelearningapproach.InECCV08,pagesI:86–\n5.2.ExploitingImageNet 98,2008."
  },
  {
    "chunk_id": "doc_6_p8_fixed_1",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\nWehopeImageNetwillbecomeacentralresourcefora A.Zisserman. ThePASCALVisualObjectClassesChallenge2008\nbroadofrangeofvisionrelatedresearch. Forthecomputer (VOC2008) Results. http://www.pascal-network.org/\nchallenges/VOC/voc2008/workshop/. vision community in particular, we envision the following\n[8] L.Fei-Fei, R.Fergus, andP.Perona. One-shotlearningofobject\npossibleapplications. categories.PAMI,28(4):594–611,April2006. Atrainingresource. Mostoftoday’sobjectrecognition [9] C.Fellbaum. WordNet: AnElectronicLexicalDatabase. Bradford\nBooks,1998. algorithmshavefocusedonasmallnumberofcommonob-\n[10] R.Fergus,L.Fei-Fei,P.Perona,andA.Zisserman. Learningobject\njects,suchaspedestrians,carsandfaces.Thisismainlydue categoriesfromgoogle’simagesearch. InICCV05,pagesII:1816–\ntothehighavailabilityofimagesforthesecategories.Fig.6 1823,2005.\nhasshownthateventhelargestdatasetstodayhaveastrong [11] M.FinkandS.Ullman. Fromaardvarktozorro: Abenchmarkfor\nmammalimageclassification.IJCV,77(1-3):143–156,May2008."
  },
  {
    "chunk_id": "doc_6_p8_fixed_2",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "bias in their coverage of different types of objects. Ima-\n[12] G.Griffin, A.Holub, andP.Perona. Caltech-256objectcategory\ngeNet,ontheotherhand,containsalargenumberofimages dataset.TechnicalReport7694,Caltech,2007. fornearlyallobjectclassesincludingrareones. Oneinter- [13] G. Huang, M. Ramesh, T. Berg, and E. Learned Miller. Labeled\nestingresearchdirectioncouldbetotransferknowledgeof facesinthewild:Adatabaseforstudyingfacerecognitioninuncon-\nstrainedenvironments.TechnicalReport07-49,UMass,2007. commonobjectstolearnrareobjectmodels. [14] L.-J. Li, G. Wang, and L. Fei-Fei. OPTIMOL: automatic Online\nA benchmark dataset. The current benchmark datasets PicturecollecTionviaIncrementalMOdelLearning. InCVPR07,\nin computer vision such as Caltech101/256 and PASCAL pages1–8,2007. [15] D.Lowe. Distinctiveimagefeaturesfromscale-invariantkeypoints. have played a critical role in advancing object recognition\nIJCV,60(2):91–110,November2004. and scene classification research. We believe that the high [16] M.MarszalekandC.Schmid.Semantichierarchiesforvisualobject\nquality, diversity and large scale of ImageNet will enable recognition.InCVPR07,pages1–7,2007."
  },
  {
    "chunk_id": "doc_6_p8_fixed_3",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "ittobecomeanewandchallengingbenchmarkdatasetfor [17] M.MarszalekandC.Schmid. Constructingcategoryhierarchiesfor\nvisualrecognition.InECCV08,pagesIV:479–491,2008. futureresearch. [18] D.NisterandH.Stewenius. Scalablerecognitionwithavocabulary\nIntroducingnewsemanticrelationsforvisualmodeling. tree.InCVPR06,pagesII:2161–2168,2006. BecauseImageNetisuniquelylinkedtoallconcretenouns [19] P.Phillips,H.Wechsler,J.Huang,andP.Rauss. Theferetdatabase\nand evaluation procedure for face-recognition algorithms. IVC,\nof WordNet whose synsets are richly interconnected, one\n16(5):295–306,April1998. could also exploit different semantic relations for instance [20] E.RoschandB.Lloyd. Principlesofcategorization. InCognition\nto learn part models. To move towards total scene under- andcategorization,pages27–48,1978. standing, it is also helpful to consider different depths of [21] B. Russell, A. Torralba, K. Murphy, and W. Freeman. Labelme:\nAdatabaseandweb-basedtoolforimageannotation. IJCV,77(1-\nthesemantichierarchy. 3):157–173,May2008. Human vision research. ImageNet’s rich structure and [22] J.Shotton,J.Winn,C.Rother,andA.Criminisi."
  },
  {
    "chunk_id": "doc_6_p8_fixed_4",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Textonboost:Joint\ndense coverage of the image world may help advance the appearance,shapeandcontextmodelingformulti-classobjectrecog-\nnitionandsegmentation.InECCV06,pagesI:1–15,2006. understanding of the human visual system. For example,\n[23] A.SorokinandD.Forsyth.Utilitydataannotationwithamazonme-\nthequestionofwhetheraconceptcanbeillustratedbyim- chanicalturk.InInterNet08,pages1–8,2008. agesismuchmorecomplexthanonewouldexpectatfirst. [24] A.Torralba,R.Fergus,andW.Freeman. 80milliontinyimages:A\nAligningthecognitivehierarchywiththe“visual”hierarchy largedatasetfornonparametricobjectandscenerecognition.PAMI,\n30(11):1958–1970,November2008. alsoremainsanunexploredarea. [25] L.vonAhnandL.Dabbish.Labelingimageswithacomputergame. InCHI04,pages319–326,2004. Acknowledgment [26] P.Vossen,K.Hofmann,M.deRijke,E.TjongKimSang,andK.De-\nschacht. TheCornettodatabase:Architectureanduser-scenarios. In\nTheauthorswouldliketothankBangpengYao,HaoSu,Barry\nProceedingsDIR2007,pages89–96,2007. Chaiandanonymousreviewersfortheirhelpfulcomments.WDis [27] B.Yao,X.Yang,andS.Zhu."
  },
  {
    "chunk_id": "doc_6_p8_fixed_5",
    "doc_id": "doc_6",
    "pdf_name": "6.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Introductiontoalarge-scalegeneral\nsupportedbyGordonWufellowship.RSissupportedbytheERP purpose ground truth database: Methodology, annotation tool and\nandUptonfellowships.KLisfundedbyNSFgrantCNS-0509447\nbenchmarks.InEMMCVPR07,pages169–183,2007. [28] A.ZweigandD.Weinshall.Exploitingobjecthierarchy:Combining\nandbyresearchgrantsfromGoogle,Intel,MicrosoftandYahoo!. modelsfromdifferentcategorylevels.InICCV07,pages1–8,2007. LFFisfundedbyresearchgrantsfromMicrosoftandGoogle."
  },
  {
    "chunk_id": "doc_7_p1_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.0after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature."
  },
  {
    "chunk_id": "doc_7_p1_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[12]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[29,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[31,21,13]. ∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch. †WorkperformedwhileatGoogleBrain."
  },
  {
    "chunk_id": "doc_7_p1_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "‡WorkperformedwhileatGoogleResearch. 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA."
  },
  {
    "chunk_id": "doc_7_p2_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[18]andconditional\ncomputation[26],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains. Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,16]. Inallbutafewcases[22],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork. InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput."
  },
  {
    "chunk_id": "doc_7_p2_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs. 2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [11]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2. Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19]."
  },
  {
    "chunk_id": "doc_7_p2_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[28]. To the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8]. 3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29]. Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively."
  },
  {
    "chunk_id": "doc_7_p2_fixed_3",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\n2"
  },
  {
    "chunk_id": "doc_7_p3_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Figure1: TheTransformer-modelarchitecture. wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlayers,produceoutputsofdimensiond =512. model\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors."
  },
  {
    "chunk_id": "doc_7_p3_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey. 3.2.1 ScaledDot-ProductAttention\nWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof\nqueriesandkeysofdimensiond ,andvaluesofdimensiond . Wecomputethedotproductsofthe\nk v\n3"
  },
  {
    "chunk_id": "doc_7_p4_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "ScaledDot-ProductAttention Multi-HeadAttention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattentionlayersrunninginparallel. √\nquerywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\nk\nvalues. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\nintoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute\nthematrixofoutputsas:\nQKT\nAttention(Q,K,V)=softmax( √ )V (1)\nd\nk\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\nplicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor\nof √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\ndk\nasinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\nmatrixmultiplicationcode. Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms\nk\ndotproductattentionwithoutscalingforlargervaluesofd [3]."
  },
  {
    "chunk_id": "doc_7_p4_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Wesuspectthatforlargevaluesof\nk\nd ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\nk\nextremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 . dk\n3.2.2 Multi-HeadAttention\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\nmodel\nwefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\nlinearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\nk k v\nqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional\nv\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepictedinFigure2. Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\nsubspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis. 4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom\nvariableswithmean0andvariance1.Thentheirdotproduct,q·k=\n(cid:80)dk\nq k ,hasmean0andvarianced . i=1 i i k\n4"
  },
  {
    "chunk_id": "doc_7_p5_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "MultiHead(Q,K,V)=Concat(head ,...,head )WO\n1 h\nwherehead =Attention(QWQ,KWK,VWV)\ni i i i\nWheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv\ni i i\nandWO ∈Rhdv×dmodel. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\nd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\nk v model\nissimilartothatofsingle-headattentionwithfulldimensionality. 3.2.3 ApplicationsofAttentioninourModel\nTheTransformerusesmulti-headattentioninthreedifferentways:\n• In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\nandthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\npositioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31,2,8]. • Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\nandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\nencoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\nencoder."
  },
  {
    "chunk_id": "doc_7_p5_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\nallpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward\ninformationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis\ninsideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput\nofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2. 3.3 Position-wiseFeed-ForwardNetworks\nInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\nconsistsoftwolineartransformationswithaReLUactivationinbetween. FFN(x)=max(0,xW +b )W +b (2)\n1 1 2 2\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner-layer has dimensionality\nmodel\nd =2048. ff\n3.4 EmbeddingsandSoftmax\nSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput\ntokensandoutputtokenstovectorsofdimensiond ."
  },
  {
    "chunk_id": "doc_7_p5_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "Wealsousetheusuallearnedlineartransfor-\nmodel\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\nourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax\nlineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d . model\n3.5 PositionalEncoding\nSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe\norderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\ntokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\n5"
  },
  {
    "chunk_id": "doc_7_p6_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\nfordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel\nsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention. LayerType ComplexityperLayer Sequential MaximumPathLength\nOperations\nSelf-Attention O(n2·d) O(1) O(1)\nRecurrent O(n·d2) O(n) O(n)\nConvolutional O(k·n·d2) O(1) O(log (n))\nk\nSelf-Attention(restricted) O(r·n·d) O(1) O(n/r)\nbottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond\nmodel\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,\nlearnedandfixed[8]. Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\nPE =sin(pos/100002i/dmodel)\n(pos,2i)\nPE =cos(pos/100002i/dmodel)\n(pos,2i+1)\nwhereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding\ncorrespondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π."
  },
  {
    "chunk_id": "doc_7_p6_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "We\nchosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\nrelativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof\npos+k\nPE . pos\nWealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo\nversionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\nbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\nduringtraining. 4 WhySelf-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\n(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden\n1 n 1 n i i\nlayerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\nconsiderthreedesiderata. Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired. Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\ndependenciesisakeychallengeinmanysequencetransductiontasks."
  },
  {
    "chunk_id": "doc_7_p6_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Onekeyfactoraffectingthe\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\ntraverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\nandoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\ndifferentlayertypes. AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\n[31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving\nverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\n6"
  },
  {
    "chunk_id": "doc_7_p7_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\npathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework. Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\npositions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,\norO(log (n))inthecaseofdilatedconvolutions[15], increasingthelengthofthelongestpaths\nk\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\ntheapproachwetakeinourmodel. Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\nfromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\nandsemanticstructureofthesentences."
  },
  {
    "chunk_id": "doc_7_p7_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "5 Training\nThissectiondescribesthetrainingregimeforourmodels. 5.1 TrainingDataandBatching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\ntargetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\n2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\nvocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\nbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\ntargettokens. 5.2 HardwareandSchedule\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\ntrainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\nbottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\n(3.5days). 5.3 Optimizer\nWeusedtheAdamoptimizer[17]withβ =0.9,β =0.98and(cid:15)=10−9."
  },
  {
    "chunk_id": "doc_7_p7_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "Wevariedthelearning\n1 2\nrateoverthecourseoftraining,accordingtotheformula:\nlrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\nmodel\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\nwarmup_steps=4000. 5.4 Regularization\nWeemploythreetypesofregularizationduringtraining:\nResidualDropout Weapplydropout[27]totheoutputofeachsub-layer,beforeitisaddedtothe\nsub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\npositionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\nP =0.1. drop\n7"
  },
  {
    "chunk_id": "doc_7_p8_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost. BLEU TrainingCost(FLOPs)\nModel\nEN-DE EN-FR EN-DE EN-FR\nByteNet[15] 23.75\nDeep-Att+PosUnk[32] 39.2 1.0·1020\nGNMT+RL[31] 24.6 39.92 2.3·1019 1.4·1020\nConvS2S[8] 25.16 40.46 9.6·1018 1.5·1020\nMoE[26] 26.03 40.56 2.0·1019 1.2·1020\nDeep-Att+PosUnkEnsemble[32] 40.4 8.0·1020\nGNMT+RLEnsemble[31] 26.30 41.16 1.8·1020 1.1·1021\nConvS2SEnsemble[8] 26.36 41.29 7.7·1019 1.2·1021\nTransformer(basemodel) 27.3 38.1 3.3·1018\nTransformer(big) 28.4 41.0 2.3·1019\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This\nls\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore."
  },
  {
    "chunk_id": "doc_7_p8_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "6 Results\n6.1 MachineTranslation\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis\nlistedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\nsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\nthecompetitivemodels. OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\npreviousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\ndropoutrateP =0.1,insteadof0.3. drop\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\nwerewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[31]. Thesehyperparameters\nwerechosenafterexperimentationonthedevelopmentset."
  },
  {
    "chunk_id": "doc_7_p8_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "Wesetthemaximumoutputlengthduring\ninferencetoinputlength+50,butterminateearlywhenpossible[31]. Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\nsingle-precisionfloating-pointcapacityofeachGPU5. 6.2 ModelVariations\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\ncheckpointaveraging. WepresenttheseresultsinTable3. InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads. 5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively. 8"
  },
  {
    "chunk_id": "doc_7_p9_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\nper-wordperplexities. train PPL BLEU params\nN d d h d d P (cid:15)\nmodel ff k v drop ls steps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n(A)\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n16 5.16 25.1 58\n(B)\n32 5.01 25.4 60\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n(C) 256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n0.0 5.77 24.6\n0.2 4.95 25.5\n(D)\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality."
  },
  {
    "chunk_id": "doc_7_p9_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "This\nk\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[8],andobservenearlyidentical\nresultstothebasemodel. 7 Conclusion\nInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\nattention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\nmulti-headedself-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest\nmodeloutperformsevenallpreviouslyreportedensembles. Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\nplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\ntoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\nsuchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours."
  },
  {
    "chunk_id": "doc_7_p9_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "The code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor. Acknowledgements Wearegratefulto NalKalchbrennerand StephanGouwsfor theirfruitful\ncomments,correctionsandinspiration. 9"
  },
  {
    "chunk_id": "doc_7_p10_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "References\n[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\narXiv:1607.06450,2016. [2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\nlearningtoalignandtranslate. CoRR,abs/1409.0473,2014. [3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\nmachinetranslationarchitectures. CoRR,abs/1703.03906,2017. [4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\nreading. arXivpreprintarXiv:1601.06733,2016. [5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\nandYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\nmachinetranslation. CoRR,abs/1406.1078,2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprintarXiv:1610.02357,2016. [7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\nofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014."
  },
  {
    "chunk_id": "doc_7_p10_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "[8] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\ntionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017. [9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850,2013. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition,pages770–778,2016. [11] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin\nrecurrentnets: thedifficultyoflearninglong-termdependencies,2001. [12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780,1997. [13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\nthelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016. [14] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\nonLearningRepresentations(ICLR),2016."
  },
  {
    "chunk_id": "doc_7_p10_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "[15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\n2017. [16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks. InInternationalConferenceonLearningRepresentations,2017. [17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015. [18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\narXiv:1703.10722,2017. [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130,2017. [20] SamyBengioŁukaszKaiser. Canactivememoryreplaceattention? InAdvancesinNeural\nInformationProcessingSystems,(NIPS),2016. 10"
  },
  {
    "chunk_id": "doc_7_p11_fixed_0",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "[21] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015. [22] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention\nmodel. InEmpiricalMethodsinNaturalLanguageProcessing,2016. [23] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\nsummarization. arXivpreprintarXiv:1705.04304,2017. [24] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\npreprintarXiv:1608.05859,2016. [25] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015. [26] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\nlayer. arXivpreprintarXiv:1701.06538,2017. [27] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\nnov."
  },
  {
    "chunk_id": "doc_7_p11_fixed_1",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\nLearningResearch,15(1):1929–1958,2014. [28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\nInc.,2015. [29] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014. [30] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna. Rethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015. [31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\narXiv:1609.08144,2016. [32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forwardconnectionsforneuralmachinetranslation."
  },
  {
    "chunk_id": "doc_7_p11_fixed_2",
    "doc_id": "doc_7",
    "pdf_name": "7.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "CoRR,abs/1606.04199,2016. 11"
  },
  {
    "chunk_id": "doc_8_p1_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Recurrent Neural Network\nTINGWU WANG,\nMACHINE LEARNING GROUP,\nUNIVERSITY OF TORONTO\nFOR CSC 2541, SPORT ANALYTICS"
  },
  {
    "chunk_id": "doc_8_p2_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Contents\n1. Why do we need Recurrent Neural Network? 1. What Problems are Normal CNNs good at? 2. What are Sequence Tasks? 3. Ways to Deal with Sequence Labeling. 2. Math in a Vanilla Recurrent Neural Network\n1. Vanilla Forward Pass\n2. Vanilla Backward Pass\n3. Vanilla Bidirectional Pass\n4. Training of Vanilla RNN\n5. Vanishing and exploding gradient problems\n3. From Vanilla to LSTM\n1. Definition\n2. Forward Pass\n3. Backward Pass\n4. Miscellaneous\n1. More than Language Model\n2. GRU\n5. Implementing RNN in Tensorflow"
  },
  {
    "chunk_id": "doc_8_p3_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Part One\nWhy do we need Recurrent Neural Network? 1. What Problems are Normal CNNs good at? 2. What is Sequence Learning? 3. Ways to Deal with Sequence Labeling."
  },
  {
    "chunk_id": "doc_8_p4_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "1. What Problems are\nCNNs normally good at? 1. Image classification as a naive example\n1. Input: one image. 2. Output: the probability distribution of classes. 3. You need to provide one guess (output), and to do that you\nonly need to look at one image (input). P(Cat|image) = 0.1\nP(Panda|image) = 0.9"
  },
  {
    "chunk_id": "doc_8_p5_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "2. What is Sequence Learning? 1. Sequence learning is the study of machine learning algorithms\ndesigned for sequential data [1]. 2. Language model is one of the most interesting topics that use\nsequence labeling. 1. Language Translation\n1. Understand the meaning of each word, and the relationship between words\n2. Input: one sentence in German\ninput = \"Ich will stark Steuern senken\"\n3. Output: one sentence in English\noutput = \"I want to cut taxes bigly\" (big league?)"
  },
  {
    "chunk_id": "doc_8_p6_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "2. What is Sequence Learning? 1. To make it easier to understand why we need RNN, let's think\nabout a simple speaking case (let's violate neuroscience a little bit)\n1. We are given a hidden state (free mind?) that encodes all the\ninformation in the sentence we want to speak. 2. We want to generate a list of words (sentence) in an one-by-one\nfashion. 1. At each time step, we can only choose a single word. 2. The hidden state is affected by the words chosen (so we could remember what we\njust say and complete the sentence)."
  },
  {
    "chunk_id": "doc_8_p7_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "2. What is Sequence Learning? 1. Plain CNNs are not born good at length-varying input and\noutput. 1. Difficult to define input and output\n1. Remember that\n1. Input image is a 3D tensor (width, length, color channels)\n2. Output is a distribution on fixed number of classes. 2. Sequence could be:\n1. \"I know that you know that I know that you know that I know that\nyou know that I know that you know that I know that you know\nthat I know that you know that I don't know\"\n2. \"I don't know\"\n2. Input and output are strongly correlated within the sequence. 3. Still, people figured out ways to use CNN on sequence learning\n(e.g. [8])."
  },
  {
    "chunk_id": "doc_8_p8_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "3. Ways to Deal with\nSequence Labeling\n1. Autoregressive models\n1. Predict the next term in a sequence from a fixed number of\nprevious terms using delay taps. 2. Feed-forward neural nets\n1. These generalize autoregressive models by using one or more\nlayers of non-linear hidden units\nMemoryless models: limited word-memory window; hidden state\ncannot be used efficiently. materials from [2]"
  },
  {
    "chunk_id": "doc_8_p9_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "3. Ways to Deal with\nSequence Labeling\n1. Linear Dynamical Systems\n1. These are generative models. They have a real-valued hidden state\nthat cannot be observed directly. 2. Hidden Markov Models\n1. Have a discrete one-of-N hidden state. Transitions between states are\nstochastic and controlled by a transition matrix. The outputs\nproduced by a state are stochastic. Memoryful models,\ntime-cost to infer the hidden state distribution. materials from [2]"
  },
  {
    "chunk_id": "doc_8_p10_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 10,
    "chunk_type": "fixed",
    "text": "3. Ways to Deal with\nSequence Labeling\n1. Finally, the RNN model! 1. Update the hidden state in a deterministic nonlinear way. 2. In the simple speaking case, we send the chosen word back to\nthe network as input. materials from [4]"
  },
  {
    "chunk_id": "doc_8_p11_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 11,
    "chunk_type": "fixed",
    "text": "3. Ways to Deal with\nSequence Labeling\n1. RNNs are very powerful, because they:\n1. Distributed hidden state that allows them to store a lot of\ninformation about the past efficiently. 2. Non-linear dynamics that allows them to update their hidden\nstate in complicated ways. 3. No need to infer hidden state, pure deterministic. 4. Weight sharing"
  },
  {
    "chunk_id": "doc_8_p12_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 12,
    "chunk_type": "fixed",
    "text": "Part Two\nMath in a Vanilla Recurrent Neural Network\n1. Vanilla Forward Pass\n2. Vanilla Backward Pass\n3. Vanilla Bidirectional Pass\n4. Training of Vanilla RNN\n5. Vanishing and exploding gradient problems"
  },
  {
    "chunk_id": "doc_8_p13_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 13,
    "chunk_type": "fixed",
    "text": "1.Vanilla Forward Pass\n1. The forward pass of a vanilla RNN\n1. The same as that of an MLP with a single hidden layer\n2. Except that activations arrive at the hidden layer from both\nthe current external input and the hidden layer activations\none step back in time. 2. For the input to hidden units we have\n3. For the output unit we have\nmaterials from [4]"
  },
  {
    "chunk_id": "doc_8_p14_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 14,
    "chunk_type": "fixed",
    "text": "1.Vanilla Forward Pass\n1. The complete sequence of hidden activations can be\ncalculated by starting at t = 1 and recursively applying the\nthree equations, incrementing t at each step."
  },
  {
    "chunk_id": "doc_8_p15_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 15,
    "chunk_type": "fixed",
    "text": "2.Vanilla Backward Pass\n1. Given the partial derivatives of the objective function with\nrespect to the network outputs, we now need the derivatives\nwith respect to the weights. 2. We focus on BPTT since it is both conceptually simpler and\nmore efficient in computation time (though not in memory). Like standard back-propagation, BPTT consists of a\nrepeated application of the chain rule."
  },
  {
    "chunk_id": "doc_8_p16_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 16,
    "chunk_type": "fixed",
    "text": "2.Vanilla Backward Pass\n1. Back-propagation through time\n1. Don't be fooled by the fancy name. It's just the standard\nback-propagation. materials from [6]"
  },
  {
    "chunk_id": "doc_8_p17_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 17,
    "chunk_type": "fixed",
    "text": "2.Vanilla Backward Pass\n1. Back-propagation through time\n1. The complete sequence of delta terms can be calculated by\nstarting at t = T and recursively applying the below functions,\ndecrementing t at each step. 2. Note that , since no error is received from beyond the\nend of the sequence. 3. Finally, bearing in mind that the weights to and from each\nunit in the hidden layer are the same at every time-step, we\nsum over the whole sequence to get the derivatives with\nrespect to each of the network weights\nmaterials from [4]"
  },
  {
    "chunk_id": "doc_8_p18_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 18,
    "chunk_type": "fixed",
    "text": "3.Vanilla Bidirectional Pass\n1. For many sequence labeling tasks, we would like to have\naccess to future."
  },
  {
    "chunk_id": "doc_8_p19_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 19,
    "chunk_type": "fixed",
    "text": "3.Vanilla Bidirectional Pass\n1. Algorithm looks like this"
  },
  {
    "chunk_id": "doc_8_p20_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 20,
    "chunk_type": "fixed",
    "text": "4.Training of Vanilla RNN\n1. So far we have discussed how RNN can be differentiated\nwith respect to suitable objective functions, and thereby\nthey could be trained with any gradient-descent based\nalgorithm\n1. just treat them as a normal CNN\n2. One of the great things about RNN: lots of engineering\nchoices\n1. Preprocessing and postprocessing"
  },
  {
    "chunk_id": "doc_8_p21_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 21,
    "chunk_type": "fixed",
    "text": "5.Vanishing and exploding\ngradient problems\n1. Multiply the same matrix at each time step during back-\nprop\nmaterials from [3]"
  },
  {
    "chunk_id": "doc_8_p22_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 22,
    "chunk_type": "fixed",
    "text": "5.Vanishing and exploding\ngradient problems\n1. Toy example how gradient vanishes\n1. Similar but simpler RNN formulation:\n2. Solutions? 1. For vanishing gradients: Initialization + ReLus\n2. Trick for exploding gradient: clipping trick"
  },
  {
    "chunk_id": "doc_8_p23_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 23,
    "chunk_type": "fixed",
    "text": "Part Three\nFrom Vanilla to LSTM\n1. Definition\n2. Forward Pass\n3. Backward Pass"
  },
  {
    "chunk_id": "doc_8_p24_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 24,
    "chunk_type": "fixed",
    "text": "1. Definition\n1. As discussed earlier, for standard RNN architectures, the range of\ncontext that can be accessed is limited. 1. The problem is that the influence of a given input on the hidden layer,\nand therefore on the network output, either decays or blows up\nexponentially as it cycles around the network's recurrent connections. 2. The most effective solution so far is the Long Short Term Memory\n(LSTM) architecture (Hochreiter and Schmidhuber, 1997). 3. The LSTM architecture consists of a set of recurrently connected\nsubnets, known as memory blocks. These blocks can be thought of\nas a differentiable version of the memory chips in a digital\ncomputer. Each block contains one or more self-connected memory\ncells and three multiplicative units that provide continuous\nanalogues of write, read and reset operations for the cells\n1. The input, output and forget gates. materials from [4]"
  },
  {
    "chunk_id": "doc_8_p25_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 25,
    "chunk_type": "fixed",
    "text": "1. Definition\n1. The multiplicative gates allow LSTM\nmemory cells to store and access\ninformation over long periods of time,\nthereby avoiding the vanishing gradient\nproblem\n1. For example, as long as the input gate\nremains closed (i.e. has an activation\nclose to 0), the activation of the cell\nwill not be overwritten by the new\ninputs arriving in the network, and can\ntherefore be made available to the net\nmuch later in the sequence, by opening\nthe output gate."
  },
  {
    "chunk_id": "doc_8_p26_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 26,
    "chunk_type": "fixed",
    "text": "1. Definition\n1. Comparison"
  },
  {
    "chunk_id": "doc_8_p27_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 27,
    "chunk_type": "fixed",
    "text": "2. Forward Pass\n1. Basically very similar to the\nvanilla RNN forward pass\n1. But it's a lot more\ncomplicated\n2. Can you do the backward\npass by yourself? 1. Quick quiz, get a white\nsheet of paper. Write your\nstudent number and name. 2. We are going to give you 10\nminutes\n3. No discussion\n4. 10% of your final grades"
  },
  {
    "chunk_id": "doc_8_p28_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 28,
    "chunk_type": "fixed",
    "text": "3. Backward Pass\n1. Just kidding! The math to\nget the backward pass\nshould be very similar to\nthe one used in vanilla\nRNN backward pass\n1. But it's a lot more\ncomplicated, too...\n2. We are not going to derive\nthat in the class"
  },
  {
    "chunk_id": "doc_8_p29_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 29,
    "chunk_type": "fixed",
    "text": "Part Four\nMiscellaneous\n1. More than Language Model\n2. GRU"
  },
  {
    "chunk_id": "doc_8_p30_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 30,
    "chunk_type": "fixed",
    "text": "1. More than Language Model\n1. Like I said, RNN could do a lot more than modeling\nlanguage\n1. Drawing pictures:\n[9] DRAW: A Recurrent Neural Network For Image Generation\n2. Computer-composed music\n[10] Song From PI: A Musically Plausible Network for Pop Music Generation\n3. Semantic segmentation\n[11] Conditional random fields as recurrent neural networks"
  },
  {
    "chunk_id": "doc_8_p31_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 31,
    "chunk_type": "fixed",
    "text": "1. More than Language Model\n1. RNN in sports\n1. Sport is a sequence of\nevent (sequence of images,\nvoices)\n2. Detecting events and key\nactors in multi-person\nvideos [12]\n1. \"In particular, we track\npeople in videos and use a\nrecurrent neural network\n(RNN) to represent the\ntrack features. We learn\ntime-varying attention\nweights to combine these\nfeatures at each time-instant. The attended features are\nthen processed using\nanother RNN for event\ndetection/classification\""
  },
  {
    "chunk_id": "doc_8_p32_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 32,
    "chunk_type": "fixed",
    "text": "1. More than Language Model\n1. RNN in sports\n1. Applying Deep Learning to Basketball Trajectories\n1. This paper applies recurrent neural networks in the form of sequence\nmodeling to predict whether a three-point shot is successful [13]\n2. Action Classification in Soccer Videos with Long Short-Term\nMemory Recurrent Neural Networks [14]"
  },
  {
    "chunk_id": "doc_8_p33_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 33,
    "chunk_type": "fixed",
    "text": "2. GRU\n1. A new type of RNN cell (Gated Feedback Recurrent Neural\nNetworks)\n1. Very similar to LSTM\n2. It merges the cell state and hidden state. 3. It combines the forget and input gates into a single \"update\ngate\". 4. Computationally more efficient. 1. less parameters, less complex structure. 2. Gaining popularity nowadays [15,16]"
  },
  {
    "chunk_id": "doc_8_p34_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 34,
    "chunk_type": "fixed",
    "text": "Part Five\nImplementing RNN in Tensorflow"
  },
  {
    "chunk_id": "doc_8_p35_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 35,
    "chunk_type": "fixed",
    "text": "1. Implementing RNN in\nTensorflow\n1. The best way should be reading the docs on Tensorflow\nwebsite [17]. 2. Let's assume you already manage how to use CNN in\nTensorflow (toy sequence decoder model)"
  },
  {
    "chunk_id": "doc_8_p36_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 36,
    "chunk_type": "fixed",
    "text": "1. Implementing RNN in\nTensorflow\n1. Standard feed dictionary just like other CNN models in\nTensorflow\n2. Word-embedding (make the words in the sentence\nunderstandable by the program)"
  },
  {
    "chunk_id": "doc_8_p37_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 37,
    "chunk_type": "fixed",
    "text": "1. Implementing RNN in\nTensorflow\n1. Simple example using Tensorflow. 1. The task: let the robot learn the atom behavior it should do,\nby following human instructions\n2. The result we could get by using RNN. 2. Task:\n1. Input: \"Sit down on the couch and watch T.V. When you are\ndone watching television turn it off. Put the pen on the table. Toast some bread in the toaster and get a knife to put butter\non the bread while you sit down at the table.\""
  },
  {
    "chunk_id": "doc_8_p38_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 38,
    "chunk_type": "fixed",
    "text": "1. Implementing RNN in\nTensorflow\n1. Simple demo result. http://www.cs.toronto.edu/~tingwuwang/outputscript_synthetic_data_cl\nean_is_rnn_encoder_True_decoder_dim_150_model.ckpt.html"
  },
  {
    "chunk_id": "doc_8_p39_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 39,
    "chunk_type": "fixed",
    "text": "References\nMost of the materials in the slides come from the following tutorials / lecture slides:\n[1] Machine Learning I Week 14: Sequence Learning Introduction, Alex Graves, Technische Universitaet Muenchen. [2] CSC2535 2013: Advanced Machine Learning, Lecture 10: Recurrent neural networks, Geoffrey Hinton, University of Toronto. [3] CS224d Deep NLP, Lecture 8: Recurrent Neural Networks, Richard Socher, Stanford University. [4] Supervised Sequence Labelling with Recurrent Neural Networks, Alex Graves, Doktors der Naturwissenschaften (Dr. rer. nat.) genehmigten Dissertation. [5] The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy, blog About Hacker's guide to Neural\nNetworks. [6] Understanding LSTM Networks, Christopher Olah, github blog. Other references\n[7] Kiros, Ryan, et al. \"Skip-thought vectors.\" Advances in neural information processing systems. 2015. [8] Dauphin, Yann N., et al. \"Language Modeling with Gated Convolutional Networks.\" arXiv preprint arXiv:1612.08083 (2016). [9] Gregor, Karol, et al. \"DRAW: A recurrent neural network for image generation.\" arXiv preprint arXiv:1502.04623 (2015)."
  },
  {
    "chunk_id": "doc_8_p40_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 40,
    "chunk_type": "fixed",
    "text": "References\n[10] Chu, Hang, Raquel Urtasun, and Sanja Fidler. \"Song From PI: A Musically Plausible Network for Pop\nMusic Generation.\" arXiv preprint arXiv:1611.03477 (2016). [11] Zheng, Shuai, et al. \"Conditional random fields as recurrent neural networks.\" Proceedings of the IEEE\nInternational Conference on Computer Vision. 2015. [12] Ramanathan, Vignesh, et al. \"Detecting events and key actors in multi-person videos.\" arXiv preprint\narXiv:1511.02917 (2015). [13] Shah, Rajiv, and Rob Romijnders. \"Applying Deep Learning to Basketball Trajectories.\" arXiv\npreprint arXiv:1608.03793 (2016). [14] Baccouche, Moez, et al. \"Action classification in soccer videos with long short-term memory recurrent\nneural networks.\" International Conference on Artificial Neural Networks. Springer Berlin Heidelberg, 2010. [15] Chung, Junyoung, et al. \"Empirical evaluation of gated recurrent neural networks on sequence\nmodeling.\" arXiv preprint arXiv:1412.3555 (2014). [16] Chung, Junyoung, et al. \"Gated feedback recurrent neural networks.\" CoRR, abs/1502.02367 (2015). [17] Tutorials on Tensorflow. https://www.tensorflow.org/tutorials/"
  },
  {
    "chunk_id": "doc_8_p41_fixed_0",
    "doc_id": "doc_8",
    "pdf_name": "8.pdf",
    "page": 41,
    "chunk_type": "fixed",
    "text": "Q&A\nThank you for listening ;P"
  },
  {
    "chunk_id": "doc_9_p1_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "FaceNet: A Unified Embedding for Face Recognition and Clustering\nFlorianSchroff DmitryKalenichenko JamesPhilbin\nfschroff@google.com dkalenichenko@google.com jphilbin@google.com\nGoogleInc. GoogleInc. GoogleInc. Abstract\nDespite significant recent advances in the field of face\nrecognition[10,14,15,17],implementingfaceverification 1.04\nand recognition efficiently at scale presents serious chal-\nlenges to current approaches. In this paper we present a\nsystem,calledFaceNet,thatdirectlylearnsamappingfrom\n1.22 1.33\nfaceimagestoacompactEuclideanspacewheredistances\ndirectly correspond to a measure of face similarity. Once\nthis space has been produced, tasks such as face recogni-\ntion,verificationandclusteringcanbeeasilyimplemented 0.78\nusingstandardtechniqueswithFaceNetembeddingsasfea-\nturevectors. Our method uses a deep convolutional network trained\n1.33 1.26\ntodirectlyoptimizetheembeddingitself,ratherthananin-\ntermediate bottleneck layer as in previous deep learning\napproaches. To train, we use triplets of roughly aligned\nmatching / non-matching face patches generated using a 0.99\nnovel online triplet mining method. The benefit of our\napproach is much greater representational efficiency: we\nachievestate-of-the-artfacerecognitionperformanceusing\nonly128-bytesperface. Figure1.IlluminationandPoseinvariance. Poseandillumina-\nOn the widely used Labeled Faces in the Wild (LFW)\ntionhavebeenalongstandingprobleminfacerecognition."
  },
  {
    "chunk_id": "doc_9_p1_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "This\ndataset, our system achieves a new record accuracy of\nfigure shows the output distances of FaceNet between pairs of\n99.63%. On YouTube Faces DB it achieves 95.12%. Our\nfacesofthesameandadifferentpersonindifferentposeandil-\nsystem cuts the error rate in comparison to the best pub- luminationcombinations. Adistanceof0.0meansthefacesare\nlishedresult[15]by30%onbothdatasets. identical,4.0correspondstotheoppositespectrum,twodifferent\nidentities.Youcanseethatathresholdof1.1wouldclassifyevery\n1.Introduction paircorrectly. In this paper we present a unified system for face veri-\nfication (is this the same person), recognition (who is this tionproblem;andclusteringcanbeachievedusingoff-the-\nperson) and clustering (find common people among these shelftechniquessuchask-meansoragglomerativecluster-\nfaces). Our method is based on learning a Euclidean em- ing. beddingperimageusingadeepconvolutionalnetwork.The Previousfacerecognitionapproachesbasedondeepnet-\nnetwork is trained such that the squared L2 distances in worksuseaclassificationlayer[15,17]trainedoverasetof\ntheembeddingspacedirectlycorrespondtofacesimilarity: knownfaceidentitiesandthentakeanintermediatebottle-\nfacesofthesamepersonhavesmalldistancesandfacesof necklayerasarepresentationusedtogeneralizerecognition\ndistinctpeoplehavelargedistances. beyondthesetofidentitiesusedintraining."
  },
  {
    "chunk_id": "doc_9_p1_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 1,
    "chunk_type": "fixed",
    "text": "Thedownsides\nOncethisembeddinghasbeenproduced,thentheafore- ofthisapproachareitsindirectnessanditsinefficiency:one\nmentioned tasks become straight-forward: face verifica- has to hope that the bottleneck representation generalizes\ntionsimplyinvolvesthresholdingthedistancebetweenthe welltonewfaces; andbyusingabottlenecklayertherep-\ntwo embeddings; recognition becomes a k-NN classifica- resentationsizeperfaceisusuallyverylarge(1000sofdi-\n1"
  },
  {
    "chunk_id": "doc_9_p2_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "mensions). Somerecentwork[15]hasreducedthisdimen- numberofparametersbyupto20timesandhavethepoten-\nsionalityusingPCA,butthisisalineartransformationthat tialtoreducethenumberofFLOPSrequiredforcomparable\ncanbeeasilylearntinonelayerofthenetwork. performance. In contrast to these approaches, FaceNet directly trains Thereisavastcorpusoffaceverificationandrecognition\nitsoutputtobeacompact128-Dembeddingusingatriplet- works. Reviewingitisoutofthescopeofthispapersowe\nbasedlossfunctionbasedonLMNN[19]. Ourtripletscon- willonlybrieflydiscussthemostrelevantrecentwork. sist of two matching face thumbnails and a non-matching Theworksof[15,17,23]allemployacomplexsystem\nfacethumbnailandthelossaimstoseparatethepositivepair ofmultiplestages,thatcombinestheoutputofadeepcon-\nfromthenegativebyadistancemargin. Thethumbnailsare volutionalnetworkwithPCAfordimensionalityreduction\ntight crops of the face area, no 2D or 3D alignment, other andanSVMforclassification. thanscaleandtranslationisperformed. Zhenyao et al. [23] employ a deep network to “warp”\nChoosing which triplets to use turns out to be very im- facesintoacanonicalfrontalviewandthenlearnCNNthat\nportant for achieving good performance and, inspired by classifies each face as belonging to a known identity."
  },
  {
    "chunk_id": "doc_9_p2_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "For\ncurriculum learning [1], we present a novel online nega- faceverification,PCAonthenetworkoutputinconjunction\ntive exemplar mining strategy which ensures consistently withanensembleofSVMsisused. increasing difficulty of triplets as the network trains. To Taigmanetal. [17]proposeamulti-stageapproachthat\nimproveclusteringaccuracy,wealsoexplorehard-positive alignsfacestoageneral3Dshapemodel.Amulti-classnet-\nmining techniques which encourage spherical clusters for workistrainedtoperformthefacerecognitiontaskonover\ntheembeddingsofasingleperson. four thousand identities. The authors also experimented\nAs an illustration of the incredible variability that our withasocalledSiamesenetworkwheretheydirectlyopti-\nmethod can handle see Figure 1. Shown are image pairs mizetheL -distancebetweentwofacefeatures. Theirbest\n1\nfrom PIE [13] that previously were considered to be very performanceonLFW(97.35%)stemsfromanensembleof\ndifficultforfaceverificationsystems. three networks using different alignments and color chan-\nAn overview of the rest of the paper is as follows: in nels. Thepredicteddistances(non-linearSVMpredictions\nsection 2 we review the literature in this area; section 3.1 basedontheχ2kernel)ofthosenetworksarecombinedus-\ndefines the triplet loss and section 3.2 describes our novel inganon-linearSVM. triplet selection and training procedure; in section 3.3 we Sunetal. [14,15]proposeacompactandthereforerel-\ndescribe the model architecture used. Finally in section 4 atively cheap to compute network."
  },
  {
    "chunk_id": "doc_9_p2_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "They use an ensemble\nand 5 we present some quantitative results of our embed- of 25 of these network, each operating on a different face\ndingsandalsoqualitativelyexploresomeclusteringresults. patch. For their final performance on LFW (99.47% [15])\nthe authors combine 50 responses (regular and flipped). 2.RelatedWork Both PCA and a Joint Bayesian model [2] that effectively\ncorrespondtoalineartransformintheembeddingspaceare\nSimilarlytootherrecentworkswhichemploydeepnet-\nemployed. Their method does not require explicit 2D/3D\nworks[15,17],ourapproachisapurelydatadrivenmethod\nalignment. The networks are trained by using a combina-\nwhich learns its representation directly from the pixels of\ntionofclassificationandverificationloss. Theverification\nthe face. Rather than using engineered features, we use a\nlossissimilartothetripletlossweemploy[12,19],inthatit\nlarge dataset of labelled faces to attain the appropriate in-\nminimizestheL -distancebetweenfacesofthesameiden-\n2\nvariancestopose,illumination,andothervariationalcondi-\ntityandenforcesamarginbetweenthedistanceoffacesof\ntions. differentidentities.Themaindifferenceisthatonlypairsof\nInthis paperweexplore twodifferentdeepnetwork ar-\nimagesarecompared,whereasthetripletlossencouragesa\nchitecturesthathavebeenrecentlyusedtogreatsuccessin\nrelativedistanceconstraint. the computer vision community."
  },
  {
    "chunk_id": "doc_9_p2_fixed_3",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 2,
    "chunk_type": "fixed",
    "text": "Both are deep convolu-\nA similar loss to the one used here was explored in\ntionalnetworks[8,11].Thefirstarchitectureisbasedonthe\nWangetal. [18]forrankingimagesbysemanticandvisual\nZeiler&Fergus[22]modelwhichconsistsofmultipleinter-\nsimilarity. leaved layers of convolutions, non-linear activations, local\nresponsenormalizations,andmaxpoolinglayers. Weaddi-\n3.Method\ntionallyaddseveral1×1×dconvolutionlayersinspiredby\nthe work of [9]. The second architecture is based on the FaceNetusesadeepconvolutionalnetwork. Wediscuss\nInceptionmodelofSzegedyetal.whichwasrecentlyused two different core architectures: The Zeiler&Fergus [22]\nas the winning approach for ImageNet 2014 [16]. These stylenetworksandtherecentInception[16]typenetworks. networksusemixedlayersthatrunseveraldifferentconvo- Thedetailsofthesenetworksaredescribedinsection3.3. lutionalandpoolinglayersinparallelandconcatenatetheir Given the model details, and treating it as a black box\nresponses. Wehavefoundthatthesemodelscanreducethe (seeFigure2),themostimportantpartofourapproachlies"
  },
  {
    "chunk_id": "doc_9_p3_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "M E where α is a margin that is enforced between positive and\n... DEEP ARCHITECTURE L2 D D B E T L r o ip s le s t negative pairs. T is the set of all possible triplets in the\nN I trainingsetandhascardinalityN. Batch G\nThelossthatisbeingminimizedisthenL=\nFigure2.Modelstructure. Ournetworkconsistsofabatchin-\np re u s t u l l a ts ye in ra th n e d f a ac d e ee e p mb C e N d N din f g o . llo T w hi e s d is by fo L llo 2 w n e o d rm by ali t z h a e ti t o ri n p , le w t h lo ic s h s (cid:88) N (cid:104) (cid:107)f(xa)−f(xp)(cid:107)2−(cid:107)f(xa)−f(xn)(cid:107)2+α (cid:105) . i i 2 i i 2\nduringtraining. i +\n(2)\nGenerating all possible triplets would result in many\nNegative\nAnchor triplets that are easily satisfied (i.e. fulfill the constraint\nLEARNING\nNegative inEq.(1)). Thesetripletswouldnotcontributetothetrain-\nAnchor ing and result in slower convergence, as they would still\nPositive Positive\nbe passed through the network. It is crucial to select hard\nFigure3.TheTripletLossminimizesthedistancebetweenanan-\ntriplets, that are active and can therefore contribute to im-\nchor and a positive, both of which have the same identity, and\nproving the model."
  },
  {
    "chunk_id": "doc_9_p3_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "The following section talks about the\nmaximizes the distance between the anchor and a negative of a\ndifferentapproachesweuseforthetripletselection. differentidentity. 3.2.TripletSelection\nintheend-to-endlearningofthewholesystem. Tothisend Inordertoensurefastconvergenceitiscrucialtoselect\nweemploythetripletlossthatdirectlyreflectswhatwewant triplets that violate the triplet constraint in Eq. (1). This\nto achieve in face verification, recognition and clustering. means that, given xa, we want to select an xp (hard pos-\ni i\nN x a i m nt e o ly a , w fe e at s u t r r e iv s e p f a o c r e a R n d e , m s b u e c d h d t i h n a g t f th ( e x) s , q f u r a o r m ed a d n is im ta a n g c e e i x ti n ve (h ) a s r u d ch ne t g h a at tiv a e r ) gm su a c x h x t p i ha (cid:107) t f a ( r x g a i m )− in f(x (cid:107)f p i ) ( (cid:107) x 2 2 a) an − d f s ( im xn il ) a (cid:107) r 2 ly . between all faces, independent of imaging conditions, of i xn i i i 2\nIt is infeasible to compute the argmin and argmax\nthesameidentityissmall,whereasthesquareddistancebe-\nacross the whole training set."
  },
  {
    "chunk_id": "doc_9_p3_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "Additionally, it might lead\ntweenapairoffaceimagesfromdifferentidentitiesislarge. to poor training, as mislabelled and poorly imaged faces\nAlthough we did not a do direct comparison to other\nwoulddominatethehardpositivesandnegatives. Thereare\nlosses,e.g.theoneusingpairsofpositivesandnegatives,as\ntwoobviouschoicesthatavoidthisissue:\nusedin[14]Eq. (2),webelievethatthetripletlossismore\nsuitableforfaceverification. Themotivationisthattheloss • Generate triplets offline every n steps, using the most\nfrom[14]encouragesallfacesofoneidentitytobeâA˘Ÿpro- recentnetworkcheckpointandcomputingtheargmin\njectedâA˘Z´ ontoasinglepointintheembeddingspace. The andargmaxonasubsetofthedata. tripletloss,however,triestoenforceamarginbetweeneach\n• Generate triplets online. This can be done by select-\npair of faces from one person to all other faces. This al-\ningthehardpositive/negativeexemplarsfromwithina\nlowsthefacesforoneidentitytoliveonamanifold,while\nmini-batch. stillenforcingthedistanceandthusdiscriminabilitytoother\nidentities. Here, we focus on the online generation and use large\nThefollowingsectiondescribesthistripletlossandhow mini-batchesintheorderofafewthousandexemplarsand\nitcanbelearnedefficientlyatscale. onlycomputetheargminandargmaxwithinamini-batch."
  },
  {
    "chunk_id": "doc_9_p3_fixed_3",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "To have a meaningful representation of the anchor-\n3.1.TripletLoss\npositive distances, it needs to be ensured that a minimal\nThe embedding is represented by f(x) ∈ Rd. It em- numberofexemplarsofanyoneidentityispresentineach\nbeds an image x into a d-dimensional Euclidean space. mini-batch. Inourexperimentswesamplethetrainingdata\nAdditionally, we constrain this embedding to live on the suchthataround40facesareselectedperidentitypermini-\nd-dimensionalhypersphere, i.e. (cid:107)f(x)(cid:107) = 1. Thislossis batch. Additionally, randomly sampled negative faces are\n2\nmotivatedin[19]inthecontextofnearest-neighborclassifi- addedtoeachmini-batch. cation.Herewewanttoensurethatanimagexa(anchor)of Insteadofpickingthehardestpositive,weuseallanchor-\ni\na specificperson is closerto all other images xp (positive) positive pairs in a mini-batch while still selecting the hard\ni\nofthesamepersonthanitistoanyimagexn (negative)of negatives. Wedon’thaveaside-by-sidecomparisonofhard\ni\nanyotherperson. ThisisvisualizedinFigure3. anchor-positivepairsversusallanchor-positivepairswithin\nThuswewant, a mini-batch, but we found in practice that the all anchor-\npositive method was more stable and converged slightly\n(cid:107)xa−xp(cid:107)2+α<(cid:107)xa−xn(cid:107)2, ∀(xa,xp,xn)∈T ."
  },
  {
    "chunk_id": "doc_9_p3_fixed_4",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 3,
    "chunk_type": "fixed",
    "text": "(1) fasteratthebeginningoftraining. i i 2 i i 2 i i i"
  },
  {
    "chunk_id": "doc_9_p4_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Wealsoexploredtheofflinegenerationoftripletsincon- layer size-in size-out kernel paramFLPS\njunctionwiththeonlinegenerationanditmayallowtheuse conv1 220×220×3 110×110×64 7×7×3,2 9K 115M\nof smaller batch sizes, but the experiments were inconclu- pool1 110×110×64 55×55×64 3×3×64,2 0\nsive. rnorm1 55×55×64 55×55×64 0\nSelectingthehardestnegativescaninpracticeleadtobad conv2a 55×55×64 55×55×64 1×1×64,1 4K 13M\nlocal minima early on in training, specifically it can result conv2 55×55×64 55×55×192 3×3×64,1 111K 335M\nrnorm2 55×55×192 55×55×192 0\nin a collapsed model (i.e. f(x) = 0). In order to mitigate\npool2 55×55×192 28×28×192 3×3×192,2 0\nthis,ithelpstoselectxnsuchthat\ni conv3a 28×28×192 28×28×192 1×1×192,1 37K 29M\n(cid:107)f(xa)−f(xp)(cid:107)2 <(cid:107)f(xa)−f(xn)(cid:107)2 ."
  },
  {
    "chunk_id": "doc_9_p4_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "(3) conv3 28×28×192 28×28×384 3×3×192,1 664K 521M\ni i 2 i i 2 pool3 28×28×384 14×14×384 3×3×384,2 0\nWecallthesenegativeexemplarssemi-hard,astheyarefur- conv4a 14×14×384 14×14×384 1×1×384,1 148K 29M\nconv4 14×14×384 14×14×256 3×3×384,1 885K 173M\nther away from the anchor than the positive exemplar, but\nconv5a 14×14×256 14×14×256 1×1×256,1 66K 13M\nstillhardbecausethesquareddistanceisclosetotheanchor-\nconv5 14×14×256 14×14×256 3×3×256,1 590K 116M\npositivedistance. Thosenegativeslieinsidethemarginα. conv6a 14×14×256 14×14×256 1×1×256,1 66K 13M\nAs mentioned before, correct triplet selection is crucial\nconv6 14×14×256 14×14×256 3×3×256,1 590K 116M\nforfastconvergence. Ontheonehandwewouldliketouse\npool4 14×14×256 7×7×256 3×3×256,2 0\nsmall mini-batches as these tend to improve convergence concat 7×7×256 7×7×256 0\nduring Stochastic Gradient Descent (SGD) [20]."
  },
  {
    "chunk_id": "doc_9_p4_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "On the fc1 7×7×256 1×32×128 maxoutp=2 103M 103M\notherhand,implementationdetailsmakebatchesoftensto fc2 1×32×128 1×32×128 maxoutp=2 34M 34M\nhundredsofexemplarsmoreefficient. Themainconstraint fc7128 1×32×128 1×1×128 524K 0.5M\nwithregardstothebatchsize,however,isthewayweselect L2 1×1×128 1×1×128 0\nhardrelevanttripletsfromwithinthemini-batches. Inmost total 140M 1.6B\nexperimentsweuseabatchsizeofaround1,800exemplars. Table 1. NN1. This table show the structure of our\n3.3.DeepConvolutionalNetworks Zeiler&Fergus [22] based model with 1×1 convolutions in-\nspired by [9]. The input and output sizes are described\nInallourexperimentswetraintheCNNusingStochastic\nin rows×cols×#filters. The kernel is specified as\nGradientDescent(SGD)withstandardbackprop[8,11]and rows×cols,strideandthemaxout[6]poolingsizeasp=2. AdaGrad[5]. Inmostexperimentswestartwithalearning\nrateof0.05whichwelowertofinalizethemodel.Themod-\nelsareinitializedfromrandom,similarto[16],andtrained (between 500M-1.6B). Some of these models are dramati-\nonaCPUclusterfor1,000to2,000hours."
  },
  {
    "chunk_id": "doc_9_p4_fixed_3",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "Thedecreasein callyreducedinsize(bothdepthandnumberoffilters),so\nthe loss (and increase in accuracy) slows down drastically that they can be run on a mobile phone. One, NNS1, has\nafter 500h of training, but additional training can still sig- 26Mparametersandonlyrequires220MFLOPSperimage. nificantlyimproveperformance. Themarginαissetto0.2. The other, NNS2, has 4.3M parameters and 20M FLOPS. We used two types of architectures and explore their Table 2 describes NN2 our largest network in detail. NN3\ntrade-offsinmoredetailintheexperimentalsection. Their is identical in architecture but has a reduced input size of\npracticaldifferenceslieinthedifferenceofparametersand 160x160. NN4 has an input size of only 96x96, thereby\nFLOPS.Thebestmodelmaybedifferentdependingonthe drastically reducing the CPU requirements (285M FLOPS\napplication. E.g.amodelrunninginadatacentercanhave vs 1.6B for NN2). In addition to the reduced input size it\nmany parameters and require a large number of FLOPS, does not use 5x5 convolutions in the higher layers as the\nwhereasamodelrunningonamobilephoneneedstohave receptive field is already too small by then. Generally we\nfew parameters, so that it can fit into memory. All our foundthatthe5x5convolutionscanberemovedthroughout\nmodelsuserectifiedlinearunitsasthenon-linearactivation withonlyaminordropinaccuracy. Figure4comparesall\nfunction."
  },
  {
    "chunk_id": "doc_9_p4_fixed_4",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 4,
    "chunk_type": "fixed",
    "text": "ourmodels. The first category, shown in Table 1, adds 1×1×d con-\nvolutionallayers,assuggestedin[9],betweenthestandard 4.DatasetsandEvaluation\nconvolutionallayersoftheZeiler&Fergus[22]architecture\nandresultsinamodel22layersdeep. Ithasatotalof140 Weevaluateourmethodonfourdatasetsandwiththeex-\nmillion parameters and requires around 1.6 billion FLOPS ception of Labelled Faces in the Wild and YouTube Faces\nperimage. we evaluate our method on the face verification task. I.e. The second category we use is based on GoogLeNet given a pair of two face images a squared L distance\n2\nstyleInceptionmodels[16]. Thesemodelshave20×fewer threshold D(x ,x ) is used to determine the classification\ni j\nparameters(around6.6M-7.5M)andupto5×fewerFLOPS ofsameanddifferent.Allfacespairs(i,j)ofthesameiden-"
  },
  {
    "chunk_id": "doc_9_p5_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": ""
  },
  {
    "chunk_id": "doc_9_p5_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "output #3×3 #5×5 pool\ntype depth #1×1 #3×3 #5×5 params FLOPS\nsize reduce reduce proj(p)\nconv1(7×7×3,2) 112×112×64 1 9K 119M\nmaxpool+norm 56×56×64 0 m3×3,2\ninception(2) 56×56×192 2 64 192 115K 360M\nnorm+maxpool 28×28×192 0 m3×3,2\ninception(3a) 28×28×256 2 64 96 128 16 32 m,32p 164K 128M\ninception(3b) 28×28×320 2 64 96 128 32 64 L ,64p 228K 179M\n2\ninception(3c) 14×14×640 2 0 128 256,2 32 64,2 m3×3,2 398K 108M\ninception(4a) 14×14×640 2 256 96 192 32 64 L ,128p 545K 107M\n2\ninception(4b) 14×14×640 2 224 112 224 32 64 L ,128p 595K 117M\n2\ninception(4c) 14×14×640 2 192 128 256 32 64 L ,128p 654K 128M\n2\ninception(4d) 14×14×640 2 160 144 288 32 64 L ,128p 722K 142M\n2\ninception(4e) 7×7×1024 2 0 160 256,2 64 128,2 m3×3,2 717K 56M\ninception(5a) 7×7×1024 2 384 192 384 48 128 L ,128p 1.6M 78M\n2\ninception(5b) 7×7×1024 2 384 192 384 48 128 m,128p 1.6M 78M\navgpool 1×1×1024 0\nfullyconn 1×1×128 1 131K 0.1M\nL2normalization 1×1×128 0\ntotal 7.5M 1.6B\nTable2.NN2."
  },
  {
    "chunk_id": "doc_9_p5_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "DetailsoftheNN2Inceptionincarnation. Thismodelisalmostidenticaltotheonedescribedin [16]. Thetwomajor\ndifferencesaretheuseofL poolinginsteadofmaxpooling(m),wherespecified.Thepoolingisalways3×3(asidefromthefinalaverage\n2\npooling)andinparalleltotheconvolutionalmodulesinsideeachInceptionmodule.Ifthereisadimensionalityreductionafterthepooling\nitisdenotedwithp.1×1,3×3,and5×5poolingarethenconcatenatedtogetthefinaloutput. tity are denoted with P , whereas all pairs of different Itconsistsofthreepersonalphotocollectionswithatotalof\nsame\nidentitiesaredenotedwithP . around 12k images. We compute the FAR and VAL rate\ndiff\nWedefinethesetofalltrueacceptsas acrossall12ksquaredpairsofimages. TA(d)={(i,j)∈P ,withD(x ,x )≤d}. (4) 4.3.AcademicDatasets\nsame i j\nLabeled Faces in the Wild (LFW) is the de-facto aca-\nThesearethefacepairs(i,j)thatwerecorrectlyclassified\ndemictestsetforfaceverification[7]. Wefollowthestan-\nassameatthresholdd."
  },
  {
    "chunk_id": "doc_9_p5_fixed_3",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "Similarly\ndardprotocolforunrestricted,labeledoutsidedataandre-\nFA(d)={(i,j)∈P ,withD(x ,x )≤d} (5) portthemeanclassificationaccuracyaswellasthestandard\ndiff i j\nerrorofthemean. isthesetofallpairsthatwasincorrectlyclassifiedassame\nYoutubeFacesDB[21]isanewdatasetthathasgained\n(falseaccept). popularityinthefacerecognitioncommunity[17,15]. The\nThe validation rate VAL(d) and the false accept rate\nsetup is similar to LFW, but instead of verifying pairs of\nFAR(d)foragivenfacedistancedarethendefinedas\nimages,pairsofvideosareused. |TA(d)| |FA(d)|\nVAL(d)= , FAR(d)= . (6) 5.Experiments\n|P | |P |\nsame diff\nIfnotmentionedotherwiseweusebetween100M-200M\n4.1.Hold-outTestSet\ntraining face thumbnails consisting of about 8M different\nWe keep a hold out set of around one million images, identities. Afacedetectorisrunoneachimageandatight\nthat has the same distribution as our training set, but dis- bounding box around each face is generated. These face\njoint identities. For evaluation we split it into five disjoint thumbnails are resized to the input size of the respective\nsetsof200kimageseach. TheFARandVALratearethen network. Input sizes range from 96x96 pixels to 224x224\ncomputed on 100k×100k image pairs. Standard error is pixelsinourexperiments. reportedacrossthefivesplits."
  },
  {
    "chunk_id": "doc_9_p5_fixed_4",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 5,
    "chunk_type": "fixed",
    "text": "5.1.ComputationAccuracyTrade-off\n4.2.PersonalPhotos\nBefore diving into the details of more specific experi-\nThisisatestsetwithsimilardistributiontoourtraining mentsletâA˘Z´sdiscussthetrade-offofaccuracyversusnum-\nset,buthasbeenmanuallyverifiedtohaveverycleanlabels. ber of FLOPS that a particular model requires. Figure 4"
  },
  {
    "chunk_id": "doc_9_p6_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "90.0%\n80.0%\n70.0%\n60.0%\n50.0%\n40.0%\n30.0%\n20.0%\n10,000,000 100,000,000 1,000,000,000\nMulti­Add (FLOPS)\nRAF\n3­01@\nLAV\nNN2 NN2 NN1 NNS1 NNS2\nNN3 111EEE000\nNNS1\nNN1\nNN4\n555EEE­­­111 NNS2\n111EEE­­­111\n111EEE­­­666 111EEE­­­555 111EEE­­­444 111EEE­­­333 111EEE­­­222 111EEE­­­111 111EEE000\nFigure4.FLOPSvs.Accuracytrade-off. Shownisthetrade-off FFFAAARRR\nbetweenFLOPSandaccuracyforawiderangeofdifferentmodel\nsizesandarchitectures. Highlightedarethefourmodelsthatwe\nfocusoninourexperiments. architecture VAL\nNN1(Zeiler&Fergus220×220) 87.9%±1.9\nNN2(Inception224×224) 89.4%±1.6\nNN3(Inception160×160) 88.3%±1.7\nNN4(Inception96×96) 82.0%±2.3\nNNS1(miniInception165×165) 82.4%±2.4\nNNS2(tinyInception140×116) 51.9%±2.9\nTable 3. Network Architectures. This table compares the per-\nformanceofourmodelarchitecturesontheholdouttestset(see\nsection4.1). ReportedisthemeanvalidationrateVALat10E-3\nfalse accept rate."
  },
  {
    "chunk_id": "doc_9_p6_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "Also shown is the standard error of the mean\nacrossthefivetestsplits. shows the FLOPS on the x-axis and the accuracy at 0.001\nfalse accept rate (FAR) on our user labelled test-data set\nfrom section 4.2. It is interesting to see the strong corre-\nlation between the computation a model requires and the\naccuracyitachieves. Thefigurehighlightsthefivemodels\n(NN1, NN2, NN3, NNS1, NNS2) that we discuss in more\ndetailinourexperiments. Wealsolookedintotheaccuracytrade-offwithregards\nto the number of model parameters. However, the picture\nis not as clear in that case. For example, the Inception\nbased model NN2 achieves a comparable performance to\nNN1, but only has a 20th of the parameters. The number\nofFLOPSiscomparable,though. Obviouslyatsomepoint\nthe performance is expected to decrease, if the number of\nparameters is reduced further. Other model architectures\nmayallowfurtherreductionswithoutlossofaccuracy,just\nlikeInception[16]didinthiscase. LLLAAAVVV\nFigure 5. Network Architectures. This plot shows the com-\nplete ROC for the four different models on our personal pho-\ntos test set from section 4.2. The sharp drop at 10E-4 FAR\ncan be explained by noise in the groundtruth labels."
  },
  {
    "chunk_id": "doc_9_p6_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "The mod-\nelsinorderofperformanceare: NN2:224×224inputInception\nbasedmodel;NN1:Zeiler&Fergusbasednetworkwith1×1con-\nvolutions; NNS1:small Inception style model with only 220M\nFLOPS;NNS2:tinyInceptionmodelwithonly20MFLOPS. 5.2.EffectofCNNModel\nWe now discuss the performance of our four selected\nmodelsinmoredetail. Ontheonehandwehaveourtradi-\ntionalZeiler&Fergusbasedarchitecturewith1×1convolu-\ntions[22,9](seeTable1).OntheotherhandwehaveIncep-\ntion [16] based models that dramatically reduce the model\nsize. Overall, in the final performance the top models of\nbotharchitecturesperformcomparably. However, someof\nourInceptionbasedmodels,suchasNN3,stillachievegood\nperformance while significantly reducing both the FLOPS\nandthemodelsize. Thedetailedevaluationonourpersonalphotostestsetis\nshowninFigure5. Whilethelargestmodelachievesadra-\nmaticimprovementinaccuracycomparedtothetinyNNS2,\nthe latter can be run 30ms / image on a mobile phone and\nis still accurate enough to be used in face clustering. The\nsharp drop in the ROC for FAR < 10−4 indicates noisy\nlabels in the test data groundtruth. At extremely low false\nacceptratesasinglemislabeledimagecanhaveasignificant\nimpactonthecurve. 5.3.SensitivitytoImageQuality\nTable4showstherobustnessofourmodelacrossawide\nrange of image sizes."
  },
  {
    "chunk_id": "doc_9_p6_fixed_3",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 6,
    "chunk_type": "fixed",
    "text": "The network is surprisingly robust\nwith respect to JPEG compression and performs very well\ndown to a JPEG quality of 20. The performance drop is\nvery small for face thumbnails down to a size of 120x120\npixelsandevenat80x80pixelsitshowsacceptableperfor-"
  },
  {
    "chunk_id": "doc_9_p7_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "jpegq val-rate #trainingimages VAL\n#pixels val-rate\n10 67.3% 2,600,000 76.3%\n1,600 37.8%\n20 81.4% 26,000,000 85.1%\n6,400 79.5%\n30 83.9% 52,000,000 85.1%\n14,400 84.5%\n50 85.5% 260,000,000 86.2%\n25,600 85.7%\n70 86.1%\n65,536 86.4%\n90 86.5% Table6.TrainingDataSize.Thistablecomparestheperformance\nafter700hoftrainingforasmallermodelwith96x96pixelinputs. Table4.ImageQuality. Thetableontheleftshowstheeffecton ThemodelarchitectureissimilartoNN2,butwithoutthe5x5con-\nthevalidationrateat10E-3precisionwithvaryingJPEGquality. volutionsintheInceptionmodules. Theoneontherightshowshowtheimagesizeinpixelseffectsthe\nvalidationrateat10E-3precision.Thisexperimentwasdonewith Falseaccept\nNN1onthefirstsplitofourtesthold-outdataset. #dims VAL\n64 86.8%±1.7\n128 87.9%±1.9\n256 87.7%±1.9\n512 85.6%±2.0\nTable 5. Embedding Dimensionality. This Table compares the\neffectoftheembeddingdimensionalityofourmodelNN1onour\nhold-out set from section 4.1."
  },
  {
    "chunk_id": "doc_9_p7_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "In addition to the VAL at 10E-3 Falsereject\nwealsoshowthestandarderrorofthemeancomputedacrossfive\nsplits. mance. Thisisnotable,becausethenetworkwastrainedon\n220x220inputimages.Trainingwithlowerresolutionfaces\ncouldimprovethisrangefurther. 5.4.EmbeddingDimensionality\nWeexploredvariousembeddingdimensionalitiesandse-\nlected128forallexperimentsotherthanthecomparisonre-\nportedinTable5. Onewouldexpectthelargerembeddings\ntoperformatleastasgoodasthesmallerones,however,itis\npossiblethattheyrequiremoretrainingtoachievethesame\naccuracy. Thatsaid, thedifferencesintheperformancere-\nportedinTable5arestatisticallyinsignificant. Itshouldbenoted,thatduringtraininga128dimensional\nfloat vector is used, but it can be quantized to 128-bytes Figure6.LFWerrors. Thisshowsallpairsofimagesthatwere\nwithoutlossofaccuracy. Thuseachfaceiscompactlyrep- incorrectlyclassifiedonLFW. resented by a 128 dimensional byte vector, which is ideal\nforlargescaleclusteringandrecognition. Smallerembed-\ndingsarepossibleataminorlossofaccuracyandcouldbe relative reduction in error is 60%. Using another order of\nemployedonmobiledevices. magnitudemoreimages(hundredsofmillions)stillgivesa\nsmallboost,buttheimprovementtapersoff."
  },
  {
    "chunk_id": "doc_9_p7_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 7,
    "chunk_type": "fixed",
    "text": "5.5.AmountofTrainingData\n5.6.PerformanceonLFW\nTable 6 shows the impact of large amounts of training\ndata. Due to time constraints this evaluation was run on a WeevaluateourmodelonLFWusingthestandardpro-\nsmallermodel;theeffectmaybeevenlargeronlargermod- tocol for unrestricted, labeled outside data. Nine training\nels.Itisclearthatusingtensofmillionsofexemplarsresults splits are used to select the L -distance threshold. Classi-\n2\nin a clear boost of accuracy on our personal photo test set fication (same or different) is then performed on the tenth\nfromsection4.2. Comparedtoonlymillionsofimagesthe testsplit.Theselectedoptimalthresholdis1.242foralltest"
  },
  {
    "chunk_id": "doc_9_p8_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "splitsexceptspliteighth(1.256). Ourmodelisevaluatedintwomodes:\n1. FixedcentercropoftheLFWprovidedthumbnail. 2. Aproprietaryfacedetector(similartoPicasa[3])isrun\nontheprovidedLFWthumbnails.Ifitfailstoalignthe\nface(thishappensfortwoimages),theLFWalignment\nisused. Figure6givesanoverviewofallfailurecases. Itshows\nfalse accepts on the top as well as false rejects at the bot-\ntom. Weachieveaclassificationaccuracyof98.87%±0.15\nwhen using the fixed center crop described in (1) and the\nrecord breaking 99.63%±0.09 standard error of the mean\nwhen using the extra face alignment (2). This reduces the\nerror reported for DeepFace in [17] by more than a factor\nof7andthepreviousstate-of-the-artreportedforDeepId2+\nin[15]by30%. ThisistheperformanceofmodelNN1,but\neven the much smaller NN3 achieves performance that is\nnotstatisticallysignificantlydifferent. 5.7.PerformanceonYoutubeFacesDB\nWeusetheaveragesimilarityofallpairsofthefirstone\nhundredframesthatourfacedetectordetectsineachvideo. This gives us a classification accuracy of 95.12%±0.39. Using the first one thousand frames results in 95.18%. Compared to [17] 91.4% who also evaluate one hundred\nframes per video we reduce the error rate by almost half."
  },
  {
    "chunk_id": "doc_9_p8_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "DeepId2+[15]achieved93.2%andourmethodreducesthis\nerrorby30%,comparabletoourimprovementonLFW. 5.8.FaceClustering\nOurcompactembeddinglendsitselftobeusedinorder\ntoclusterauserspersonalphotosintogroupsofpeoplewith\nthe same identity. The constraints in assignment imposed\nbyclusteringfaces, comparedtothepureverificationtask,\nleadtotrulyamazingresults. Figure7showsoneclusterin\nauserspersonalphotocollection,generatedusingagglom-\nerative clustering. It is a clear showcase of the incredible\nFigure7.FaceClustering. Shownisanexemplarclusterforone\ninvariancetoocclusion,lighting,poseandevenage. user. Alltheseimagesintheuserspersonalphotocollectionwere\nclusteredtogether. 6.Summary\nWeprovideamethodtodirectlylearnanembeddinginto\nexperimentedwithasimilaritytransformalignmentandno-\nan Euclidean space for face verification. This sets it apart\ntice that this can actually improve performance slightly. It\nfrom other methods [15, 17] who use the CNN bottleneck\nisnotclearifitisworththeextracomplexity. layer,orrequireadditionalpost-processingsuchasconcate-\nnation of multiple models and PCA, as well as SVM clas- Future work will focus on better understanding of the\nsification. Ourend-to-endtrainingbothsimplifiesthesetup error cases, further improving the model, and also reduc-\nandshowsthatdirectlyoptimizingalossrelevanttothetask ing model size and reducing CPU requirements."
  },
  {
    "chunk_id": "doc_9_p8_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 8,
    "chunk_type": "fixed",
    "text": "We will\nathandimprovesperformance. also look into ways of improving the currently extremely\nAnother strength of our model is that it only requires longtrainingtimes,e.g.variationsofourcurriculumlearn-\nminimal alignment (tight crop around the face area). [17], ing with smaller batch sizes and offline as well as online\nfor example, performs a complex 3D alignment. We also positiveandnegativemining."
  },
  {
    "chunk_id": "doc_9_p9_fixed_0",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Acknowledgments [15] Y. Sun, X. Wang, and X. Tang. Deeply learned face\nrepresentations are sparse, selective, and robust. CoRR,\nWewouldliketothankJohannesSteffensforhisdiscus-\nabs/1412.1265,2014. 1,2,5,8\nsions and great insights on face recognition and Christian\n[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nSzegedy for providing new network architectures like [16] D.Anguelov, D.Erhan, V.Vanhoucke, andA.Rabinovich. and discussing network design choices. Also we are in- Going deeper with convolutions. CoRR, abs/1409.4842,\ndebted to the DistBelief [4] team for their support espe- 2014. 2,4,5,6,9\nciallytoRajatMongaforhelpinsettingupefficienttraining [17] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf. Deepface:\nschemes. Closingthegaptohuman-levelperformanceinfaceverifica-\nAlsoourworkwouldnothavebeenpossiblewithoutthe tion. InIEEEConf.onCVPR,2014. 1,2,5,8\nsupport of Chuck Rosenberg, Hartwig Adam, and Simon [18] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,\nHan. J.Philbin,B.Chen,andY.Wu."
  },
  {
    "chunk_id": "doc_9_p9_fixed_1",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Learningfine-grainedimage\nsimilaritywithdeepranking.CoRR,abs/1404.4661,2014.2\nReferences [19] K.Q.Weinberger,J.Blitzer,andL.K.Saul.Distancemetric\nlearningforlargemarginnearestneighborclassification. In\n[1] Y.Bengio,J.Louradour,R.Collobert,andJ.Weston. Cur- NIPS.MITPress,2006. 2,3\nriculumlearning. InProc.ofICML,NewYork, NY,USA, [20] D.R.WilsonandT.R.Martinez. Thegeneralinefficiency\n2009. 2 ofbatchtrainingforgradientdescentlearning. NeuralNet-\n[2] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian works,16(10):1429–1451,2003. 4\nfacerevisited:Ajointformulation. InProc.ECCV,2012. 2 [21] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-\n[3] D.Chen,S.Ren,Y.Wei,X.Cao,andJ.Sun. Jointcascade constrainedvideoswithmatchedbackgroundsimilarity. In\nfacedetectionandalignment. InProc.ECCV,2014. 8 IEEEConf.onCVPR,2011. 5\n[4] J.Dean,G.Corrado,R.Monga,K.Chen,M.Devin,M.Mao, [22] M.D.ZeilerandR.Fergus. Visualizingandunderstanding\nM. Ranzato, A."
  },
  {
    "chunk_id": "doc_9_p9_fixed_2",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Senior, P. Tucker, K. Yang, Q. V. Le, and convolutionalnetworks.CoRR,abs/1311.2901,2013.2,4,6\nA. Y. Ng. Large scale distributed deep networks. In [23] Z.Zhu,P.Luo,X.Wang,andX.Tang. Recovercanonical-\nP. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Wein- view faces in the wild with deep neural networks. CoRR,\nberger,editors,NIPS,pages1232–1240.2012. 9 abs/1404.3543,2014. 2\n[5] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient\nmethodsforonlinelearningandstochasticoptimization. J.\nMach.Learn.Res.,12:2121–2159,July2011. 4\n[6] I. J. Goodfellow, D. Warde-farley, M. Mirza, A. Courville,\nandY.Bengio. Maxoutnetworks. InInICML,2013. 4\n[7] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face\nrecognition in unconstrained environments. Technical Re-\nport07-49, UniversityofMassachusetts, Amherst, October\n2007. 5\n[8] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel."
  },
  {
    "chunk_id": "doc_9_p9_fixed_3",
    "doc_id": "doc_9",
    "pdf_name": "9.pdf",
    "page": 9,
    "chunk_type": "fixed",
    "text": "Backpropagation\nappliedtohandwrittenzipcoderecognition.NeuralCompu-\ntation,1(4):541–551,Dec.1989. 2,4\n[9] M.Lin,Q.Chen,andS.Yan. Networkinnetwork. CoRR,\nabs/1312.4400,2013. 2,4,6\n[10] C. Lu and X. Tang. Surpassing human-level face veri-\nfication performance on LFW with gaussianface. CoRR,\nabs/1404.3840,2014. 1\n[11] D.E.Rumelhart,G.E.Hinton,andR.J.Williams.Learning\nrepresentationsbyback-propagatingerrors.Nature,1986.2,\n4\n[12] M.SchultzandT.Joachims.Learningadistancemetricfrom\nrelativecomparisons.InS.Thrun,L.Saul,andB.Schölkopf,\neditors,NIPS,pages41–48.MITPress,2004. 2\n[13] T.Sim,S.Baker,andM.Bsat.TheCMUpose,illumination,\nandexpression(PIE)database. InInProc.FG,2002. 2\n[14] Y. Sun, X. Wang, and X. Tang. Deep learning face\nrepresentation by joint identification-verification. CoRR,\nabs/1406.4773,2014. 1,2,3"
  }
]